{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from active_critic.utils.pytorch_utils import calcMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork(th.nn.Module):\n",
    "    def __init__(self, arch:list[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = th.nn.Sequential()\n",
    "        for i in range(len(arch)-2):\n",
    "            self.layers.append(th.nn.Linear(arch[i], arch[i+1]))\n",
    "            self.layers.append(th.nn.ReLU())\n",
    "        self.layers.append(th.nn.Linear(arch[-2], arch[-1]))\n",
    "\n",
    "    def forward(self, inpt:th.Tensor) -> th.Tensor:\n",
    "\n",
    "        return self.layers.forward(inpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nw = MLPNetwork([3, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5408, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt = th.rand(2, 3)\n",
    "outpt = th.rand(2, 2)\n",
    "\n",
    "result = test_nw.forward(inpt)\n",
    "loss = calcMSE(result, outpt)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "\n",
    "\n",
    "class StateModel(nn.Module):\n",
    "    def __init__(self, arch, lr) -> None:\n",
    "        super().__init__()\n",
    "        self.arch = arch\n",
    "        self.lr = lr\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = MLPNetwork(arch=self.arch)\n",
    "        self.optimizer = th.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=0)\n",
    "                \n",
    "\n",
    "    def forward(self, inpt):\n",
    "        return self.model.forward(inpt)\n",
    "\n",
    "    def optimizer_step(self, inpt, label):\n",
    "        result = self.model.forward(inpt=inpt)\n",
    "        loss = calcMSE(result, label)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return {'Loss ':loss.detach()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveCritic(th.nn.Module):\n",
    "    def __init__(self, embedding_model:StateModel, reward_model:StateModel, action_model:StateModel, prediction_model:StateModel) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.reward_model = reward_model\n",
    "        self.action_model = action_model\n",
    "        self.prediction_model = prediction_model\n",
    "        self.current_step = 0\n",
    "\n",
    "    def predict(self, observation:th.Tensor, horizon:int):\n",
    "        embedding = self.embedding_model.forward(observation)\n",
    "\n",
    "    def step_predict(self, embedding:th.Tensor, action:th.Tensor):\n",
    "        state_action = th.cat((action, embedding), dim=-1)\n",
    "        next_embedding = self.prediction_model.forward(state_action)\n",
    "        next_reward = self.reward_model.forward(next_embedding)\n",
    "        return next_embedding, next_reward\n",
    "\n",
    "    def build_sequence(self, observation:th.Tensor, horizon:int, actions:th.Tensor = None):\n",
    "        embeddings_seq = []\n",
    "        action_dependend_embeddings_seq = []\n",
    "        actions_seq = []\n",
    "        rewards_seq = []\n",
    "\n",
    "        embedding = self.embedding_model.forward(observation).detach()\n",
    "        embedding.requires_grad = True\n",
    "        reward = self.reward_model.forward(embedding)\n",
    "        action = self.action_model.forward(embedding).detach()\n",
    "        action.requires_grad = True\n",
    "        embeddings_seq.append(embedding)\n",
    "        actions_seq.append(action)\n",
    "        rewards_seq.append(reward)\n",
    "\n",
    "        for i in range(horizon - self.current_step - 1):\n",
    "            embedding, reward = self.step_predict(embedding, action)\n",
    "            if actions is None:\n",
    "                action = self.action_model.forward(embedding).detach()\n",
    "            else:\n",
    "                action = actions[i]\n",
    "            action_dependend_embeddings_seq.append(embedding)\n",
    "            embedding = embedding.detach()\n",
    "            embedding.requires_grad = True\n",
    "            action.requires_grad = True\n",
    "            embeddings_seq.append(embedding)\n",
    "            actions_seq.append(action)\n",
    "            rewards_seq.append(reward)\n",
    "        \n",
    "\n",
    "        return embeddings_seq, action_dependend_embeddings_seq, actions_seq, rewards_seq\n",
    "            \n",
    "    def optimize_sequence_step(self, sequence:list[th.Tensor], lr:float):\n",
    "        e_n0, e_n1, actions_n1, rewards_n0, rewards_n1 = sequence\n",
    "        goal_label = th.ones_like(rewards_n1[-1])\n",
    "        \n",
    "        r_last_optimizer = th.optim.Adam([e_n0[-1], actions_n1[-1]], lr=lr)\n",
    "        reward_loss = calcMSE(goal_label, rewards_n1[-1])\n",
    "        reward_loss.backward()\n",
    "        r_last_optimizer.step()\n",
    "\n",
    "        for i in range(len(sequence[0])-1, -1, -1):\n",
    "            print(i)\n",
    "            print(e_n0[i])\n",
    "            print(e_n1[i])\n",
    "            print(actions_n1[i])\n",
    "            r_optimizer = th.optim.Adam([e_n0[i], actions_n1[i]], lr=lr)\n",
    "\n",
    "            loss_reward = calcMSE(rewards_n0[i], goal_label)\n",
    "            loss_embedding = calcMSE(e_n1[i-1], e_n0[i])\n",
    "            loss = loss_embedding + loss_reward\n",
    "            loss.backward()\n",
    "            r_optimizer.step()\n",
    "            \n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 2\n",
    "obsv_dim = 3\n",
    "emb_dim = 3\n",
    "rew_dim = 1\n",
    "seq_len = 3\n",
    "batch_size = 2\n",
    "\n",
    "embedding_model = StateModel(arch=[obsv_dim, 10, emb_dim], lr=1e-2)\n",
    "reward_model = StateModel(arch=[emb_dim, 10, rew_dim], lr=1e-2)\n",
    "action_model = StateModel(arch=[emb_dim, 10, action_dim], lr=1e-2)\n",
    "prediction_model = StateModel(arch=[emb_dim+action_dim, 10, emb_dim], lr=1e-2)\n",
    "ac = ActiveCritic(embedding_model=embedding_model, reward_model=reward_model, action_model=action_model, prediction_model=prediction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv = th.ones([batch_size, 1, obsv_dim])\n",
    "embedding = embedding_model.forward(obsv)\n",
    "action = th.ones([batch_size, 1, action_dim])\n",
    "embedding, reward = ac.step_predict(embedding=embedding, action=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = ac.build_sequence(obsv, horizon=seq_len)\n",
    "#embeddings_seq, action_dependend_embeddings_seq, actions_seq, rewards_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.1659, -0.1835, -0.0809]],\n",
       " \n",
       "         [[-0.1659, -0.1835, -0.0809]]], requires_grad=True),\n",
       " tensor([[[ 0.1325, -0.1063, -0.0314]],\n",
       " \n",
       "         [[ 0.1325, -0.1063, -0.0314]]], requires_grad=True),\n",
       " tensor([[[ 0.1625, -0.1167,  0.0453]],\n",
       " \n",
       "         [[ 0.1625, -0.1167,  0.0453]]], requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.1325, -0.1063, -0.0314]],\n",
       " \n",
       "         [[ 0.1325, -0.1063, -0.0314]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[[ 0.1625, -0.1167,  0.0453]],\n",
       " \n",
       "         [[ 0.1625, -0.1167,  0.0453]]], grad_fn=<ViewBackward0>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_r1 = sequence[-1][1]\n",
    "actions_2 = sequence[2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_rewards = th.ones_like(rewards_r1)\n",
    "loss_rewards = calcMSE(goal_rewards, rewards_r1)\n",
    "loss_rewards.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence[0][1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_0 = sequence[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_2 = sequence[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1625, -0.1167,  0.0453]],\n",
       "\n",
       "        [[ 0.1625, -0.1167,  0.0453]]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
