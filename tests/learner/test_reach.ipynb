{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from active_critic.learner.active_critic_learner import ActiveCriticLearner, ACLScores\n",
    "from active_critic.learner.active_critic_args import ActiveCriticLearnerArgs\n",
    "from active_critic.policy.active_critic_policy import ActiveCriticPolicy\n",
    "from active_critic.utils.gym_utils import make_dummy_vec_env, make_vec_env, parse_sampled_transitions, sample_expert_transitions, DummyExtractor, new_epoch_reach, sample_new_episode\n",
    "from active_critic.utils.pytorch_utils import make_part_obs_data, count_parameters\n",
    "from active_critic.utils.dataset import DatasetAC\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from active_critic.utils.dataset import DatasetAC\n",
    "from active_critic.model_src.whole_sequence_model import (\n",
    "    WholeSequenceModelSetup, WholeSequenceModel)\n",
    "from active_critic.model_src.transformer import (\n",
    "    ModelSetup, generate_square_subsequent_mask)\n",
    "from active_critic.policy.active_critic_policy import ActiveCriticPolicySetup, ActiveCriticPolicy\n",
    "\n",
    "\n",
    "from gym import Env\n",
    "th.manual_seed(0)\n",
    "\n",
    "\n",
    "def make_wsm_setup(seq_len, d_output, device='cuda'):\n",
    "    wsm = WholeSequenceModelSetup()\n",
    "    wsm.model_setup = ModelSetup()\n",
    "    seq_len = seq_len\n",
    "    d_output = d_output\n",
    "    wsm.model_setup.d_output = d_output\n",
    "    wsm.model_setup.nhead = 1\n",
    "    wsm.model_setup.d_hid = 512\n",
    "    wsm.model_setup.d_model = 512\n",
    "    wsm.model_setup.nlayers = 4\n",
    "    wsm.model_setup.seq_len = seq_len\n",
    "    wsm.model_setup.dropout = 0\n",
    "    wsm.lr = 1e-4\n",
    "    wsm.model_setup.device = device\n",
    "    wsm.optimizer_class = th.optim.Adam\n",
    "    wsm.optimizer_kwargs = {}\n",
    "    return wsm\n",
    "\n",
    "\n",
    "def make_acps(seq_len, extractor, new_epoch, batch_size=32):\n",
    "    acps = ActiveCriticPolicySetup()\n",
    "    acps.device = 'cuda'\n",
    "    acps.epoch_len = seq_len\n",
    "    acps.extractor = extractor\n",
    "    acps.new_epoch = new_epoch\n",
    "    acps.opt_steps = 100\n",
    "    acps.optimisation_threshold = 0.95\n",
    "    acps.inference_opt_lr = 5e-2\n",
    "    acps.optimize = True\n",
    "    acps.batch_size = 32\n",
    "    return acps\n",
    "\n",
    "\n",
    "def setup_ac_reach(seq_len, num_cpu):\n",
    "    seq_len = seq_len\n",
    "    env, expert = make_vec_env('reach', num_cpu, seq_len=seq_len)\n",
    "    d_output = env.action_space.shape[0]\n",
    "    wsm_actor_setup = make_wsm_setup(\n",
    "        seq_len=seq_len, d_output=d_output)\n",
    "    wsm_critic_setup = make_wsm_setup(\n",
    "        seq_len=seq_len, d_output=1)\n",
    "    acps = make_acps(\n",
    "        seq_len=seq_len, extractor=DummyExtractor(), new_epoch=new_epoch_reach)\n",
    "    actor = WholeSequenceModel(wsm_actor_setup)\n",
    "    critic = WholeSequenceModel(wsm_critic_setup)\n",
    "    ac = ActiveCriticPolicy(observation_space=env.observation_space, action_space=env.action_space,\n",
    "                            actor=actor, critic=critic, acps=acps)\n",
    "    return ac, acps, env, expert\n",
    "\n",
    "\n",
    "def make_acl():\n",
    "    device = 'cuda'\n",
    "    acla = ActiveCriticLearnerArgs()\n",
    "    acla.data_path = '/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/'\n",
    "    acla.device = device\n",
    "    acla.extractor = DummyExtractor()\n",
    "    acla.imitation_phase = False\n",
    "    acla.logname = 'reach_vec_test'\n",
    "    acla.tboard = True\n",
    "    acla.batch_size = 32\n",
    "    acla.val_every = 10\n",
    "    acla.add_data_every = 1\n",
    "    acla.validation_episodes = 10\n",
    "    acla.training_epsiodes = 1\n",
    "    acla.actor_threshold = 5e-2\n",
    "    acla.critic_threshold = 5e-2\n",
    "    acla.num_cpu = 10\n",
    "\n",
    "    seq_len = 100\n",
    "    epsiodes = 30\n",
    "    ac, acps, env, expert = setup_ac_reach(seq_len=seq_len, num_cpu=min(acla.training_epsiodes, acla.num_cpu))\n",
    "    eval_env, expert = make_vec_env('reach', num_cpu=acla.num_cpu, seq_len=seq_len)\n",
    "    acl = ActiveCriticLearner(ac_policy=ac, env=env, eval_env=eval_env, network_args_obj=acla)\n",
    "    return acl, env, expert, seq_len, epsiodes, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl, env, expert, seq_len, epsiodes, device = make_acl()\n",
    "'''transitions = sample_expert_transitions(\n",
    "    policy=expert.predict, env=env, episodes=epsiodes)\n",
    "exp_actions, exp_observations, exp_rewards = parse_sampled_transitions(\n",
    "    transitions=transitions, new_epoch=new_epoch_reach, extractor=DummyExtractor(), device=device)\n",
    "part_acts, part_obsv, part_rews = make_part_obs_data(\n",
    "    actions=exp_actions, observations=exp_observations, rewards=exp_rewards)\n",
    "imitation_data = DatasetAC(device='cuda')\n",
    "imitation_data.onyl_positiv = False\n",
    "imitation_data.add_data(obsv=part_obsv, actions=part_acts, reward=part_rews)'''\n",
    "# acl.setDatasets(train_data=imitation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling expert transitions. 1\n",
      "Sampling expert transitions. 10\n",
      "/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/best_validation\n",
      "Sampling expert transitions. 1\n",
      "Sampling expert transitions. 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acl\u001b[39m.\u001b[39;49mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/learner/active_critic_learner.py:160\u001b[0m, in \u001b[0;36mActiveCriticLearner.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    158\u001b[0m next_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork_args\u001b[39m.\u001b[39mval_every\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork_args\u001b[39m.\u001b[39mtboard:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_validation()\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/learner/active_critic_learner.py:176\u001b[0m, in \u001b[0;36mActiveCriticLearner.run_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_validation\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    175\u001b[0m     h \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 176\u001b[0m     opt_actions, gen_actions, observations, rewards, expected_rewards_before, expected_rewards_after \u001b[39m=\u001b[39m sample_new_episode(\n\u001b[1;32m    177\u001b[0m         policy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy,\n\u001b[1;32m    178\u001b[0m         env\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_env,\n\u001b[1;32m    179\u001b[0m         episodes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork_args\u001b[39m.\u001b[39mvalidation_episodes,\n\u001b[1;32m    180\u001b[0m         return_gen_trj\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    181\u001b[0m     debug_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    182\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mValidation epoch time\u001b[39m\u001b[39m'\u001b[39m: th\u001b[39m.\u001b[39mtensor(time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m h)\n\u001b[1;32m    183\u001b[0m     }\n\u001b[1;32m    184\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_tboard_scalar(debug_dict\u001b[39m=\u001b[39mdebug_dict, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/utils/gym_utils.py:149\u001b[0m, in \u001b[0;36msample_new_episode\u001b[0;34m(policy, env, episodes, return_gen_trj)\u001b[0m\n\u001b[1;32m    147\u001b[0m policy\u001b[39m.\u001b[39meval()\n\u001b[1;32m    148\u001b[0m policy\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 149\u001b[0m transitions \u001b[39m=\u001b[39m sample_expert_transitions(\n\u001b[1;32m    150\u001b[0m     policy\u001b[39m.\u001b[39;49mpredict, env, episodes)\n\u001b[1;32m    151\u001b[0m expected_rewards_after \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mopt_scores[\u001b[39m0\u001b[39m]\n\u001b[1;32m    152\u001b[0m expected_rewards_before \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mgen_scores[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/utils/gym_utils.py:124\u001b[0m, in \u001b[0;36msample_expert_transitions\u001b[0;34m(policy, env, episodes)\u001b[0m\n\u001b[1;32m    122\u001b[0m expert \u001b[39m=\u001b[39m policy\n\u001b[1;32m    123\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSampling expert transitions. \u001b[39m\u001b[39m{\u001b[39;00mepisodes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m rollouts \u001b[39m=\u001b[39m rollout(\n\u001b[1;32m    125\u001b[0m     expert,\n\u001b[1;32m    126\u001b[0m     env,\n\u001b[1;32m    127\u001b[0m     make_sample_until(min_timesteps\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, min_episodes\u001b[39m=\u001b[39;49mepisodes),\n\u001b[1;32m    128\u001b[0m     unwrap\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    129\u001b[0m     exclude_infos\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m flatten_trajectories(rollouts)\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/utils/rollout.py:582\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(policy, venv, sample_until, unwrap, exclude_infos, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrollout\u001b[39m(\n\u001b[1;32m    547\u001b[0m     policy: AnyPolicy,\n\u001b[1;32m    548\u001b[0m     venv: VecEnv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    555\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence[types\u001b[39m.\u001b[39mTrajectoryWithRew]:\n\u001b[1;32m    556\u001b[0m     \u001b[39m\"\"\"Generate policy rollouts.\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \n\u001b[1;32m    558\u001b[0m \u001b[39m    The `.infos` field of each Trajectory is set to `None` to save space.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39m        should truncate if required.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m     trajs \u001b[39m=\u001b[39m generate_trajectories(policy, venv, sample_until, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    583\u001b[0m     \u001b[39mif\u001b[39;00m unwrap:\n\u001b[1;32m    584\u001b[0m         trajs \u001b[39m=\u001b[39m [unwrap_traj(traj) \u001b[39mfor\u001b[39;00m traj \u001b[39min\u001b[39;00m trajs]\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/utils/rollout.py:362\u001b[0m, in \u001b[0;36mgenerate_trajectories\u001b[0;34m(policy, venv, sample_until, deterministic_policy, rng)\u001b[0m\n\u001b[1;32m    360\u001b[0m active \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(venv\u001b[39m.\u001b[39mnum_envs, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m    361\u001b[0m \u001b[39mwhile\u001b[39;00m np\u001b[39m.\u001b[39many(active):\n\u001b[0;32m--> 362\u001b[0m     acts \u001b[39m=\u001b[39m get_actions(obs)\n\u001b[1;32m    363\u001b[0m     obs, rews, dones, infos \u001b[39m=\u001b[39m venv\u001b[39m.\u001b[39mstep(acts)\n\u001b[1;32m    365\u001b[0m     \u001b[39m# If an environment is inactive, i.e. the episode completed for that\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[39m# environment after `sample_until(trajectories)` was true, then we do\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[39m# *not* want to add any subsequent trajectories from it. We avoid this\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39m# by just making it never done.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/policy/active_critic_policy.py:117\u001b[0m, in \u001b[0;36mActiveCriticPolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    113\u001b[0m     action_seq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_result\u001b[39m.\u001b[39mgen_trj\n\u001b[1;32m    115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_seq[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, :] \u001b[39m=\u001b[39m vec_obsv\n\u001b[0;32m--> 117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m    118\u001b[0m     observation_seq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_seq, action_seq\u001b[39m=\u001b[39;49maction_seq, optimize\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs_obj\u001b[39m.\u001b[39;49moptimize, current_step\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_step)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs_obj\u001b[39m.\u001b[39moptimize:\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39madd_value(\n\u001b[1;32m    122\u001b[0m         history\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mopt_scores, \n\u001b[1;32m    123\u001b[0m         value\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_result\u001b[39m.\u001b[39mexpected_succes_after[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step]\u001b[39m.\u001b[39mdetach(), \n\u001b[1;32m    124\u001b[0m         current_step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step,\n\u001b[1;32m    125\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/policy/active_critic_policy.py:154\u001b[0m, in \u001b[0;36mActiveCriticPolicy.forward\u001b[0;34m(self, observation_seq, action_seq, optimize, current_step)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     actions, expected_success_opt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimize_act_sequence(\n\u001b[1;32m    155\u001b[0m         actions\u001b[39m=\u001b[39;49mactions, observations\u001b[39m=\u001b[39;49mobservation_seq, current_step\u001b[39m=\u001b[39;49mcurrent_step)\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m ACPOptResult(\n\u001b[1;32m    158\u001b[0m         gen_trj\u001b[39m=\u001b[39mactions\u001b[39m.\u001b[39mdetach(),\n\u001b[1;32m    159\u001b[0m         expected_succes_before\u001b[39m=\u001b[39mexpected_success,\n\u001b[1;32m    160\u001b[0m         expected_succes_after\u001b[39m=\u001b[39mexpected_success_opt)\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/policy/active_critic_policy.py:175\u001b[0m, in \u001b[0;36mActiveCriticPolicy.optimize_act_sequence\u001b[0;34m(self, actions, observations, current_step)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    173\u001b[0m \u001b[39mwhile\u001b[39;00m (\u001b[39mnot\u001b[39;00m th\u001b[39m.\u001b[39mall(expected_success[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs_obj\u001b[39m.\u001b[39moptimisation_threshold)) \u001b[39mand\u001b[39;00m (step \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs_obj\u001b[39m.\u001b[39mopt_steps):\n\u001b[0;32m--> 175\u001b[0m     optimized_actions, expected_success \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference_opt_step(\n\u001b[1;32m    176\u001b[0m         org_actions\u001b[39m=\u001b[39;49mactions,\n\u001b[1;32m    177\u001b[0m         opt_actions\u001b[39m=\u001b[39;49moptimized_actions,\n\u001b[1;32m    178\u001b[0m         obs_seq\u001b[39m=\u001b[39;49mobservations,\n\u001b[1;32m    179\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    180\u001b[0m         goal_label\u001b[39m=\u001b[39;49mgoal_label,\n\u001b[1;32m    181\u001b[0m         current_step\u001b[39m=\u001b[39;49mcurrent_step)\n\u001b[1;32m    182\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m optimized_actions, expected_success\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/policy/active_critic_policy.py:193\u001b[0m, in \u001b[0;36mActiveCriticPolicy.inference_opt_step\u001b[0;34m(self, org_actions, opt_actions, obs_seq, optimizer, goal_label, current_step)\u001b[0m\n\u001b[1;32m    189\u001b[0m critic_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mloss_fct(\n\u001b[1;32m    190\u001b[0m     result\u001b[39m=\u001b[39mcritic_result, label\u001b[39m=\u001b[39mgoal_label)\n\u001b[1;32m    192\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 193\u001b[0m critic_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    194\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    196\u001b[0m actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_actions(\n\u001b[1;32m    197\u001b[0m     org_actions\u001b[39m=\u001b[39morg_actions, new_actions\u001b[39m=\u001b[39mopt_actions, current_step\u001b[39m=\u001b[39mcurrent_step)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acl.train(epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl.policy.score_history_after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
