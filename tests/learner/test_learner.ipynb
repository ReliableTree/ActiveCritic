{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from active_critic.learner.active_critic_learner import ActiveCriticLearner, ACLScores\n",
    "from active_critic.learner.active_critic_args import ActiveCriticLearnerArgs\n",
    "from active_critic.policy.active_critic_policy import ActiveCriticPolicy\n",
    "from active_critic.utils.test_utils import setup_ac_reach\n",
    "from active_critic.utils.gym_utils import make_dummy_vec_env, parse_sampled_transitions, sample_expert_transitions, DummyExtractor, new_epoch_reach, sample_new_episode\n",
    "from active_critic.utils.pytorch_utils import make_part_obs_data\n",
    "from active_critic.utils.dataset import DatasetAC\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from active_critic.utils.dataset import DatasetAC\n",
    "from gym import Env\n",
    "th.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "acla = ActiveCriticLearnerArgs()\n",
    "acla.data_path = '/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/'\n",
    "acla.device = device\n",
    "acla.extractor = DummyExtractor()\n",
    "acla.imitation_phase = False\n",
    "acla.logname = 'test_acl'\n",
    "acla.tboard = True\n",
    "acla.batch_size = 32\n",
    "acla.validation_episodes = 5\n",
    "acla.val_every = 100\n",
    "seq_len = 100\n",
    "epsiodes = 2\n",
    "ac, acps, env = setup_ac_reach(seq_len=seq_len)\n",
    "acl = ActiveCriticLearner(ac_policy=ac, env=env, network_args_obj=acla)\n",
    "env, expert = make_dummy_vec_env(name='reach', seq_len=seq_len)\n",
    "transitions = sample_expert_transitions(policy=expert.predict, env=env, episodes=epsiodes)\n",
    "exp_actions, exp_observations, exp_rewards = parse_sampled_transitions(transitions=transitions, new_epoch=new_epoch_reach, extractor=DummyExtractor(), device=device)\n",
    "assert th.all(exp_rewards[:,-1]==1), 'Expert cant solve Environment.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "acla = ActiveCriticLearnerArgs()\n",
    "acla.data_path = '/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/'\n",
    "acla.device = device\n",
    "acla.extractor = DummyExtractor()\n",
    "acla.imitation_phase = True\n",
    "acla.logname = 'test_acl'\n",
    "acla.tboard = True\n",
    "acla.batch_size = 32\n",
    "acla.val_every = 10000\n",
    "acla.validation_episodes = 5\n",
    "seq_len = 5\n",
    "epsiodes = 2\n",
    "ac, acps, env = setup_ac_reach(seq_len=seq_len)\n",
    "acl = ActiveCriticLearner(ac_policy=ac, env=env, network_args_obj=acla)\n",
    "env, expert = make_dummy_vec_env(name='reach', seq_len=seq_len)\n",
    "transitions = sample_expert_transitions(policy=expert.predict, env=env, episodes=epsiodes)\n",
    "exp_actions, exp_observations, exp_rewards = parse_sampled_transitions(transitions=transitions, new_epoch=new_epoch_reach, extractor=DummyExtractor(), device=device)\n",
    "\n",
    "part_acts, part_obsv, part_rews = make_part_obs_data(actions=exp_actions, observations=exp_observations, rewards=exp_rewards)\n",
    "\n",
    "for i in range(seq_len*epsiodes):\n",
    "    org_index = int(i/seq_len)\n",
    "    part_obs = th.clone(exp_observations[org_index])\n",
    "    part_obs[i%seq_len + 1 :] = 0\n",
    "    assert th.equal(part_acts[i], exp_actions[org_index]), 'make_part_obs_data corrupted'\n",
    "    assert th.equal(part_obsv[i], part_obs), 'make_part_obs_data corrupted'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_scores = ACLScores()\n",
    "acl_scores.update_min_score(acl_scores.mean_actor, 0)\n",
    "assert acl_scores.mean_actor == [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, observations, rewards, expected_rewards_before, expected_rewards_after = sample_new_episode(\n",
    "    policy=ac,\n",
    "    env=env,\n",
    "    episodes=1,)\n",
    "assert list(actions.shape) == [1, seq_len, env.action_space.shape[0]]\n",
    "assert list(observations.shape) == [1, seq_len, env.observation_space.shape[0]]\n",
    "assert list(rewards.shape) == [1, seq_len, 1]\n",
    "assert list(expected_rewards_after.shape) == [1, seq_len, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_data = DatasetAC(device='cuda')\n",
    "imitation_data.onyl_positiv = False\n",
    "imitation_data.add_data(obsv=part_obsv, actions=part_acts, reward=part_rews)\n",
    "assert len(imitation_data) == epsiodes * seq_len\n",
    "assert list(imitation_data.actions.shape) == [epsiodes * seq_len, seq_len, env.action_space.shape[0]]\n",
    "assert list(imitation_data.obsv.shape) == [epsiodes * seq_len, seq_len, env.observation_space.shape[0]]\n",
    "assert list(imitation_data.reward.shape) == [epsiodes * seq_len, seq_len, 1]\n",
    "imitation_data.onyl_positiv = True\n",
    "assert len(imitation_data) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_data.onyl_positiv = False\n",
    "dataloader = th.utils.data.DataLoader(imitation_data, batch_size=2*len(imitation_data))\n",
    "for data in dataloader:\n",
    "    dobsv, dact, drews = data\n",
    "assert th.equal(dact, part_acts)\n",
    "assert th.equal(dobsv, part_obsv)\n",
    "assert th.equal(drews, part_rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_data.add_data(obsv=part_obsv, actions=part_acts, reward=part_rews)\n",
    "imitation_data.onyl_positiv = False\n",
    "dataloader = th.utils.data.DataLoader(imitation_data, batch_size=2*len(imitation_data))\n",
    "for data in dataloader:\n",
    "    dobsv, dact, drews = data\n",
    "assert list(dobsv.shape) == [2*epsiodes * seq_len, seq_len, env.observation_space.shape[0]]\n",
    "assert list(dact.shape) == [2*epsiodes * seq_len, seq_len, env.action_space.shape[0]]\n",
    "assert list(drews.shape) == [2*epsiodes * seq_len, seq_len, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl.setDatasets(train_data=imitation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl.policy.args_obj.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl.train(epochs=100)\n",
    "#assert acl.scores.mean_actor[0] < 1e-3\n",
    "#assert acl.scores.mean_critic[0] < 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl.scores.mean_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from active_critic.learner.active_critic_learner import ActiveCriticLearner, ACLScores\n",
    "from active_critic.learner.active_critic_args import ActiveCriticLearnerArgs\n",
    "from active_critic.policy.active_critic_policy import ActiveCriticPolicy\n",
    "from active_critic.utils.test_utils import setup_ac_reach\n",
    "from active_critic.utils.gym_utils import make_dummy_vec_env, parse_sampled_transitions, sample_expert_transitions, DummyExtractor, new_epoch_reach, sample_new_episode\n",
    "from active_critic.utils.pytorch_utils import make_part_obs_data\n",
    "from active_critic.utils.dataset import DatasetAC\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from active_critic.utils.dataset import DatasetAC\n",
    "from gym import Env\n",
    "th.manual_seed(0)\n",
    "\n",
    "def make_acl():\n",
    "        device = 'cuda'\n",
    "        acla = ActiveCriticLearnerArgs()\n",
    "        acla.data_path = '/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/'\n",
    "        acla.device = device\n",
    "        acla.extractor = DummyExtractor()\n",
    "        acla.imitation_phase = True\n",
    "        acla.logname = 'test_acl'\n",
    "        acla.tboard = False\n",
    "        acla.batch_size = 32\n",
    "        acla.val_every = 10000\n",
    "        acla.validation_episodes = 5\n",
    "        seq_len = 5\n",
    "        epsiodes = 2\n",
    "        ac, acps, env = setup_ac_reach(seq_len=seq_len)\n",
    "        acl = ActiveCriticLearner(ac_policy=ac, env=env, network_args_obj=acla)\n",
    "        env, expert = make_dummy_vec_env(name='reach', seq_len=seq_len)\n",
    "        return acl, env, expert, seq_len, epsiodes, device\n",
    "seq_len = 5\n",
    "epsiodes = 2\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "epsiodes = 30\n",
    "device = 'cuda'\n",
    "env, expert = make_dummy_vec_env(name='reach', seq_len=seq_len)\n",
    "transitions = sample_expert_transitions(policy=expert.predict, env=env, episodes=epsiodes)\n",
    "exp_actions, exp_observations, exp_rewards = parse_sampled_transitions(transitions=transitions, new_epoch=new_epoch_reach, extractor=DummyExtractor(), device=device)\n",
    "part_acts, part_obsv, part_rews = make_part_obs_data(actions=exp_actions, observations=exp_observations, rewards=exp_rewards)\n",
    "imitation_data = DatasetAC(device='cuda')\n",
    "imitation_data.onyl_positiv = False\n",
    "imitation_data.add_data(obsv=part_obsv, actions=part_acts, reward=part_rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "acl, env, expert, seq_len, epsiodes, device = make_acl()\n",
    "acl.setDatasets(train_data=imitation_data)\n",
    "acl.train(epochs=200)\n",
    "acl.run_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_before = acl.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "acl, env, expert, seq_len, epsiodes, device = make_acl()\n",
    "acl.setDatasets(train_data=imitation_data)\n",
    "acl.train(epochs=200)\n",
    "acl.saveNetworkToFile(data_path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/', add='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl, env, expert, seq_len, epsiodes, device = make_acl()\n",
    "acl.loadNetworkFromFile(path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/best/')\n",
    "acl.train(epochs=200)\n",
    "acl.run_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl.scores.mean_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_before.mean_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert th.allclose(acl.scores.mean_actor[0], scores_before.mean_actor[0])\n",
    "assert th.allclose(acl.scores.mean_critic[0], scores_before.mean_critic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "import torch as th\n",
    "from active_critic.learner.active_critic_args import ActiveCriticLearnerArgs\n",
    "from active_critic.learner.active_critic_learner import ActiveCriticLearner, ACLScores\n",
    "from active_critic.utils.dataset import DatasetAC\n",
    "from active_critic.utils.gym_utils import (DummyExtractor, make_dummy_vec_env,\n",
    "                                           new_epoch_reach,\n",
    "                                           parse_sampled_transitions,\n",
    "                                           sample_expert_transitions,\n",
    "                                           sample_new_episode)\n",
    "from active_critic.utils.pytorch_utils import make_part_obs_data, calcMSE\n",
    "from active_critic.utils.test_utils import setup_ac_reach\n",
    "from gym import Env\n",
    "import numpy as np\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "th.manual_seed(0)\n",
    "\n",
    "\n",
    "class TestLerner(unittest.TestCase):\n",
    "    def make_acl(self, device='cuda'):\n",
    "        acla = ActiveCriticLearnerArgs()\n",
    "        acla.data_path = '/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/'\n",
    "        acla.device = device\n",
    "        acla.extractor = DummyExtractor()\n",
    "        acla.imitation_phase = True\n",
    "        acla.logname = 'test_acl'\n",
    "        acla.tboard = True\n",
    "        acla.batch_size = 32\n",
    "        acla.val_every = 100000\n",
    "        acla.validation_episodes = 5\n",
    "        acla.training_epsiodes = 1\n",
    "        acla.num_cpu = 1\n",
    "        acla.actor_threshold = 0.5\n",
    "        acla.critic_threshold = 0.5\n",
    "        seq_len = 5\n",
    "        epsiodes = 2\n",
    "        ac, acps, env = setup_ac_reach(seq_len=seq_len)\n",
    "        acl = ActiveCriticLearner(ac_policy=ac, env=env, eval_env=env, network_args_obj=acla)\n",
    "        env, expert = make_dummy_vec_env(name='reach', seq_len=seq_len)\n",
    "        return acl, env, expert, seq_len, epsiodes, device\n",
    "\n",
    "\n",
    "    def test_make_part_seq_with_td(self):\n",
    "        acl, env, expert, seq_len, epsiodes, device = self.make_acl()\n",
    "        transitions = sample_expert_transitions(\n",
    "            policy=expert.predict, env=env, episodes=epsiodes)\n",
    "        exp_actions, exp_observations, exp_rewards = parse_sampled_transitions(\n",
    "            transitions=transitions, new_epoch=new_epoch_reach, extractor=DummyExtractor(), device=device)\n",
    "\n",
    "        part_acts, part_obsv, part_rews = make_part_obs_data(\n",
    "            actions=exp_actions, observations=exp_observations, rewards=exp_rewards)\n",
    "\n",
    "        for i in range(seq_len*epsiodes):\n",
    "            org_index = int(i/seq_len)\n",
    "            part_obs = th.clone(exp_observations[org_index])\n",
    "            part_obs[i % seq_len + 1:] = 0\n",
    "            self.assertTrue(\n",
    "                th.equal(part_acts[i], exp_actions[org_index]), 'make_part_obs_data corrupted')\n",
    "            self.assertTrue(\n",
    "                th.equal(part_obsv[i], part_obs), 'make_part_obs_data corrupted')\n",
    "\n",
    "    def test_sample_new_epsiode(self):\n",
    "        acl, env, expert, seq_len, epsiodes, device = self.make_acl()\n",
    "\n",
    "        actions, observations, rewards,expected_rewards_before, expected_rewards_after = sample_new_episode(\n",
    "            policy=acl.policy,\n",
    "            env=env,\n",
    "            episodes=1,\n",
    "        )\n",
    "        self.assertTrue(list(actions.shape) == [\n",
    "                        1, seq_len, env.action_space.shape[0]])\n",
    "        self.assertTrue(list(observations.shape) == [\n",
    "                        1, seq_len, env.observation_space.shape[0]])\n",
    "        self.assertTrue(list(rewards.shape) == [1, seq_len, 1])\n",
    "        self.assertTrue(list(expected_rewards_after.shape) == [1, seq_len, 1])\n",
    "\n",
    "    def test_convergence(self):\n",
    "        th.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        acl, env, expert, seq_len, epsiodes, device = self.make_acl()\n",
    "        transitions = sample_expert_transitions(\n",
    "            policy=expert.predict, env=env, episodes=epsiodes)\n",
    "        exp_actions, exp_observations, exp_rewards = parse_sampled_transitions(\n",
    "            transitions=transitions, new_epoch=new_epoch_reach, extractor=DummyExtractor(), device=device)\n",
    "        part_acts, part_obsv, part_rews = make_part_obs_data(\n",
    "            actions=exp_actions, observations=exp_observations, rewards=exp_rewards)\n",
    "\n",
    "        imitation_data = DatasetAC(device='cuda')\n",
    "        imitation_data.onyl_positiv = False\n",
    "        imitation_data.add_data(\n",
    "            obsv=part_obsv, actions=part_acts, reward=part_rews)\n",
    "\n",
    "        self.assertTrue(len(imitation_data) == epsiodes * seq_len)\n",
    "        self.assertTrue(list(imitation_data.actions.shape) == [\n",
    "                        epsiodes * seq_len, seq_len, env.action_space.shape[0]])\n",
    "        self.assertTrue(list(imitation_data.obsv.shape) == [\n",
    "                        epsiodes * seq_len, seq_len, env.observation_space.shape[0]])\n",
    "        self.assertTrue(list(imitation_data.reward.shape)\n",
    "                        == [epsiodes * seq_len, seq_len, 1])\n",
    "        imitation_data.onyl_positiv = True\n",
    "        self.assertTrue(len(imitation_data) == 0)\n",
    "\n",
    "        imitation_data.onyl_positiv = False\n",
    "        dataloader = th.utils.data.DataLoader(\n",
    "            imitation_data, batch_size=2*len(imitation_data))\n",
    "        for data in dataloader:\n",
    "            dobsv, dact, drews = data\n",
    "        self.assertTrue(th.equal(dact, part_acts))\n",
    "        self.assertTrue(th.equal(dobsv, part_obsv))\n",
    "        self.assertTrue(th.equal(drews, part_rews))\n",
    "\n",
    "        imitation_data.add_data(\n",
    "            obsv=part_obsv, actions=part_acts, reward=part_rews)\n",
    "        imitation_data.onyl_positiv = False\n",
    "        dataloader = th.utils.data.DataLoader(\n",
    "            imitation_data, batch_size=2*len(imitation_data))\n",
    "        for data in dataloader:\n",
    "            dobsv, dact, drews = data\n",
    "        self.assertTrue(list(dobsv.shape) == [\n",
    "                        2*epsiodes * seq_len, seq_len, env.observation_space.shape[0]])\n",
    "        self.assertTrue(list(dact.shape) == [\n",
    "                        2*epsiodes * seq_len, seq_len, env.action_space.shape[0]])\n",
    "        self.assertTrue(list(drews.shape) == [\n",
    "                        2*epsiodes * seq_len, seq_len, 1])\n",
    "\n",
    "        acl.setDatasets(train_data=imitation_data)\n",
    "        \n",
    "        actions_L2_b = []\n",
    "        rew_L2_b = []\n",
    "        acl.policy.args_obj.optimize = False\n",
    "        for data in acl.train_loader:\n",
    "            dobsv, dact, drews = data\n",
    "            for epoch in range(dobsv.shape[0]):\n",
    "                predicted_actions = []\n",
    "                acl.policy.reset()\n",
    "                if th.count_nonzero(dobsv[epoch][:,-3:]) == th.numel(dobsv[epoch][:,-3:]):\n",
    "                    for step in range(dobsv.shape[1]):\n",
    "                        action = acl.policy.predict(observation=dobsv[epoch, step].unsqueeze(0))\n",
    "                        predicted_actions.append(action)\n",
    "                    th_action = th.tensor(np.array(predicted_actions))\n",
    "                    actions_L2_b.append(calcMSE(th_action, dact[epoch].to('cpu')))\n",
    "                    rew_L2_b.append(calcMSE(acl.policy.history.gen_scores[0], drews[epoch]))\n",
    "\n",
    "        L2_actions_mean_b = th.tensor([*actions_L2_b]).mean()\n",
    "        L2_critic_mean_b = th.tensor([*rew_L2_b]).mean()\n",
    "                \n",
    "\n",
    "        acl.train(epochs=1000)\n",
    "        self.assertTrue(acl.scores.mean_actor[0] < 1e-3)\n",
    "        self.assertTrue(acl.scores.mean_critic[0] < 1e-3)\n",
    "        \n",
    "        actions_L2 = []\n",
    "        rew_L2 = []\n",
    "        acl.policy.args_obj.optimize = False\n",
    "        for data in acl.train_loader:\n",
    "            dobsv, dact, drews = data\n",
    "            for epoch in range(dobsv.shape[0]):\n",
    "                predicted_actions = []\n",
    "                acl.policy.reset()\n",
    "                if th.count_nonzero(dobsv[epoch][:,-3:]) == th.numel(dobsv[epoch][:,-3:]):\n",
    "                    for step in range(dobsv.shape[1]):\n",
    "                        action = acl.policy.predict(observation=dobsv[epoch, step].unsqueeze(0))\n",
    "                        predicted_actions.append(action)\n",
    "                    th_action = th.tensor(np.array(predicted_actions))\n",
    "                    actions_L2.append(calcMSE(th_action, dact[epoch].to('cpu')))\n",
    "                    rew_L2.append(calcMSE(acl.policy.history.gen_scores[0], drews[epoch]))\n",
    "\n",
    "        L2_actions_mean = th.tensor([*actions_L2]).mean()\n",
    "        L2_critic_mean = th.tensor([*rew_L2]).mean()\n",
    "\n",
    "        self.assertTrue(L2_actions_mean < 0.1 * L2_actions_mean_b, 'The prediction did not converge.')\n",
    "        self.assertTrue(L2_critic_mean < 0.1 * L2_critic_mean_b, 'The prediction did not converge.')\n",
    "\n",
    "        lenght_before = 0\n",
    "        for data in acl.train_loader:\n",
    "            obsv, actions, reward = data\n",
    "            lenght_before += len(obsv)\n",
    "\n",
    "        acl.add_training_data()\n",
    "        lenght_after = 0\n",
    "        for data in acl.train_loader:\n",
    "            obsv, actions, reward = data\n",
    "            lenght_after += len(obsv)          \n",
    "        self.assertTrue(lenght_before + acl.policy.args_obj.epoch_len == lenght_after, 'data has not ben added to externally set training data set.')\n",
    "\n",
    "\n",
    "\n",
    "    def test_ac_score(self):\n",
    "        acl_scores = ACLScores()\n",
    "        acl_scores.update_min_score(acl_scores.mean_actor, 0)\n",
    "        self.assertTrue(acl_scores.mean_actor == [0])\n",
    "\n",
    "    def test_add_training_data(self):\n",
    "        acl, env, expert, seq_len, epsiodes, device = self.make_acl()\n",
    "\n",
    "        acl.add_training_data()\n",
    "        length = 0\n",
    "        for data in acl.train_loader:\n",
    "            obsv, actions, reward = data\n",
    "            length += len(obsv)\n",
    "        self.assertTrue(acl.policy.args_obj.epoch_len == length)\n",
    "\n",
    "    def test_save_and_load(self):\n",
    "        seq_len = 5\n",
    "        epsiodes = 2\n",
    "        device = 'cpu'\n",
    "        env, expert = make_dummy_vec_env(name='reach', seq_len=seq_len)\n",
    "        transitions = sample_expert_transitions(policy=expert.predict, env=env, episodes=epsiodes)\n",
    "        exp_actions, exp_observations, exp_rewards = parse_sampled_transitions(transitions=transitions, new_epoch=new_epoch_reach, extractor=DummyExtractor(), device=device)\n",
    "        part_acts, part_obsv, part_rews = make_part_obs_data(actions=exp_actions, observations=exp_observations, rewards=exp_rewards)\n",
    "        imitation_data = DatasetAC(device=device)\n",
    "        imitation_data.onyl_positiv = False\n",
    "        imitation_data.add_data(obsv=part_obsv, actions=part_acts, reward=part_rews)\n",
    "        th.manual_seed(0)\n",
    "        acl, env, expert, seq_len, epsiodes, device = self.make_acl(device=device)\n",
    "        acl.setDatasets(train_data=imitation_data)\n",
    "        self.assertTrue(len(acl.train_data) == seq_len*epsiodes, 'set dataset failed')\n",
    "\n",
    "        acl.train(epochs=400)\n",
    "        acl.run_validation()\n",
    "        scores_before = acl.scores\n",
    "\n",
    "        th.manual_seed(0)\n",
    "        acl, env, expert, seq_len, epsiodes, device = self.make_acl(device=device)\n",
    "        acl.setDatasets(train_data=imitation_data)\n",
    "        acl.train(epochs=200)\n",
    "        acl.saveNetworkToFile(data_path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/', add='best')\n",
    "        acl, env, expert, seq_len, epsiodes, device = self.make_acl(device=device)\n",
    "        acl.loadNetworkFromFile(path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/best/')\n",
    "        acl.train(epochs=200)\n",
    "        acl.run_validation()\n",
    "\n",
    "        self.assertTrue(len(acl.train_data) == seq_len*epsiodes, 'Load dataset failed.')\n",
    "\n",
    "        self.assertTrue(th.allclose(acl.scores.mean_actor[0], scores_before.mean_actor[0]), 'Network did not converge as predicted after load from file.')\n",
    "        self.assertTrue(th.allclose(acl.scores.mean_critic[0], scores_before.mean_critic[0]))\n",
    "\n",
    "tu = TestLerner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gym.vector.make('Hopper-v2', num_envs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    obs, rew, done, info = a.step(np.array([[1,100,0], [1,0,0]]))\n",
    "    if rew[0]:\n",
    "        rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.19898477,  0.91285998])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
