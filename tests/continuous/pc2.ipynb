{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import sample_expert_transitions\n",
    "import torch as th\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import gym\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.policies import BaseModel\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "from active_critic.utils.gym_utils import DummyExtractor\n",
    "import numpy as np\n",
    "from active_critic.utils.gym_utils import make_policy_dict\n",
    "from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from gym.wrappers import TimeLimit\n",
    "from active_critic.utils.pytorch_utils import detokenize, tokenize, calcMSE\n",
    "from stable_baselines3 import SAC\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "import stable_baselines3\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from stable_baselines3.common.torch_layers import FlattenExtractor\n",
    "class TestExtractor(FlattenExtractor):\n",
    "    def __init__(self, observation_space) -> None:\n",
    "        super().__init__(observation_space)\n",
    "        self._features_dim = observation_space.shape[0]\n",
    "        self.initial_phase = True\n",
    "        self.emitter = None\n",
    "\n",
    "    def forward(self, observations):\n",
    "        ext_obsv = super().forward(observations)\n",
    "        result = th.zeros([*ext_obsv.shape[:-1], self._features_dim], device=ext_obsv.device, dtype=ext_obsv.dtype)\n",
    "        with th.no_grad():\n",
    "            if (not self.initial_phase) and (observations.flatten()[-1] == 0):\n",
    "                embedding = self.emitter(ext_obsv.squeeze()[..., :-4])\n",
    "            else:\n",
    "                embedding = ext_obsv\n",
    "            \n",
    "            \n",
    "            result[..., :embedding.shape[-1]] = embedding\n",
    "            result[..., -3:] = ext_obsv[..., -4:-1]\n",
    "            \n",
    "        return result\n",
    "\n",
    "def quantize(inpt, minimum, maximum, nquants):\n",
    "    return inpt\n",
    "    '''scale = maximum - minimum\n",
    "    rec_inpt = ((inpt - minimum) / scale)*(nquants-1)\n",
    "    rounded = th.round(rec_inpt)\n",
    "    result = (rounded / (nquants - 1))*scale\n",
    "    return result'''\n",
    "\n",
    "\n",
    "class MDPData(th.utils.data.Dataset):\n",
    "    def __init__(self, device) -> None:\n",
    "        super().__init__()\n",
    "        self.obsv = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.done = None\n",
    "        self.device = device\n",
    "\n",
    "    def add_step(self, obsv:th.Tensor, action:th.Tensor, reward:th.Tensor, done:th.Tensor):\n",
    "        if self.obsv is None:\n",
    "            self.obsv = obsv.reshape([1, -1]).to(self.device)\n",
    "        else:\n",
    "            self.obsv = th.cat((self.obsv, obsv.to(self.device).reshape([1, -1])), dim=0)\n",
    "\n",
    "        if self.action is None:\n",
    "            self.action = action.to(self.device).reshape([1, -1])\n",
    "        else:\n",
    "            self.action = th.cat((self.action, action.to(self.device).reshape([1, -1])), dim=0)\n",
    "\n",
    "        if self.reward is None:\n",
    "            self.reward = reward.to(self.device).reshape([1, -1])\n",
    "        else:\n",
    "            self.reward = th.cat((self.reward, reward.to(self.device).reshape([1, -1])), dim=0)\n",
    "\n",
    "        if self.done is None:\n",
    "            self.done = done.to(self.device).reshape([1])\n",
    "        else:\n",
    "            self.done = th.cat((self.done, done.to(self.device).reshape([-1])), dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obsv)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        done = self.done[index]\n",
    "\n",
    "        if done:\n",
    "            return self.obsv[index], th.zeros_like(self.obsv[index]), self.action[index], th.zeros_like(self.action[index]), self.reward[index], th.zeros_like(self.reward[index]), done\n",
    "        else:\n",
    "            return self.obsv[index], self.obsv[index+1], self.action[index], self.action[index+1], self.reward[index], self.reward[index+1], done\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, quantisation=0, activation=nn.ReLU(), dropout=0, use_batch_norm=False):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # create a sequential container to hold the layers\n",
    "        self.layers = nn.Sequential()\n",
    "        \n",
    "        # create the input layer\n",
    "        self.layers.add_module(\"input\", nn.Linear(input_size, hidden_sizes[0]))\n",
    "        \n",
    "        # create the hidden layers\n",
    "        for i, size in enumerate(hidden_sizes[1:]):\n",
    "            self.layers.add_module(f\"hidden_{i+1}\", nn.Linear(hidden_sizes[i], size))\n",
    "            if use_batch_norm:\n",
    "                self.layers.add_module(f\"batch_norm_{i+1}\", nn.BatchNorm1d(size))\n",
    "            self.layers.add_module(f\"activation_{i+1}\", activation)\n",
    "            if dropout > 0:\n",
    "                self.layers.add_module(f\"dropout_{i+1}\", nn.Dropout(dropout))\n",
    "        \n",
    "        # create the output layer\n",
    "        self.layers.add_module(\"output\", nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.quantisation = quantisation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_shape = x.shape\n",
    "        quantized = len(x_shape) == 4\n",
    "        if quantized: #quantized input\n",
    "            x = x.reshape([x.shape[0], x.shape[1], -1])\n",
    "        # forward pass through the layers\n",
    "\n",
    "        result = self.layers(x)\n",
    "        if self.quantisation != 0:\n",
    "            result = result.reshape([x_shape[0], x_shape[1], -1, self.quantisation])\n",
    "        return result\n",
    "\n",
    "class MDPLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embbed_size, \n",
    "        ntokens_obsv, \n",
    "        ntokens_act, \n",
    "        observation_space, \n",
    "        action_space, \n",
    "        min_obsv,\n",
    "        max_obsv,\n",
    "        min_action,\n",
    "        max_action,\n",
    "        embedding_decimals:int,  \n",
    "        device:str, \n",
    "        max_batch_size = 64) -> None:\n",
    "\n",
    "        \n",
    "        super().__init__()\n",
    "        obsv_size = observation_space.shape[0] - 4\n",
    "        action_size = action_space.shape[0]\n",
    "\n",
    "        hidden_size = 128\n",
    "        goal_size = 3\n",
    "\n",
    "        self.emitter = MLP(input_size=obsv_size, hidden_sizes=[hidden_size, hidden_size, hidden_size], output_size=embbed_size, quantisation=0).to(device)\n",
    "        self.predictor = MLP(input_size=(embbed_size+action_size), hidden_sizes=[hidden_size, hidden_size, hidden_size], output_size=embbed_size, quantisation=0).to(device)\n",
    "        self.reward_model = MLP(input_size=(embbed_size+action_size+goal_size), hidden_sizes=[hidden_size, hidden_size, hidden_size], output_size=1, quantisation=0).to(device)\n",
    "        \n",
    "        self.optimizer = th.optim.Adam(params=list(self.emitter.parameters()) + list(self.predictor.parameters())+ list(self.reward_model.parameters()), lr=1e-4)\n",
    "        self.scheduler = th.optim.lr_scheduler.StepLR(optimizer=self.optimizer, step_size=300000/32, gamma=0.95)\n",
    "        self.obs_minimum = min_obsv.to(device)\n",
    "        self.obs_maximum = max_obsv.to(device)\n",
    "        self.action_minimum = min_action.to(device)\n",
    "        self.action_maximum = max_action.to(device)\n",
    "        self.ntokens_obsv = ntokens_obsv\n",
    "        self.ntokens_act = ntokens_act\n",
    "        self.embbed_size = embbed_size\n",
    "\n",
    "        self.embedding_decimals = embedding_decimals\n",
    "\n",
    "        self.obs_minimum = self.obs_minimum.reshape([1,1,-1]).repeat([max_batch_size, 1, 1]).to(device)\n",
    "        self.obs_maximum = self.obs_maximum.reshape([1,1,-1]).repeat([max_batch_size, 1, 1]).to(device)\n",
    "        self.action_minimum = self.action_minimum.reshape([1,1,-1]).repeat([max_batch_size, 1, 1]).to(device)\n",
    "        self.action_maximum = self.action_maximum.reshape([1,1,-1]).repeat([max_batch_size, 1, 1]).to(device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def qemb_qact_f_obsv(self, obsvs, actions):\n",
    "        assert len(obsvs.shape) == 3\n",
    "        goals, obsvs = self.get_goal(obsvs=obsvs)\n",
    "        embeddings = self.emitter.forward(obsvs)\n",
    "        return self.get_q_emb_q_act(embeddings=embeddings, actions=actions, goals=goals), goals\n",
    "\n",
    "    def get_goal(self, obsvs):\n",
    "        goals = obsvs[...,-4:-1]\n",
    "        obsvs = obsvs[...,:-4]\n",
    "        return goals, obsvs\n",
    "\n",
    "    def get_q_emb_q_act(self, embeddings, actions, goals):\n",
    "        batch_size = actions.shape[0]\n",
    "        qactions = quantize(actions, minimum=self.action_minimum[:batch_size], maximum=self.action_maximum[:batch_size], nquants=self.ntokens_act)\n",
    "        #qembeddings = th.round(embeddings, decimals=self.embedding_decimals)\n",
    "        qembeddings = embeddings\n",
    "        emb_act = th.cat((qembeddings, qactions), dim=2)\n",
    "        emb_act_goal = th.cat((emb_act, goals), dim=2)\n",
    "        return emb_act, qembeddings, emb_act_goal\n",
    "\n",
    "\n",
    "    def learn_step(self, obsvs:th.Tensor, n_obsvs:th.Tensor, actions:th.Tensor, n_actions:th.Tensor, rewards:th.Tensor, n_rewards:th.Tensor, dones:th.Tensor):\n",
    "        #Inputs are step wise, so seq_len = 1\n",
    "        obsvs = obsvs.unsqueeze(1)\n",
    "        n_obsvs = n_obsvs.unsqueeze(1)\n",
    "        actions = actions.unsqueeze(1)\n",
    "        n_actions = n_actions.unsqueeze(1)\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        n_rewards = n_rewards.unsqueeze(1)\n",
    "\n",
    "        #Reshape the maximum and minimum according to batch size\n",
    "\n",
    "        nd_n_actions = n_actions[~dones]\n",
    "        nd_n_observations = n_obsvs[~dones]\n",
    "        nd_nrewards = n_rewards[~dones]\n",
    "\n",
    "        '''print(f'actions: {actions.shape}')\n",
    "        print(f'obsvs: {obsvs.shape}')\n",
    "        print(f'rewards: {rewards.shape}')\n",
    "        print(f'actions: {actions.shape}')\n",
    "        print(f'nd_n_actions: {nd_n_actions.shape}')\n",
    "        print(f'nd_n_observations: {nd_n_observations.shape}')\n",
    "        print(f'nd_nrewards: {nd_nrewards.shape}')\n",
    "        print(f'dones: {dones.shape}')'''\n",
    "\n",
    "\n",
    "        rew1_loss, emb_act1, embeddings1, expected_rewards1, goals = self.step_reward_model(actions=actions, observations=obsvs, rewards=rewards, do_print=False)\n",
    "        rew2_loss, emb_act2, q_embeddings2, expected_rewards2, ngoals = self.step_reward_model(actions=nd_n_actions, observations=nd_n_observations, rewards=nd_nrewards, do_print=False)\n",
    "        \n",
    "        nd_emb_act1 = emb_act1[~dones]\n",
    "\n",
    "        pred_loss, pred_n_embeddings = self.step_predictor(emb_act=nd_emb_act1, n_embeddings=q_embeddings2)\n",
    "\n",
    "        pred_rew2_loss, _, _, _ = self.step_predictive_reward_model(actions=nd_n_actions, embeddings=pred_n_embeddings, rewards=nd_nrewards, goals=ngoals)\n",
    "\n",
    "\n",
    "        loss = rew1_loss + rew2_loss + pred_loss + pred_rew2_loss\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        #q_pred_embeddings2 = th.round(pred_n_embeddings, decimals=self.embedding_decimals)\n",
    "        q_pred_embeddings2 = pred_n_embeddings\n",
    "        return rew1_loss.detach(), rew2_loss.detach(), pred_loss.detach(), q_embeddings2.detach(), q_pred_embeddings2.detach(), pred_rew2_loss.detach()\n",
    "\n",
    "    def step_reward_model(self, actions:th.Tensor, observations:th.Tensor, rewards:th.Tensor, do_print:bool):\n",
    "        (emb_act, qembeddings, emb_act_goal), goals = self.qemb_qact_f_obsv(obsvs=observations, actions=actions)\n",
    "        expected_rewards = self.reward_model.forward(emb_act_goal)\n",
    "\n",
    "        loss = calcMSE(expected_rewards, rewards)\n",
    "        return loss, emb_act, qembeddings, expected_rewards, goals\n",
    "\n",
    "    def step_predictive_reward_model(self, actions:th.Tensor, embeddings:th.Tensor, rewards:th.Tensor, goals):\n",
    "        emb_act, qembeddings, emb_act_goal = self.get_q_emb_q_act(embeddings=embeddings, actions=actions, goals=goals)\n",
    "        expected_rewards = self.reward_model.forward(emb_act_goal)\n",
    "\n",
    "        loss = calcMSE(expected_rewards, rewards)\n",
    "        return loss, emb_act, qembeddings, expected_rewards\n",
    "\n",
    "    def step_predictor(self, emb_act:th.Tensor, n_embeddings:th.Tensor):\n",
    "\n",
    "        pred_n_embeddings = self.predictor(emb_act)\n",
    "        pred_loss = calcMSE(pred_n_embeddings, n_embeddings)\n",
    "\n",
    "        pred_loss = pred_loss.sum() * 100\n",
    "        return pred_loss, pred_n_embeddings\n",
    "\n",
    "    def predict_step_embedding(self, embeddings, actions, goals):\n",
    "        q_emb_q_act, _, _ = self.get_q_emb_q_act(embeddings=embeddings, actions=actions, goals=goals)\n",
    "\n",
    "        pred_n_embeddings = self.predictor(q_emb_q_act)\n",
    "        #q_pred_n_embeddings = th.round(pred_n_embeddings, decimals=self.embedding_decimals)\n",
    "        q_pred_n_embeddings = pred_n_embeddings\n",
    "        return q_pred_n_embeddings, q_emb_q_act\n",
    "\n",
    "    def pred_n_steps(self, obsv, actions):\n",
    "        rewards = []\n",
    "        embeddings = []\n",
    "        steps = actions.shape[1]\n",
    "\n",
    "        obsv = obsv.unsqueeze(1)\n",
    "\n",
    "\n",
    "        (q_emb_q_act, qembeddings, emb_act_goal), goals = self.qemb_qact_f_obsv(obsvs=obsv, actions=actions[:, :1])\n",
    "        for i in range(steps):\n",
    "            emb_act, qembeddings, emb_act_goal = self.get_q_emb_q_act(embeddings=qembeddings, actions=actions[:, i:i+1], goals=goals)\n",
    "            pred_reward = self.reward_model(emb_act_goal)\n",
    "            rewards.append(pred_reward)\n",
    "\n",
    "            q_pred_n_embeddings, _  = self.predict_step_embedding(embeddings=qembeddings, actions=actions[:, i:i+1], goals=goals)\n",
    "            embeddings.append(q_pred_n_embeddings.detach())\n",
    "            qembeddings = q_pred_n_embeddings.detach()\n",
    "\n",
    "        return rewards, embeddings\n",
    "\n",
    "    def pred_rewards(self, obsvs, actions):\n",
    "        self.eval()\n",
    "        (q_emb_q_act, qembeddings, emb_act_goals), goals = self.qemb_qact_f_obsv(obsvs=obsvs, actions=actions)\n",
    "        rewards = self.reward_model.forward(emb_act_goals)\n",
    "        return rewards\n",
    "\n",
    "    def pred_step(self, embeddings, actions, goals):\n",
    "        embeddings = self.reshape(embeddings)\n",
    "        actions = self.reshape(actions)\n",
    "        goals = self.reshape(goals)\n",
    "        n_embedding, currten_emb_act = self.predict_step_embedding(embeddings=embeddings, actions=actions, goals=goals)\n",
    "        emb_act, qembeddings, emb_act_goal = self.get_q_emb_q_act(embeddings=embeddings, actions=actions, goals=goals)\n",
    "        rewards = self.reward_model(emb_act_goal)\n",
    "        info = {}\n",
    "        #definitely a hack\n",
    "        dones = rewards == 10\n",
    "\n",
    "        #n_embedding = th.round(n_embedding, decimals=self.embedding_decimals)\n",
    "\n",
    "        return n_embedding.detach(), rewards.detach(), dones.detach(), info\n",
    "\n",
    "    def reshape(self, inpt):\n",
    "        if len(inpt.shape) == 1:\n",
    "            return inpt.reshape([1,1,-1])\n",
    "        elif len(inpt.shape) == 2:\n",
    "            return inpt.reshape([inpt.shape[0], 1, -1])\n",
    "        else:\n",
    "            return inpt\n",
    "\n",
    "    def learn(self, max_steps, rew1_thr, rew2_thr, embedd_thr, dataloader:th.utils.data.DataLoader, tboard:TBoardGraphs, steps=0):\n",
    "        self.train()\n",
    "        l2_rew1 = float('inf')\n",
    "        l2_rew2 = float('inf')\n",
    "        n_equal_embedding = float('inf')\n",
    "        \n",
    "        while (steps <= max_steps) and ((l2_rew1 > rew1_thr) or (l2_rew2 > rew2_thr) or (n_equal_embedding > embedd_thr)):\n",
    "\n",
    "            r1l = 0\n",
    "            r2l = 0\n",
    "            pr2l = 0\n",
    "            el2 = 0\n",
    "            nem = 0\n",
    "            iteration_counter = 0\n",
    "            for obsv, nobsv, action, naction, reward, nreward, done in dataloader:\n",
    "                rew1_loss, rew2_loss, pred_loss, q_embeddings2, q_pred_embeddings2, pred_rew2_loss = self.learn_step(\n",
    "                    obsvs=obsv, \n",
    "                    n_obsvs=nobsv, \n",
    "                    actions=action, \n",
    "                    n_actions=naction, \n",
    "                    rewards=reward,\n",
    "                    n_rewards=nreward,\n",
    "                    dones=done)\n",
    "                steps += obsv.shape[0]\n",
    "                iteration_counter+= 1\n",
    "                r1l = r1l + rew1_loss\n",
    "                r2l = r2l + rew2_loss\n",
    "                el2 = el2 + pred_loss\n",
    "                pr2l = pr2l + pred_rew2_loss\n",
    "\n",
    "                nee = (q_embeddings2 != q_pred_embeddings2).sum()\n",
    "                nem = nem + nee\n",
    "                \n",
    "\n",
    "            l2_rew1 = r1l / iteration_counter\n",
    "            l2_rew2 = r2l / iteration_counter\n",
    "            el2 = el2 / iteration_counter\n",
    "            pr2l = pr2l / iteration_counter\n",
    "            n_equal_embedding = nem\n",
    "            tboard.addTrainScalar('l2_rew1', value=l2_rew1.to('cpu'), stepid=steps)\n",
    "            tboard.addTrainScalar('l2_rew2', value=l2_rew2.to('cpu'), stepid=steps)\n",
    "            tboard.addTrainScalar('pr2l', value=pr2l.to('cpu'), stepid=steps)\n",
    "            tboard.addTrainScalar('pred_loss', value=el2.to('cpu'), stepid=steps)\n",
    "            tboard.addTrainScalar('lr', value=th.tensor(self.optimizer.state_dict()['param_groups'][0]['lr']), stepid=steps)\n",
    "            tboard.addTrainScalar('n_equal_embedding', n_equal_embedding.to('cpu'), stepid=steps)\n",
    "        \n",
    "class QuantzedMDP(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, ntokens_obsv, ntokens_act, obsv_low, obsv_high, action_low, action_high, device, mdpLearner:MDPLearner, seq_len=100) -> None:\n",
    "        super().__init__(env)\n",
    "        self.ntokens_obsv= ntokens_obsv\n",
    "        self.ntokens_act = ntokens_act\n",
    "\n",
    "        lowq = self.observation_space.low\n",
    "        lowq = np.append(lowq, 0)\n",
    "        highq = self.observation_space.low\n",
    "        highq = np.append(highq, 1)\n",
    "\n",
    "        observation_space = gym.spaces.Box(low=lowq, high=highq)\n",
    "        self.observation_space = observation_space\n",
    "\n",
    "        min_obsv = self.observation_space.low\n",
    "        min_obsv = np.maximum(min_obsv, obsv_low)\n",
    "        self.min_obsv = th.tensor(min_obsv)\n",
    "        max_obsv = self.observation_space.high\n",
    "        max_obsv = np.minimum(max_obsv, obsv_high)\n",
    "        self.max_obsv = th.tensor(max_obsv)\n",
    "\n",
    "        min_action = self.action_space.low\n",
    "        min_action = np.maximum(min_action, action_low)\n",
    "        self.min_action = th.tensor(min_action)\n",
    "        max_action = self.action_space.high\n",
    "        max_action = np.minimum(max_action, action_high)\n",
    "        self.max_action = th.tensor(max_action)\n",
    "\n",
    "        self.max_recoreded_obsv = -float(\"inf\")\n",
    "        self.min_recoreded_obsv = float(\"inf\")\n",
    "\n",
    "        self.replay_data = MDPData(device)\n",
    "\n",
    "        self.current_obsv = None\n",
    "        self.first_observations = []\n",
    "\n",
    "        self.predict_MDP = False\n",
    "\n",
    "        self.mdpLearner = mdpLearner        \n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.current_step = 0\n",
    "\n",
    "    def quantize(self, inpt, min, max, ntokens):\n",
    "        return inpt\n",
    "        '''th_inpt = th.tensor(inpt).reshape([1,1,-1])\n",
    "        th_inpt = tokenize(inpt=th_inpt, minimum=min, maximum=max, ntokens=ntokens)\n",
    "        th_inpt = detokenize(inpt=th_inpt, minimum=min, maximum=max, ntokens=ntokens)\n",
    "        return th_inpt.numpy().squeeze()'''\n",
    "\n",
    "    def reset(self) -> Any:\n",
    "        self.current_step = 0\n",
    "\n",
    "        if self.predict_MDP:\n",
    "            obsv = np.copy(random.choice(self.first_observations))\n",
    "            assert len(obsv.shape) == 1\n",
    "            goal, pure_obsv = self.mdpLearner.get_goal(obsv)\n",
    "            self.current_goal = th.tensor(goal, dtype=th.float, device=self.mdpLearner.device)\n",
    "            current_embedding = self.mdpLearner.emitter(th.tensor(pure_obsv, dtype=th.float, device=self.mdpLearner.device)).squeeze()\n",
    "            self.current_embedding = current_embedding.detach()\n",
    "\n",
    "            obsv = np.zeros([*obsv.shape], dtype=np.float).squeeze()\n",
    "            obsv[:self.current_embedding.shape[0]] = current_embedding.detach().cpu().numpy()\n",
    "            obsv[-4:-1] = goal\n",
    "            obsv[-1] = 1\n",
    "\n",
    "        else:\n",
    "            obsv = super().reset()\n",
    "            #obsv = self.quantize(inpt=obsv, min=self.min_obsv, max=self.max_obsv, ntokens=self.ntokens_obsv)\n",
    "            obsv = np.append(obsv, 0)\n",
    "            self.current_obsv = obsv\n",
    "            self.first_observations.append(obsv)\n",
    "\n",
    "        return obsv\n",
    "\n",
    "    def step(self, action):\n",
    "        q_act = self.quantize(inpt=action, min=self.min_action, max=self.max_action, ntokens=self.ntokens_act)\n",
    "\n",
    "        if self.predict_MDP:\n",
    "            obsv = th.zeros([self.observation_space.shape[0]], dtype=th.float, device=self.mdpLearner.device)\n",
    "            q_act = th.tensor(q_act, dtype=th.float, device=self.mdpLearner.device)\n",
    "            emb_obsv, reward, dones, info  = self.mdpLearner.pred_step(embeddings=self.current_embedding, actions=q_act, goals=self.current_goal)\n",
    "            self.current_step += 1\n",
    "            dones = dones or (self.current_step >= (self.seq_len))\n",
    "            obsv[:emb_obsv.shape[-1]] = emb_obsv.squeeze()\n",
    "            self.current_embedding = emb_obsv.squeeze().detach()\n",
    "            obsv[-4:-1] = self.current_goal\n",
    "            obsv[-1] = 1\n",
    "            obsv = obsv.detach().to('cpu').numpy()\n",
    "            reward = float(reward.detach().to('cpu').numpy())\n",
    "            reward = min(10, max(0, reward))\n",
    "\n",
    "        else:\n",
    "            obsv, reward, dones, info = super().step(q_act)\n",
    "            self.current_step += 1\n",
    "            dones = (reward == 10) or (self.current_step >= (self.seq_len))\n",
    "            obsv = np.append(obsv, 0)\n",
    "            \n",
    "            self.replay_data.add_step(th.tensor(self.current_obsv, dtype=th.float), th.tensor(q_act, dtype=th.float), th.tensor(reward, dtype=th.float), th.tensor(dones, dtype=th.bool))\n",
    "            self.current_obsv = obsv\n",
    "        \n",
    "        return obsv, reward, dones, info\n",
    "\n",
    "    def learn(self, max_steps: int, rew1_thr: float, rew2_thr: float, embedd_thr: float, dataloader: Any, tboard: TBoardGraphs):\n",
    "        self.mdpLearner.learn(max_steps=max_steps, rew1_thr=rew1_thr, rew2_thr=rew2_thr, embedd_thr=embedd_thr, dataloader=dataloader, tboard=tboard)\n",
    "\n",
    "    def set_predictive_mode(self, predict:bool, sac:SAC):\n",
    "        with th.no_grad():\n",
    "            sac.policy.actor.features_extractor.emitter = self.mdpLearner.emitter\n",
    "            sac.policy.critic.features_extractor.emitter = self.mdpLearner.emitter\n",
    "            sac.policy.critic_target.features_extractor.emitter = self.mdpLearner.emitter\n",
    "            if predict:\n",
    "                self.predict_MDP = True\n",
    "            else:\n",
    "                self.predict_MDP = False\n",
    "\n",
    "    def set_initial_phase(self, sac, ip):\n",
    "        if ip:\n",
    "            sac.policy.actor.features_extractor.initial_phase = True\n",
    "            sac.policy.critic.features_extractor.initial_phase = True\n",
    "            sac.policy.critic_target.features_extractor.initial_phase = True\n",
    "        else:\n",
    "            sac.policy.actor.features_extractor.initial_phase = False\n",
    "            sac.policy.critic.features_extractor.initial_phase = False\n",
    "            sac.policy.critic_target.features_extractor.initial_phase = False   \n",
    "\n",
    "def gradient_max(input):\n",
    "    max_indices = th.max(input, dim=-1)[1]\n",
    "    one_hot = th.nn.functional.one_hot(max_indices, num_classes=input.shape[-1])\n",
    "    with th.no_grad():\n",
    "        input -= input\n",
    "        input += one_hot\n",
    "    return input\n",
    "\n",
    "def make_mdp_data(env:QuantzedMDP, device:str):\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    actions_size = env.action_space.shape[0]\n",
    "    observation_size = env.observation_space.shape[0]\n",
    "\n",
    "    observations = th.ones([batch_size, seq_len, observation_size], dtype=th.float, device=device)\n",
    "    observations[0, 1:] = 0\n",
    "    observations[0, -1] = 1\n",
    "\n",
    "    actions = th.ones([batch_size, seq_len, actions_size], dtype=th.float, device=device)\n",
    "    actions[0] = 0\n",
    "    actions[0, -2] = 1\n",
    "\n",
    "    rewards = th.ones([batch_size, seq_len, 1], dtype=th.float, device=device)\n",
    "    rewards[1] = 0\n",
    "    rewards[0,0] = 1\n",
    "    rewards[0, 1:] = 2\n",
    "    rewards[0, -2] = 3\n",
    "    rewards[0, -1] = 1\n",
    "\n",
    "    dones = th.zeros([batch_size, seq_len], dtype=bool, device=device)\n",
    "    dones[:, -1] = 1\n",
    "\n",
    "    n_observations = th.zeros_like(observations)\n",
    "    n_observations[:, :-1] = observations[:, 1:]\n",
    "\n",
    "    n_rewards = th.zeros_like(rewards)\n",
    "    n_rewards[:, :-1] = rewards[:, 1:]\n",
    "\n",
    "    n_actions = th.zeros_like(actions)\n",
    "    n_actions[:, :-1] = actions[:, 1:]\n",
    "\n",
    "    return observations, n_observations, actions, n_actions, rewards, n_rewards, dones\n",
    "\n",
    "def init_policy(model:SAC):\n",
    "    for module in model.policy.critic.qf0:\n",
    "        model.policy.init_weights(module)\n",
    "    for module in model.policy.critic.qf1:\n",
    "        model.policy.init_weights(module)\n",
    "\n",
    "    for module in model.policy.critic_target.qf0:\n",
    "        model.policy.init_weights(module)\n",
    "    for module in model.policy.critic_target.qf1:\n",
    "        model.policy.init_weights(module)\n",
    "    for module in model.policy.actor.latent_pi:\n",
    "        model.policy.init_weights(module)\n",
    "    model.policy.init_weights(model.policy.actor.mu)\n",
    "\n",
    "def test_policy(model, env, iterations):\n",
    "    rews = []\n",
    "    for i in range(iterations):\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            if type(reward) is not float:\n",
    "                rews.append(reward.detach().cpu().numpy())\n",
    "            else:\n",
    "                rews.append(reward)\n",
    "\n",
    "\n",
    "            #env.render()\n",
    "            if done:\n",
    "                break\n",
    "    rews = np.array(rews)\n",
    "    return rews\n",
    "\n",
    "def test_policy_initial_state(model, envs, obsvs):\n",
    "    rews = []\n",
    "    for i in range(len(envs)):\n",
    "        obs = obsvs[i]\n",
    "        while True:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = envs[i].step(action)\n",
    "            if type(reward) is not float:\n",
    "                rews.append(reward.detach().cpu().numpy())\n",
    "            else:\n",
    "                rews.append(reward)\n",
    "\n",
    "\n",
    "            #env.render()\n",
    "            if done:\n",
    "                break\n",
    "    rews = np.array(rews)\n",
    "    return rews\n",
    "\n",
    "\n",
    "def test_gradient_steps_SAC(model:SAC, tboard:TBoardGraphs, env:QuantzedMDP, eval_env:QuantzedMDP, predict):\n",
    "    gradient_stepsize = 1000\n",
    "    gradient_steps = gradient_stepsize\n",
    "    env.set_initial_phase(model, False)\n",
    "    for i in range(15):\n",
    "        print(i)\n",
    "        init_policy(model=model)\n",
    "        env.set_predictive_mode(predict=predict, sac=model)\n",
    "        model.train(gradient_steps=gradient_steps, batch_size=model.batch_size)\n",
    "        print('_______________')\n",
    "        eval_env.set_initial_phase(model, False)\n",
    "        eval_env.set_predictive_mode(False, model)\n",
    "        rews = test_policy(model=model, env=eval_env, iterations=40)\n",
    "        rews = th.tensor(rews.mean())\n",
    "        tboard.addValidationScalar(name='Rewards Gradienttest', value=rews, stepid=gradient_steps)\n",
    "        gradient_steps += gradient_stepsize\n",
    "        \n",
    "def test_SAC(env, eval_env, eval_epochs, iterations, path, logname, device, model = None, lf=10):\n",
    "    tb = TBoardGraphs(logname=logname, data_path=path)\n",
    "\n",
    "    if model is None:\n",
    "        pkwarg = dict(net_arch=[512, 512, 512])\n",
    "        pkwarg = dict(features_extractor_class=TestExtractor)\n",
    "        model = SAC(\"MlpPolicy\", env=env, verbose=1, policy_kwargs=pkwarg, device=device)\n",
    "\n",
    "    #env.set_predictive_mode(predict=False, sac=model)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        rews = []\n",
    "        for eval_run in range(eval_epochs):\n",
    "            obs = eval_env.reset()\n",
    "            while True:\n",
    "                action, _states = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, info = eval_env.step(action)\n",
    "                rews.append(reward)\n",
    "                #env.render()\n",
    "                if done:\n",
    "                    break\n",
    "        rews_np = np.array(rews)\n",
    "        tb.addValidationScalar(name='Average Reward', value=th.tensor(rews_np.mean()), stepid=iteration)\n",
    "        #env.set_predictive_mode(predict=pmdp, sac=model)\n",
    "\n",
    "        model.learn(total_timesteps=100*lf, log_interval=100000)\n",
    "        model.save(logname)\n",
    "    return model\n",
    "\n",
    "def createGraphs(tboard, trjs:list([th.tensor]), trj_names:list([str]), plot_name:str, step):\n",
    "    np_trjs = []\n",
    "    trj_colors = ['forestgreen', 'orange', 'pink']\n",
    "    for trj in trjs:\n",
    "        np_trjs.append(trj.detach().cpu().numpy())\n",
    "    tboard.plot_graph(trjs=np_trjs, trj_names=trj_names, trj_colors=trj_colors, plot_name=plot_name, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "env_id = 'reach'\n",
    "policy_dict = make_policy_dict()\n",
    "device = 'cuda'\n",
    "max_episode_steps = seq_len\n",
    "env = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_id][1]]()\n",
    "env._freeze_rand_vec = False\n",
    "env = TimeLimit(env=env, max_episode_steps=max_episode_steps)\n",
    "\n",
    "lowq = env.observation_space.low\n",
    "lowq = np.append(lowq, 0)\n",
    "highq = env.observation_space.low\n",
    "highq = np.append(highq, 1)\n",
    "\n",
    "observation_space = gym.spaces.Box(low=lowq, high=highq)\n",
    "\n",
    "min_obsv = observation_space.low\n",
    "min_obsv = np.maximum(min_obsv, -1)\n",
    "min_obsv = th.tensor(min_obsv)\n",
    "max_obsv = observation_space.high\n",
    "max_obsv = np.minimum(max_obsv, 1)\n",
    "max_obsv = th.tensor(max_obsv)\n",
    "\n",
    "action_minimum = th.tensor(np.maximum(env.action_space.low, -1))\n",
    "action_maximum = th.tensor(np.minimum(env.action_space.high, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_learner = MDPLearner(\n",
    "    embbed_size=20,\n",
    "    ntokens_obsv=1000,\n",
    "    ntokens_act=1000,\n",
    "    observation_space=observation_space,\n",
    "    action_space=env.action_space,\n",
    "    min_obsv=min_obsv,\n",
    "    max_obsv=max_obsv,\n",
    "    min_action=action_minimum,\n",
    "    max_action=action_maximum,\n",
    "    embedding_decimals=100,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv= QuantzedMDP(env=env, ntokens_obsv=1000, ntokens_act=1000, obsv_low=-1, obsv_high=1, action_low=-1, action_high=1, device=device, mdpLearner=mdp_learner)\n",
    "qenv_eval= QuantzedMDP(env=env, ntokens_obsv=1000, ntokens_act=1000, obsv_low=-1, obsv_high=1, action_low=-1, action_high=1, device=device, mdpLearner=mdp_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env = test_SAC(env=qenv, eval_env=qenv_eval, eval_epochs=40, iterations=10, logname='reach 1000', path='/data/bing/hendrik/', model=None, device=device, lf=100)\n",
    "dataloader = th.utils.data.DataLoader(dataset=qenv.replay_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TBoardGraphs(logname='mdp test reach 1000', data_path='/data/bing/hendrik/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_learner.learn(max_steps=1e7, rew1_thr=1, rew2_thr=1, embedd_thr=0, tboard=tb, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_learner.optimizer = th.optim.Adam(params=list(mdp_learner.emitter.parameters()) + list(mdp_learner.predictor.parameters())+ list(mdp_learner.reward_model.parameters()), lr=1e-4)\n",
    "mdp_learner.scheduler = th.optim.lr_scheduler.StepLR(optimizer=mdp_learner.optimizer, step_size=300000/32, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [517], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mdp_learner\u001b[39m.\u001b[39;49mlearn(max_steps\u001b[39m=\u001b[39;49m\u001b[39m3e7\u001b[39;49m, rew1_thr\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, rew2_thr\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, embedd_thr\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, tboard\u001b[39m=\u001b[39;49mtb, dataloader\u001b[39m=\u001b[39;49mdataloader)\n",
      "Cell \u001b[0;32mIn [514], line 340\u001b[0m, in \u001b[0;36mMDPLearner.learn\u001b[0;34m(self, max_steps, rew1_thr, rew2_thr, embedd_thr, dataloader, tboard)\u001b[0m\n\u001b[1;32m    338\u001b[0m iteration_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    339\u001b[0m \u001b[39mfor\u001b[39;00m obsv, nobsv, action, naction, reward, nreward, done \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m--> 340\u001b[0m     rew1_loss, rew2_loss, pred_loss, q_embeddings2, q_pred_embeddings2, pred_rew2_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn_step(\n\u001b[1;32m    341\u001b[0m         obsvs\u001b[39m=\u001b[39;49mobsv, \n\u001b[1;32m    342\u001b[0m         n_obsvs\u001b[39m=\u001b[39;49mnobsv, \n\u001b[1;32m    343\u001b[0m         actions\u001b[39m=\u001b[39;49maction, \n\u001b[1;32m    344\u001b[0m         n_actions\u001b[39m=\u001b[39;49mnaction, \n\u001b[1;32m    345\u001b[0m         rewards\u001b[39m=\u001b[39;49mreward,\n\u001b[1;32m    346\u001b[0m         n_rewards\u001b[39m=\u001b[39;49mnreward,\n\u001b[1;32m    347\u001b[0m         dones\u001b[39m=\u001b[39;49mdone)\n\u001b[1;32m    348\u001b[0m     steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m obsv\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    349\u001b[0m     iteration_counter\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn [514], line 230\u001b[0m, in \u001b[0;36mMDPLearner.learn_step\u001b[0;34m(self, obsvs, n_obsvs, actions, n_actions, rewards, n_rewards, dones)\u001b[0m\n\u001b[1;32m    226\u001b[0m nd_emb_act1 \u001b[39m=\u001b[39m emb_act1[\u001b[39m~\u001b[39mdones]\n\u001b[1;32m    228\u001b[0m pred_loss, pred_n_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_predictor(emb_act\u001b[39m=\u001b[39mnd_emb_act1, n_embeddings\u001b[39m=\u001b[39mq_embeddings2)\n\u001b[0;32m--> 230\u001b[0m pred_rew2_loss, _, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_predictive_reward_model(actions\u001b[39m=\u001b[39;49mnd_n_actions, embeddings\u001b[39m=\u001b[39;49mpred_n_embeddings, rewards\u001b[39m=\u001b[39;49mnd_nrewards, goals\u001b[39m=\u001b[39;49mngoals)\n\u001b[1;32m    233\u001b[0m loss \u001b[39m=\u001b[39m rew1_loss \u001b[39m+\u001b[39m rew2_loss \u001b[39m+\u001b[39m pred_loss \u001b[39m+\u001b[39m pred_rew2_loss\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn [514], line 253\u001b[0m, in \u001b[0;36mMDPLearner.step_predictive_reward_model\u001b[0;34m(self, actions, embeddings, rewards, goals)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_predictive_reward_model\u001b[39m(\u001b[39mself\u001b[39m, actions:th\u001b[39m.\u001b[39mTensor, embeddings:th\u001b[39m.\u001b[39mTensor, rewards:th\u001b[39m.\u001b[39mTensor, goals):\n\u001b[0;32m--> 253\u001b[0m     emb_act, qembeddings, emb_act_goal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_q_emb_q_act(embeddings\u001b[39m=\u001b[39;49membeddings, actions\u001b[39m=\u001b[39;49mactions, goals\u001b[39m=\u001b[39;49mgoals)\n\u001b[1;32m    254\u001b[0m     expected_rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_model\u001b[39m.\u001b[39mforward(emb_act_goal)\n\u001b[1;32m    256\u001b[0m     loss \u001b[39m=\u001b[39m calcMSE(expected_rewards, rewards)\n",
      "Cell \u001b[0;32mIn [514], line 194\u001b[0m, in \u001b[0;36mMDPLearner.get_q_emb_q_act\u001b[0;34m(self, embeddings, actions, goals)\u001b[0m\n\u001b[1;32m    192\u001b[0m qembeddings \u001b[39m=\u001b[39m embeddings\n\u001b[1;32m    193\u001b[0m emb_act \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat((qembeddings, qactions), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m emb_act_goal \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39;49mcat((emb_act, goals), dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m emb_act, qembeddings, emb_act_goal\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mdp_learner.learn(max_steps=3e7, rew1_thr=1, rew2_thr=1, embedd_thr=0, tboard=tb, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.MDPLearner'>: it's not the same object as __main__.MDPLearner",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [515], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m th\u001b[39m.\u001b[39;49msave(mdp_learner, \u001b[39m'\u001b[39;49m\u001b[39m/data/bing/hendrik/mdp1000\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/torch/serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    378\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 379\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    380\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/torch/serialization.py:589\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    587\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    588\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[0;32m--> 589\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    590\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    591\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.MDPLearner'>: it's not the same object as __main__.MDPLearner"
     ]
    }
   ],
   "source": [
    "th.save(mdp_learner, '/data/bing/hendrik/mdp1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(mdp_learner, '/data/bing/hendrik/reach100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_prediction_and_simulation(qenv, tb, step):\n",
    "    eval_qenv_1 = copy.deepcopy(qenv)\n",
    "    qenv.set_predictive_mode(predict=True, sac=model)\n",
    "\n",
    "    qenv.replay_data = MDPData(device=device)\n",
    "    eval_qenv_1.replay_data = MDPData(device=device)\n",
    "\n",
    "    obsv = eval_qenv_1.reset()\n",
    "    eval_qenv_2 = copy.deepcopy(eval_qenv_1)\n",
    "    eval_qenv_2.replay_data = MDPData(device=device)\n",
    "\n",
    "    init_obsv = np.copy(obsv)\n",
    "    qenv.first_observations = [np.copy(obsv)]\n",
    "\n",
    "    virtual_obsv = qenv.reset()\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(virtual_obsv)\n",
    "        actions.append(action)\n",
    "        virtual_obsv, rew, done, info = qenv.step(action)\n",
    "        rewards.append(rew)\n",
    "        \n",
    "    eval_qenv_1.set_predictive_mode(False, model)\n",
    "    \n",
    "    sim_rew = []\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        obsv, rew, done, info = eval_qenv_1.step(actions[i])\n",
    "        sim_rew.append(rew)\n",
    "        i += 1\n",
    "\n",
    "    eval_qenv_2.set_predictive_mode(False, model)\n",
    "\n",
    "    sim_rew_in_the_loop = []\n",
    "    actions_in_the_loop = []\n",
    "    done = False\n",
    "    obsv = init_obsv\n",
    "    while not done:\n",
    "        action, _ = model.predict(obsv)\n",
    "        obsv, rew, done, info = eval_qenv_2.step(action)\n",
    "        sim_rew_in_the_loop.append(rew)\n",
    "        actions_in_the_loop.append(action)\n",
    "\n",
    "    predicted_rewards = th.tensor(np.array(rewards)).reshape([-1, 1])\n",
    "    actual_rewards = th.tensor(np.array(sim_rew)).reshape([-1, 1])\n",
    "    in_the_loop_rewards = th.tensor(np.array(sim_rew_in_the_loop)).reshape([-1, 1])\n",
    "    actions = th.tensor(np.array(actions)).reshape([-1, 4])\n",
    "    actions_in_the_loop = th.tensor(np.array(actions_in_the_loop)).reshape([-1, 4])\n",
    "\n",
    "    createGraphs(tb, trjs=[actions, actions_in_the_loop], trj_names=['actions', 'actions_in_the_loop'], plot_name='Actions', step=step)\n",
    "    createGraphs(tb, trjs=[predicted_rewards, actual_rewards, in_the_loop_rewards], trj_names=['predicted_rewards', 'actual_rewards', 'in_the_loop_rewards'], plot_name='Rewards', step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv_dc = copy.deepcopy(qenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv.mdpLearner = th.load('/data/bing/hendrik/reach100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_prediction_and_simulation(qenv=qenv, tb=tb, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qenv.first_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.replay_buffer.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrd = copy.deepcopy(model.replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    model.replay_buffer = mrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv_eval.set_predictive_mode(False, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.replay_buffer.observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gradient_steps_SAC(model=model, tboard=tb, env=qenv, eval_env=qenv_eval, predict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TBoardGraphs(logname='mdp test reach 100 + 40', data_path='/data/bing/hendrik/gboard/a/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv.set_initial_phase(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv_eval.set_initial_phase(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gradient_steps_SAC(model=model, tboard=tb, env=qenv, eval_env=qenv_eval, predict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv.set_initial_phase(model, False)\n",
    "qenv.set_predictive_mode(True, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_SAC(env=qenv, eval_env=None, eval_epochs=0, iterations=1, path = '/data/bing/hendrik/', logname='test', device=device, model=model, lf=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv_eval.set_predictive_mode(False, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gradient_steps_SAC(model=model, tboard=tb, env=qenv, eval_env=qenv_eval, predict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_policy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.replay_buffer.pos = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv = qenv_eval.reset()\n",
    "copy_obsv = copy.deepcopy(obsv)\n",
    "dcqee = copy.deepcopy(qenv_eval)\n",
    "dcqee2 = copy.deepcopy(qenv_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews = []\n",
    "done = False\n",
    "while not done:\n",
    "    actions, _ = model.predict(obsv)\n",
    "    obsv, rew, done, info = qenv_eval.step(actions)\n",
    "    rews.append(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rews).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv.first_observations = [copy_obsv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv.set_predictive_mode(True, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_SAC(env=qenv, eval_env=None, eval_epochs=0, iterations=1, path='/data/bing/hendrik/', logname='asd', device=device, model=model, lf = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_policy(model=model)\n",
    "model.train(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    dcqee.mdpLearner = qenv.mdpLearner\n",
    "    dcqee2.mdpLearner = qenv.mdpLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcqee.set_predictive_mode(False, model)\n",
    "rews = []\n",
    "done = False\n",
    "obsv = copy_obsv\n",
    "while not done:\n",
    "    actions, _ = model.predict(obsv)\n",
    "    obsv, rew, done, info = dcqee.step(actions)\n",
    "    rews.append(rew)\n",
    "np.array(rews).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv = copy_obsv\n",
    "with th.no_grad():\n",
    "    dcqee2.current_embedding = dcqee2.mdpLearner.emitter(th.tensor(obsv[...,:-4], dtype=th.float, device=device))\n",
    "    dcqee2.current_goal = th.tensor(obsv[...,-4:-1], dtype=th.float, device=device)\n",
    "dcqee2.set_predictive_mode(True, model)\n",
    "rews = []\n",
    "done = False\n",
    "obsv = copy_obsv\n",
    "while not done:\n",
    "    actions, _ = model.predict(obsv)\n",
    "    obsv, rew, done, info = dcqee2.step(actions)\n",
    "    rews.append(rew)\n",
    "np.array(rews).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcqee2.predict_MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv.set_predictive_mode(False, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv.set_predictive_mode(predict=True, sac=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv_eval.replay_data = MDPData(device=device)\n",
    "qenv.replay_data = MDPData(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv = qenv_eval.reset()\n",
    "init_obsv = np.copy(obsv)\n",
    "qenv.first_observations = [np.copy(obsv)]\n",
    "qenv_eval_copy = copy.deepcopy(qenv_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_obsv = qenv.reset()\n",
    "actions = []\n",
    "rewards = []\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(virtual_obsv)\n",
    "    actions.append(action)\n",
    "    virtual_obsv, rew, done, info = qenv.step(action)\n",
    "    rewards.append(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv_eval.set_predictive_mode(True, model)\n",
    "qenv_eval.predict_MDP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_rew = []\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    obsv, rew, done, info = qenv_eval.step(actions[i])\n",
    "    sim_rew.append(rew)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_rew_in_the_loop = []\n",
    "actions_in_the_loop = []\n",
    "done = False\n",
    "obsv = init_obsv\n",
    "while not done:\n",
    "    action, _ = model.predict(obsv)\n",
    "    obsv, rew, done, info = qenv_eval_copy.step(action)\n",
    "    sim_rew_in_the_loop.append(rew)\n",
    "    actions_in_the_loop.append(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
