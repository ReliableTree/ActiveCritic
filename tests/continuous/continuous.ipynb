{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import sample_expert_transitions, DummyExtractor, make_policy_dict, parse_sampled_transitions\n",
    "import torch as th\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import gym\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.policies import BaseModel\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from gym.wrappers import TimeLimit\n",
    "from gym import Env\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "env_id = 'push'\n",
    "policy_dict = make_policy_dict()\n",
    "max_episode_steps = seq_len\n",
    "env = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_id][1]]()\n",
    "env._freeze_rand_vec = False\n",
    "env = TimeLimit(env=env, max_episode_steps=max_episode_steps)\n",
    "env = RolloutInfoWrapper(env)\n",
    "dv1 = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_neurons, use_layer_norm=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "\n",
    "        # create a sequence of linear layers with the specified number of neurons\n",
    "        self.layers = nn.Sequential(\n",
    "            *[nn.Linear(input_size if i == 0 else num_neurons[i-1], num_neurons[i]) for i in range(len(num_neurons))]\n",
    "        )\n",
    "\n",
    "        # if using layer normalization, create a layer normalization layer for each linear layer\n",
    "        if self.use_layer_norm:\n",
    "            self.layer_norms = nn.ModuleList(\n",
    "                [nn.LayerNorm(num_neurons[i]) for i in range(len(num_neurons))]\n",
    "            )\n",
    "\n",
    "        # create the output layer\n",
    "        self.output_layer = nn.Linear(num_neurons[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply the linear layers and optional layer normalization\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if self.use_layer_norm:\n",
    "                x = self.layer_norms[i](x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        # apply the output layer and return the result\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Hopper-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPLearner(gym.Wrapper): #Wrapper?\n",
    "    def __init__(self, env: Env) -> None:\n",
    "        super().__init__(env)\n",
    "        self.obsv_predictor = MLP(input_size=env.action_space.shape[0] + env.observation_space.shape[0] , output_size=env.observation_space.shape[0], num_neurons=[64, 64, 64])\n",
    "        self.rew_predictor = MLP(input_size=env.action_space.shape[0] + env.observation_space.shape[0] , output_size=1, num_neurons=[64, 64, 64])\n",
    "        self.optimizer = torch.optim.Adam(params=\n",
    "            [\n",
    "                {\"params\": self.obsv_predictor.parameters()},\n",
    "                {\"params\": self.rew_predictor.parameters()},\n",
    "            ]\n",
    "            , lr = 1e-3\n",
    "        )\n",
    "\n",
    "    def predict(self, obsv:th.Tensor, action:th.Tensor, predictor:nn.Module):\n",
    "        input = th.cat((obsv, action), dim=-1)\n",
    "        prediction = predictor.forward(input)\n",
    "        return prediction\n",
    "\n",
    "    def learn_step(self, obsv1, obsv2, rew1, rew2, action1, action2):\n",
    "        pred_reward1 = self.predict(obsv=obsv1, action=action1, predictor=self.rew_predictor)\n",
    "        pred_reward2 = self.predict(obsv=obsv2, action=action2, predictor=self.rew_predictor)\n",
    "\n",
    "        pred_obsv = self.predict(obsv=obsv1, action=action1, predictor=self.obsv_predictor)\n",
    "\n",
    "        loss_rew_1 = ((pred_reward1.reshape(-1)-rew1.reshape(-1))**2).mean()\n",
    "        loss_rew_2 = ((pred_reward2.reshape(-1)-rew2.reshape(-1))**2).mean()\n",
    "\n",
    "        loss_observation = ((pred_obsv.reshape(-1)-obsv2.reshape(-1))**2).mean()\n",
    "\n",
    "        loss = loss_rew_1+loss_rew_2+loss_observation\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss_rew_1, loss_rew_2, loss_observation\n",
    "\n",
    "    def learn(self, obsvs, actions, rewards):\n",
    "        pass\n",
    "\n",
    "    def step(self, action):\n",
    "        next_observation = self.predict(obsv=self.current_obsv, action=action, predictor=self.obsv_predictor)\n",
    "        next_observation = next_observation.numpy()\n",
    "        self.current_obsv = next_observation\n",
    "        self.actions.append(action)\n",
    "        return next_observation\n",
    "\n",
    "    def reset(self) -> Any:\n",
    "        obsv = super().reset()\n",
    "        self.current_obsv = obsv\n",
    "        self.actions = []\n",
    "        return obsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "a = th.ones([3])\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPLearner(gym.Wrapper): #Wrapper?\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        #obsv space\n",
    "        #action space\n",
    "\n",
    "    def emit(self, obsv):\n",
    "        pass\n",
    "        #emitter forward\n",
    "\n",
    "    def predict_emb(self, embedding, action):\n",
    "        pass\n",
    "        #predicter forward\n",
    "\n",
    "    def pred_rew(self, embedding):\n",
    "        pass\n",
    "        #reward_predictor forward\n",
    "\n",
    "    def learn(self, obsvs1, obsvs2, reward1, reward2, actions):\n",
    "        pass\n",
    "        #emit(obsv1) -> reward1*\n",
    "        #emit(obsv1) -> actions -> predictions -> reward2*\n",
    "        #emit(obsv2) -> reward2**\n",
    "        #L1 = emit(obsv1) - emit(obsv2)\n",
    "        #L2 = reward1* - reward1\n",
    "        #L2 = reward2* - reward2\n",
    "        #L3 = reward2 ** - reward2\n",
    "\n",
    "    def step(self, action):\n",
    "        pass\n",
    "        #obsv = self.env.step(action)\n",
    "        #return self.emit(obsv)\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "        #self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_push = SAC(\"MlpPolicy\", env, verbose=1, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"Hopper-v3\")\n",
    "\n",
    "model_push = SAC(\"MlpPolicy\", dv1, verbose=1)\n",
    "model_push.learn(total_timesteps=100*100, log_interval=4)\n",
    "model_push.save(\"sac_push\")\n",
    "\n",
    "model = SAC.load(\"sac_push\")\n",
    "\n",
    "'''while True:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model_push = SAC(\"MlpPolicy\", dv1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Hopper-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = dv1.reset()\n",
    "rews = []\n",
    "while True:\n",
    "    action, _states = model_push.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = dv1.step(action)\n",
    "    rews.append(reward)\n",
    "    #env.render()\n",
    "    if done:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling transitions. 10\n"
     ]
    }
   ],
   "source": [
    "transitions = sample_expert_transitions(policy=model_push.predict, env=dv1, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, observations, rewards = parse_sampled_transitions(transitions=transitions, extractor=DummyExtractor(), seq_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 39])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews_np = np.array(rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews_np[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rews_np[1:] - rews_np[:-1]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env):\n",
    "    def _init():\n",
    "        env._freeze_rand_vec = False\n",
    "        #rce = ResetCounterWrapper(env)\n",
    "        riw = RolloutInfoWrapper(env)\n",
    "        return riw\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vec_env_gym(env, num_cpu, seq_len):\n",
    "    env = SubprocVecEnv([make_env(env) for i in range(num_cpu)])\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = make_vec_env_gym(gym.make('Hopper-v3'), 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv = vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGymPolicy(BaseModel):\n",
    "    def __init__(self, observation_space: gym.spaces.Space, action_space: gym.spaces.Space, features_extractor_class: Type[BaseFeaturesExtractor] = ..., features_extractor_kwargs: Optional[Dict[str, Any]] = None, features_extractor: Optional[nn.Module] = None, normalize_images: bool = True, optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam, optimizer_kwargs: Optional[Dict[str, Any]] = None):\n",
    "        super().__init__(observation_space, action_space, features_extractor_class, features_extractor_kwargs, features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        observation: Union[th.Tensor, Dict[str, th.Tensor]],\n",
    "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
    "        episode_start: Optional[np.ndarray] = None,\n",
    "        deterministic: bool = False,\n",
    "    ) -> th.Tensor:\n",
    "\n",
    "        result = np.array([[0,0,0]]*observation.shape[0])\n",
    "        result[episode_start] = 1\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsisode_start = np.array([True, True, False])\n",
    "dgp = DummyGymPolicy(vec_env.observation_space, vec_env.action_space, features_extractor=DummyExtractor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = dgp.predict(observation=obsv, episode_start=epsisode_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = sample_expert_transitions(policy=dgp.predict, env=vec_env, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = 0\n",
    "for i, transition in enumerate(transitions):\n",
    "    if transition['dones']:\n",
    "        tf += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttf = 0\n",
    "for act in transitions:\n",
    "    if act['acts'][0] == 1:\n",
    "        ttf+=1\n",
    "ttf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in transitions[21]['infos']['rollout']:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.model_src.transformer import *\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 6\n",
    "batch_size = 2\n",
    "dim = 3\n",
    "\n",
    "tms = ModelSetup()\n",
    "tms.d_hid = 12\n",
    "tms.d_model = 12\n",
    "tms.d_output = 2\n",
    "tms.device = 'cuda'\n",
    "tms.nhead = 1\n",
    "tms.nlayers = 2\n",
    "tms.seq_len = seq_len\n",
    "\n",
    "tm = TransformerModel(model_setup=tms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = th.ones([batch_size, seq_len, dim], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tm.forward(inpt, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
