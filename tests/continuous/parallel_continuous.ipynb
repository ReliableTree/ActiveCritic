{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "2022-12-18 17:19:02.354291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from active_critic.utils.gym_utils import sample_expert_transitions\n",
    "import torch as th\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import gym\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.policies import BaseModel\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "from active_critic.utils.gym_utils import DummyExtractor\n",
    "import numpy as np\n",
    "from active_critic.utils.gym_utils import make_policy_dict\n",
    "from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from gym.wrappers import TimeLimit\n",
    "from active_critic.utils.pytorch_utils import detokenize, tokenize, calcMSE\n",
    "from stable_baselines3 import SAC\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "import stable_baselines3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestExtractor(stable_baselines3.common.torch_layers.FlattenExtractor):\n",
    "    def __init__(self, observation_space) -> None:\n",
    "        super().__init__(observation_space)\n",
    "        self._features_dim = observation_space.shape[0]\n",
    "        self.switch = True\n",
    "\n",
    "    def forward(self, observations):\n",
    "        ext_obsv = super().forward(observations)\n",
    "        if self.switch:\n",
    "            ext_obsv = th.flip(ext_obsv, dims=[-1])\n",
    "        return ext_obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "env_id = 'reach'\n",
    "policy_dict = make_policy_dict()\n",
    "max_episode_steps = seq_len\n",
    "env = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_id][1]]()\n",
    "env._freeze_rand_vec = False\n",
    "env = TimeLimit(env=env, max_episode_steps=max_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPData(th.utils.data.Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.obsv = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.done = None\n",
    "\n",
    "    def add_step(self, obsv:th.Tensor, action:th.Tensor, reward:th.Tensor, done:th.Tensor):\n",
    "        if self.obsv is None:\n",
    "            self.obsv = obsv.reshape([1, -1])\n",
    "        else:\n",
    "            self.obsv = th.cat((self.obsv, obsv.reshape([1, -1])), dim=0)\n",
    "\n",
    "        if self.action is None:\n",
    "            self.action = action.reshape([1, -1])\n",
    "        else:\n",
    "            self.action = th.cat((self.action, action.reshape([1, -1])), dim=0)\n",
    "\n",
    "        if self.reward is None:\n",
    "            self.reward = reward.reshape([1, -1])\n",
    "        else:\n",
    "            self.reward = th.cat((self.reward, reward.reshape([1, -1])), dim=0)\n",
    "\n",
    "        if self.done is None:\n",
    "            self.done = done.reshape([1, -1])\n",
    "        else:\n",
    "            self.done = th.cat((self.done, done.reshape([1, -1])), dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obsv)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        done = self.done[index]\n",
    "\n",
    "        if done:\n",
    "            return self.obsv[index], th.zeros_like(self.obsv[index]), self.action[index], th.zeros_like(self.action[index]), self.reward[index], th.zeros_like(self.reward[index]), done\n",
    "        else:\n",
    "            return self.obsv[index], self.obsv[index+1], self.action[index], self.action[index+1], self.reward[index], self.reward[index+1], done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, quantized, activation=nn.ReLU(), dropout=0, use_batch_norm=False):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # create a sequential container to hold the layers\n",
    "        self.layers = nn.Sequential()\n",
    "        \n",
    "        # create the input layer\n",
    "        self.layers.add_module(\"input\", nn.Linear(input_size, hidden_sizes[0]))\n",
    "        \n",
    "        # create the hidden layers\n",
    "        for i, size in enumerate(hidden_sizes[1:]):\n",
    "            self.layers.add_module(f\"hidden_{i+1}\", nn.Linear(hidden_sizes[i], size))\n",
    "            if use_batch_norm:\n",
    "                self.layers.add_module(f\"batch_norm_{i+1}\", nn.BatchNorm1d(size))\n",
    "            self.layers.add_module(f\"activation_{i+1}\", activation)\n",
    "            if dropout > 0:\n",
    "                self.layers.add_module(f\"dropout_{i+1}\", nn.Dropout(dropout))\n",
    "        \n",
    "        # create the output layer\n",
    "        self.layers.add_module(\"output\", nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.quantized = quantized\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_shape = x.shape\n",
    "        quantized = len(x_shape) == 4\n",
    "        if quantized: #quantized input\n",
    "            x = x.reshape([x.shape[0], x.shape[1], -1])\n",
    "        # forward pass through the layers\n",
    "\n",
    "        result = self.layers(x)\n",
    "        if self.quantized:\n",
    "            result = result.reshape([x_shape[0], x_shape[1], x_shape[2], -1])\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_sm(input, label, scale):\n",
    "    sm = th.nn.Softmax(dim=-1)\n",
    "    prob_input = sm(input)\n",
    "    dist = th.arange(input.shape[-1]).reshape([1,1,1,-1]).repeat([input.shape[0], input.shape[1],input.shape[2], 1])\n",
    "    arg_label = th.argmax(label, dim=-1)\n",
    "    arg_dist = dist - arg_label[:, :,:, None]\n",
    "    arg_dist = arg_dist**2\n",
    "    arg_dist = arg_dist * scale[None, None,:, None]\n",
    "    arg_dist = arg_dist * prob_input\n",
    "    return arg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_max(input):\n",
    "    max_indices = th.max(input, dim=-1)[1]\n",
    "    one_hot = th.nn.functional.one_hot(max_indices, num_classes=input.shape[-1])\n",
    "    with th.no_grad():\n",
    "        input -= input\n",
    "        input += one_hot\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantzedMDP(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, ntokens_obsv, ntokens_act, obsv_low, obsv_high, action_low, action_high, batch_size) -> None:\n",
    "        super().__init__(env)\n",
    "        self.ntokens_obsv= ntokens_obsv\n",
    "        self.ntokens_act = ntokens_act\n",
    "\n",
    "        min_obsv = self.observation_space.low\n",
    "        min_obsv = np.maximum(min_obsv, obsv_low)\n",
    "        self.min_obsv = th.tensor(min_obsv)\n",
    "        max_obsv = self.observation_space.high\n",
    "        max_obsv = np.minimum(max_obsv, obsv_high)\n",
    "        self.max_obsv = th.tensor(max_obsv)\n",
    "\n",
    "        min_action = self.action_space.low\n",
    "        min_action = np.maximum(min_action, action_low)\n",
    "        self.min_action = th.tensor(min_action)\n",
    "        max_action = self.action_space.high\n",
    "        max_action = np.minimum(max_action, action_high)\n",
    "        self.max_action = th.tensor(max_action)\n",
    "\n",
    "        self.max_recoreded_obsv = -float(\"inf\")\n",
    "        self.min_recoreded_obsv = float(\"inf\")\n",
    "\n",
    "        self.replay_data = MDPData()\n",
    "\n",
    "        self.current_obsv = None\n",
    "\n",
    "        \n",
    "\n",
    "    def quantize(self, inpt, min, max, ntokens):\n",
    "        th_inpt = th.tensor(inpt).reshape([1,1,-1])\n",
    "        th_inpt = tokenize(inpt=th_inpt, minimum=min, maximum=max, ntokens=ntokens)\n",
    "        th_inpt = detokenize(inpt=th_inpt, minimum=min, maximum=max, ntokens=ntokens)\n",
    "        return th_inpt.numpy().squeeze()\n",
    "\n",
    "    def reset(self) -> Any:\n",
    "        obsv = super().reset()\n",
    "        if max(obsv) > self.max_recoreded_obsv:\n",
    "            self.max_recoreded_obsv = max(obsv)\n",
    "\n",
    "        if min(obsv) < self.min_recoreded_obsv:\n",
    "            self.min_recoreded_obsv = min(obsv)\n",
    "\n",
    "        q_obsv = self.quantize(inpt=obsv, min=self.min_obsv, max=self.max_obsv, ntokens=self.ntokens_obsv)\n",
    "        self.current_obsv = q_obsv\n",
    "        return q_obsv\n",
    "\n",
    "    def step(self, action):\n",
    "        q_act = self.quantize(inpt=action, min=self.min_action, max=self.max_action, ntokens=self.ntokens_act)\n",
    "        obsv, reward, dones, info = super().step(q_act)\n",
    "        if max(obsv) > self.max_recoreded_obsv:\n",
    "            self.max_recoreded_obsv = max(obsv)\n",
    "            \n",
    "        if min(obsv) < self.min_recoreded_obsv:\n",
    "            self.min_recoreded_obsv = min(obsv)\n",
    "            \n",
    "        q_obsv = self.quantize(inpt=obsv, min=self.min_obsv, max=self.max_obsv, ntokens=self.ntokens_obsv)\n",
    "        self.replay_data.add_step(th.tensor(self.current_obsv), th.tensor(q_act), th.tensor(reward), th.tensor(dones))\n",
    "        self.current_obsv = q_obsv\n",
    "\n",
    "        return q_obsv, reward, dones, info\n",
    "\n",
    "    def learn(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MDPLearner(nn.Module):\n",
    "    def __init__(self, embbed_size, env:QuantzedMDP, device) -> None:\n",
    "        super().__init__()\n",
    "        ntokens_obsv = env.ntokens_obsv\n",
    "        ntokens_act = env.ntokens_act\n",
    "        obsv_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.shape[0]\n",
    "        self.emitter = MLP(input_size=obsv_size, hidden_sizes=[256, 256], output_size=embbed_size*ntokens_obsv, quantized=True).to(device)\n",
    "        self.predictor = MLP(input_size=(embbed_size+action_size)*ntokens_obsv, hidden_sizes=[256, 256], output_size=embbed_size*ntokens_obsv, quantized=True).to(device)\n",
    "        self.reward_model = MLP(input_size=(embbed_size+action_size)*ntokens_obsv, hidden_sizes=[256, 256], output_size=1, quantized=False).to(device)\n",
    "        self.optimizer = th.optim.Adam(params=list(self.emitter.parameters()) + list(self.predictor.parameters())+ list(self.reward_model.parameters()), lr=1e-3)\n",
    "        self.env = env\n",
    "        self.obs_minimum = env.min_obsv.to(device)\n",
    "        self.obs_maximum = env.max_obsv.to(device)\n",
    "        self.action_minimum = env.min_action.to(device)\n",
    "        self.action_maximum = env.max_action.to(device)\n",
    "        self.ntokens_obsv = ntokens_obsv\n",
    "        self.ntokens_act = ntokens_act\n",
    "        self.embbed_size = embbed_size\n",
    "\n",
    "    def get_reward_input(self, embeddings, actions):\n",
    "        return th.cat((embeddings, actions), dim=-2).type(th.float)\n",
    "\n",
    "\n",
    "    def step(self, obsvs:th.Tensor, n_obsvs:th.Tensor, actions:th.Tensor, n_actions:th.Tensor, rewards:th.Tensor, n_rewards:th.Tensor, dones:th.Tensor):\n",
    "        obsvs = obsvs.unsqueeze(1)\n",
    "        n_obsvs = n_obsvs.unsqueeze(1)\n",
    "        actions = actions.unsqueeze(1)\n",
    "        n_actions = n_actions.unsqueeze(1)\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        n_rewards = n_rewards.unsqueeze(1)\n",
    "        batch_size = obsvs.shape[0]\n",
    "\n",
    "        obs_minimum = self.obs_minimum.reshape([1,1,-1]).repeat([batch_size, 1, 1])\n",
    "        obs_maximum = self.obs_maximum.reshape([1,1,-1]).repeat([batch_size, 1, 1])\n",
    "        action_minimum = self.action_minimum.reshape([1,1,-1]).repeat([batch_size, 1, 1])\n",
    "        action_maximum = self.action_maximum.reshape([1,1,-1]).repeat([batch_size, 1, 1])\n",
    "\n",
    "        qobsvs = tokenize(obsvs, minimum=obs_minimum, maximum=obs_maximum, ntokens=self.ntokens_obsv)\n",
    "        n_qobsvs = tokenize(n_obsvs, minimum=obs_minimum, maximum=obs_maximum, ntokens=self.ntokens_obsv)\n",
    "        qactions = tokenize(actions, minimum=action_minimum, maximum=action_maximum, ntokens=self.ntokens_act)\n",
    "        n_qactions = tokenize(n_actions, minimum=action_minimum, maximum=action_maximum, ntokens=self.ntokens_act)\n",
    "\n",
    "        print(f\"n_qactions.shape; {n_qactions.shape}\")\n",
    "\n",
    "        nd_qobsvs = qobsvs[~dones]\n",
    "        nd_n_qobsvs = n_qobsvs[~dones]\n",
    "        nd_qactions = qactions[~dones]\n",
    "        nd_n_qactions = n_qactions[~dones]\n",
    "        nd_nrewards = n_rewards[~dones]\n",
    "        nd_n_obsvs = n_obsvs[~dones]\n",
    "\n",
    "        print(f\"nd_n_qactions.shape; {nd_n_qactions.shape}\")\n",
    "        print(f\"nd_qactions.shape; {nd_qactions.shape}\")\n",
    "        print(f\"n_qactions.shape; {n_qactions.shape}\")\n",
    "\n",
    "        embeddings = self.emitter(obsvs)\n",
    "        embeddings = embeddings.reshape([embeddings.shape[0], embeddings.shape[1], self.embbed_size , self.ntokens_obsv])\n",
    "        max_embeddings = gradient_max(embeddings)\n",
    "\n",
    "        print(f'max_embeddings: {max_embeddings.shape}')\n",
    "        print(f'qactions: {qactions.shape}')\n",
    "\n",
    "        emb_act = self.get_reward_input(embeddings=max_embeddings, actions=qactions)\n",
    "\n",
    "        print(f'emb_act: {emb_act.shape}')\n",
    "\n",
    "        expected_rewards = self.reward_model(emb_act)\n",
    "\n",
    "        print(f'expected_rewards: {expected_rewards.shape}')\n",
    "        rew1_loss = calcMSE(expected_rewards, rewards)\n",
    "\n",
    "        nd_embeddings = max_embeddings[~dones]\n",
    "        \n",
    "        print(f'nd_qactions: {nd_qactions.shape}')\n",
    "        print(f'nd_embeddings: {nd_embeddings.shape}')\n",
    "\n",
    "        nd_emb_act = self.get_reward_input(embeddings=nd_embeddings, actions=nd_qactions)\n",
    "        print(f'nd_emb_act: {nd_emb_act.shape}')\n",
    "\n",
    "        n_embeddings = self.emitter(nd_n_obsvs)\n",
    "        max_n_embeddings = gradient_max(n_embeddings)\n",
    "        pred_n_embeddings = self.predictor(nd_emb_act)\n",
    "        max_pred_n_embeddings = gradient_max(pred_n_embeddings)\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'n_embeddings: {n_embeddings.shape}')\n",
    "        print(f'pred_n_embeddings: {pred_n_embeddings.shape}')\n",
    "\n",
    "        pred_loss = dist_sm(input=n_embeddings, label=pred_n_embeddings, scale=1)\n",
    "        \n",
    "        n_emb_act = self.get_reward_input(embeddings=max_n_embeddings, actions=nd_nactions)\n",
    "        expected_n_rewards = self.reward_model(n_emb_act)\n",
    "\n",
    "        rew2_loss = calcMSE(expected_n_rewards, nd_nrewards)\n",
    "\n",
    "        loss = rew1_loss + rew2_loss + pred_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return rew1_loss, rew2_loss, pred_loss, max_n_embeddings, max_pred_n_embeddings\n",
    "\n",
    "    def learn(self):\n",
    "        #while max_n_embeddings != max_pred_n_embeddings and rew1, rew2 too high.\n",
    "        #for obsvs:th.Tensor, n_obsvs:th.Tensor, actions:th.Tensor, n_actions:th.Tensor, rewards:th.Tensor, n_rewards:th.Tensor, dones:th.Tensor in Loader:\n",
    "        ##step\n",
    "        ##Write tboard\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Learn\n",
    "#Predict episode with act actions\n",
    "# Predict episode with sampled actions \n",
    "# Extractor model with emitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv= QuantzedMDP(env=env, ntokens_obsv=10, ntokens_act=10, obsv_low=-1, obsv_high=1, action_low=-1, action_high=1, batch_size=32)\n",
    "qenv_eval= QuantzedMDP(env=env, ntokens_obsv=10, ntokens_act=10, obsv_low=-1, obsv_high=1, action_low=-1, action_high=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized: torch.Size([2, 2, 39, 10])\n"
     ]
    }
   ],
   "source": [
    "tokenizer_input = th.zeros([2,2,39])\n",
    "tokenizer_input[0,0,0] = 1\n",
    "minimum = -th.ones_like(tokenizer_input)\n",
    "maximum = th.ones_like(tokenizer_input)\n",
    "\n",
    "quantized = tokenize(inpt=tokenizer_input, minimum=minimum, maximum=maximum, ntokens=10)\n",
    "print(f'quantized: {quantized.shape}')\n",
    "dequantized = detokenize(inpt=quantized, minimum=minimum, maximum=maximum, ntokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "mdp_learner = MDPLearner(embbed_size=2, env=qenv, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_qactions.shape; torch.Size([2, 1, 4, 10])\n",
      "nd_n_qactions.shape; torch.Size([2, 1, 4, 10])\n",
      "nd_qactions.shape; torch.Size([2, 1, 4, 10])\n",
      "n_qactions.shape; torch.Size([2, 1, 4, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 1, 39, -1]' is invalid for input of size 40",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [111], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m actions \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mzeros([batch_size, qenv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m rewards \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mzeros([batch_size, \u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m mdp_learner\u001b[39m.\u001b[39;49mstep(obsvs\u001b[39m=\u001b[39;49mobservation, n_obsvs\u001b[39m=\u001b[39;49mobservation, actions\u001b[39m=\u001b[39;49mactions, n_actions\u001b[39m=\u001b[39;49mactions, rewards\u001b[39m=\u001b[39;49mrewards, n_rewards\u001b[39m=\u001b[39;49mrewards, dones\u001b[39m=\u001b[39;49mth\u001b[39m.\u001b[39;49mzeros([batch_size], dtype\u001b[39m=\u001b[39;49mth\u001b[39m.\u001b[39;49mbool, device\u001b[39m=\u001b[39;49mdevice))\n",
      "Cell \u001b[0;32mIn [106], line 57\u001b[0m, in \u001b[0;36mMDPLearner.step\u001b[0;34m(self, obsvs, n_obsvs, actions, n_actions, rewards, n_rewards, dones)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnd_qactions.shape; \u001b[39m\u001b[39m{\u001b[39;00mnd_qactions\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mn_qactions.shape; \u001b[39m\u001b[39m{\u001b[39;00mn_qactions\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49memitter(obsvs)\n\u001b[1;32m     58\u001b[0m embeddings \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mreshape([embeddings\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], embeddings\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membbed_size , \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mntokens_obsv])\n\u001b[1;32m     59\u001b[0m max_embeddings \u001b[39m=\u001b[39m gradient_max(embeddings)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [102], line 33\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers(x)\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantized:\n\u001b[0;32m---> 33\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49mreshape([x_shape[\u001b[39m0\u001b[39;49m], x_shape[\u001b[39m1\u001b[39;49m], x_shape[\u001b[39m2\u001b[39;49m], \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     34\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 1, 39, -1]' is invalid for input of size 40"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "observation = mdp_learner.obs_minimum.repeat([2]).reshape([batch_size, -1]).to(device)\n",
    "actions = th.zeros([batch_size, qenv.action_space.shape[0]]).to(device)\n",
    "rewards = th.zeros([batch_size, 1]).to(device)\n",
    "\n",
    "mdp_learner.step(obsvs=observation, n_obsvs=observation, actions=actions, n_actions=actions, rewards=rewards, n_rewards=rewards, dones=th.zeros([batch_size], dtype=th.bool, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_SAC(env, eval_env, eval_epochs, iterations, path, logname, model = None):\n",
    "    tb = TBoardGraphs(logname=logname, data_path=path)\n",
    "\n",
    "    if model is None:\n",
    "        pkwarg = dict(net_arch=[512, 512, 512])\n",
    "        pkwarg = dict(features_extractor_class=TestExtractor)\n",
    "        model = SAC(\"MlpPolicy\", env=env, verbose=1, policy_kwargs=pkwarg)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        rews = []\n",
    "        for eval_run in range(eval_epochs):\n",
    "            obs = env.reset()\n",
    "            while True:\n",
    "                action, _states = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                rews.append(reward)\n",
    "                #env.render()\n",
    "                if done:\n",
    "                    break\n",
    "        rews_np = np.array(rews)\n",
    "        tb.addValidationScalar(name='Average Reward', value=th.tensor(rews_np.mean()), stepid=iteration)\n",
    "        model.learn(total_timesteps=100*1, log_interval=1000)\n",
    "        model.save(logname)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkwarg = dict(features_extractor_class=TestExtractor)\n",
    "model_env = SAC(\"MlpPolicy\", env, verbose=1, policy_kwargs=pkwarg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_SAC(env=qenv, eval_env=qenv_eval, eval_epochs=0, iterations=1, logname='Test', path='/data/bing/hendrik/', model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0058,  0.5999,  0.1946,  1.0000,  0.0110,  0.6757,  0.0190,  0.0010,\n",
       "          0.0010,  0.0010,  1.0000,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
       "          0.0010,  0.0010,  0.0058,  0.5999,  0.1946,  1.0000,  0.0310,  0.6336,\n",
       "          0.0190,  0.0010,  0.0010,  0.0010,  1.0000,  0.0010,  0.0010,  0.0010,\n",
       "          0.0010,  0.0010,  0.0010,  0.0010, -0.0898,  0.8194,  0.0583]),\n",
       " tensor([ 0.0047,  0.6006,  0.1938,  0.9980,  0.0110,  0.6757,  0.0190,  0.0010,\n",
       "          0.0010, -0.0010,  1.0000,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
       "          0.0010,  0.0010,  0.0058,  0.5999,  0.1946,  1.0000,  0.0110,  0.6757,\n",
       "          0.0190,  0.0010,  0.0010,  0.0010,  1.0000,  0.0010,  0.0010,  0.0010,\n",
       "          0.0010,  0.0010,  0.0010,  0.0010, -0.0898,  0.8194,  0.0583]),\n",
       " tensor([-0.9700,  0.8258, -0.6737,  0.9800]),\n",
       " tensor([-0.3473,  0.8338, -0.6597, -0.4334]),\n",
       " tensor([1.6786]),\n",
       " tensor([1.7176]),\n",
       " tensor([False]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qenv.replay_data.__getitem__(index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34675765,  0.8342813 , -0.6591253 , -0.432451  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.replay_buffer.actions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.replay_buffer.rewards[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qenv.replay_data.done[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC.load('Reach Quantized Switched Reinit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.actor.features_extractor.switch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.critic_target.features_extractor.switch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.critic.features_extractor.switch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_policy(model:SAC):\n",
    "    for module in model.policy.critic.qf0:\n",
    "        model.policy.init_weights(module)\n",
    "    for module in model.policy.critic.qf1:\n",
    "        model.policy.init_weights(module)\n",
    "\n",
    "    for module in model.policy.critic_target.qf0:\n",
    "        model.policy.init_weights(module)\n",
    "    for module in model.policy.critic_target.qf1:\n",
    "        model.policy.init_weights(module)\n",
    "    for module in model.policy.actor.latent_pi:\n",
    "        model.policy.init_weights(module)\n",
    "    model.policy.init_weights(model.policy.actor.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(gradient_steps=model.total_gradient_steps, batch_size=model.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews = []\n",
    "eval_epochs = 20\n",
    "env = qenv\n",
    "for eval_run in range(eval_epochs):\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rews.append(reward)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            break\n",
    "rews_np = np.array(rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews_np.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews_np.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews_np.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
