{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/Hendrik/PycharmProjects') # go to parent dir\n",
    "\n",
    "from ActiveCritic.model_src.whole_sequence_model import WholeSequenceActor, WholeSequenceCritic, WholeSequenceModelSetup\n",
    "from ActiveCritic.model_src.transformer import TransformerModel, CriticTransformer, ModelSetup\n",
    "import torch as th\n",
    "from ActiveCritic.tests.test_utils.utils import make_mask_data, make_seq_encoding_data, make_critic_data, make_wsm_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = WholeSequenceModelSetup()\n",
    "wsm.model_setup = ModelSetup()\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "wsm.model_setup.d_output = d_output\n",
    "wsm.model_setup.nhead = 1\n",
    "wsm.model_setup.d_hid = 10\n",
    "wsm.model_setup.d_model = 10\n",
    "wsm.model_setup.nlayers = 2\n",
    "wsm.model_setup.seq_len = seq_len\n",
    "wsm.model_setup.dropout = 0\n",
    "wsm.lr = 1e-3\n",
    "wsm.model_setup.device = 'cuda'\n",
    "wsm.optimizer_class = th.optim.Adam\n",
    "wsm.optimizer_kwargs = {}\n",
    "wsm.model_setup.model_class:TransformerModel = TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = make_wsm_setup(seq_len=seq_len, d_output=d_output, model_class=TransformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa = WholeSequenceActor(wsms=wsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = th.ones([batch_size, seq_len, d_intput], dtype=th.float, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wsa.forward(inputs=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = output.shape\n",
    "assert shape[0] == batch_size\n",
    "assert shape[1] == seq_len\n",
    "assert shape[2] == d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq, outpt_seq = make_seq_encoding_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken, d_out=d_output)\n",
    "success = th.ones_like(inpt_seq, dtype=th.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa = WholeSequenceActor(wsms=wsm)\n",
    "data = inpt_seq, outpt_seq, success\n",
    "for i in range(3000):\n",
    "    result = wsa.forward(inputs=inpt_seq)\n",
    "    resp = wsa.optimizer_step(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wsa.optimizer_step(data=data)\n",
    "assert res['Trajectory Loss '] < 1e-2\n",
    "\n",
    "res = wsa.optimizer_step(data=data, prefix='test')\n",
    "assert 'Trajectory Loss test' in res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inpt_seq, outpt_seq, success\n",
    "for i in range(1000):\n",
    "    result = wsa.forward(inputs=inpt_seq)\n",
    "    wsa.optimizer_step(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wsa.optimizer_step(data=data)\n",
    "assert res['Trajectory Loss '] < 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = WholeSequenceModelSetup()\n",
    "wsm.model_setup = ModelSetup()\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_result = 1\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "wsm.model_setup.d_output = d_output\n",
    "wsm.model_setup.nhead = 1\n",
    "wsm.model_setup.d_hid = 10\n",
    "wsm.model_setup.d_model = 10\n",
    "wsm.model_setup.nlayers = 2\n",
    "wsm.model_setup.seq_len = seq_len\n",
    "wsm.model_setup.dropout = 0\n",
    "wsm.lr = 1e-3\n",
    "wsm.model_setup.d_result = d_result\n",
    "wsm.model_setup.device = 'cuda'\n",
    "wsm.optimizer_class = th.optim.Adam\n",
    "wsm.optimizer_kwargs = {}\n",
    "wsm.model_setup.model_class:TransformerModel = CriticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc = make_wsm_setup(seq_len=seq_len, d_output=d_output, model_class=CriticTransformer, d_result=d_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq, outpt_seq = make_critic_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken)\n",
    "data = inpt_seq, None, outpt_seq\n",
    "model = WholeSequenceCritic(wsc)\n",
    "for i in range(3000):\n",
    "    res = model.optimizer_step(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.optimizer_step(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = WholeSequenceModelSetup()\n",
    "wsm.model_setup = ModelSetup()\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "wsm.model_setup.d_output = d_output\n",
    "wsm.model_setup.nhead = 1\n",
    "wsm.model_setup.d_hid = 10\n",
    "wsm.model_setup.d_model = 10\n",
    "wsm.model_setup.nlayers = 2\n",
    "wsm.model_setup.seq_len = seq_len\n",
    "wsm.model_setup.dropout = 0\n",
    "wsm.lr = 1e-3\n",
    "wsm.model_setup.device = 'cuda'\n",
    "wsm.optimizer_class = th.optim.AdamW\n",
    "wsm.optimizer_kwargs = {}\n",
    "wsm.model_setup.model_class:TransformerModel = TransformerModel\n",
    "wsa = WholeSequenceActor(wsms=wsm)\n",
    "input = th.ones([batch_size, seq_len, d_intput], dtype=th.float, device='cuda')\n",
    "output = wsa.forward(inputs=input)\n",
    "shape = output.shape\n",
    "\n",
    "assert(shape[0] == batch_size)\n",
    "assert(shape[1] == seq_len)\n",
    "assert(shape[2] == d_output)\n",
    "\n",
    "inpt_seq, outpt_seq = make_seq_encoding_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken, d_out=d_output)\n",
    "success = th.ones_like(inpt_seq, dtype=th.bool)\n",
    "wsa = WholeSequenceActor(wsms=wsm)\n",
    "data = inpt_seq, outpt_seq, success\n",
    "for i in range(1000):\n",
    "    res = wsa.optimizer_step(data=data)\n",
    "assert(res['Trajectory Loss '] < 1e-2)\n",
    "\n",
    "res = wsa.optimizer_step(data=data, prefix='test')\n",
    "assert('Trajectory Loss test' in res)\n",
    "\n",
    "wsa.init_model()\n",
    "res = wsa.optimizer_step(data=data)\n",
    "assert(res['Trajectory Loss '] > 1e-1, 'Init Model did not cange the parameters.')\n",
    "for i in range(1000):\n",
    "    res = wsa.optimizer_step(data=data)   \n",
    "assert(res['Trajectory Loss '] < 1e-2, 'Did not converge after reinit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from active_critic.model_src.whole_sequence_model import WholeSequenceModel\n",
    "from active_critic.model_src.transformer import ModelSetup, TransformerModel\n",
    "import torch as th\n",
    "from active_critic.utils.test_utils import make_mask_data, make_seq_encoding_data, make_critic_data, make_wsm_setup\n",
    "import unittest\n",
    "from active_critic.utils.pytorch_utils import get_rew_mask, get_seq_end_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from active_critic.utils.pytorch_utils import calcMSE\n",
    "\n",
    "\n",
    "th.manual_seed(0)\n",
    "\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "current_step = 3\n",
    "\n",
    "input = th.ones([batch_size, seq_len, 1],\n",
    "                dtype=th.float, device='cuda')\n",
    "input[0,0] = 3\n",
    "input[1,1] = 2\n",
    "org_input = th.clone(input)\n",
    "input.requires_grad = True\n",
    "\n",
    "goal = th.ones([batch_size, seq_len, 1],\n",
    "        dtype=th.float, device='cuda')\n",
    "\n",
    "opt = th.optim.SGD([input], lr=1e-1)\n",
    "\n",
    "wsa_setup = make_wsm_setup(seq_len=seq_len, d_output=d_output)\n",
    "wsm = WholeSequenceModel(wsms=wsa_setup)\n",
    "mask = get_seq_end_mask(input, current_step)\n",
    "print(input[mask])\n",
    "print(goal[mask])\n",
    "print(calcMSE(input[mask], goal[mask]))\n",
    "loss = wsm.loss_fct(result=input, label=goal, mask=mask)\n",
    "\n",
    "assert loss == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "current_step = 3\n",
    "\n",
    "input = th.ones([batch_size, seq_len, 1],\n",
    "                dtype=th.float, device='cuda')\n",
    "input[0,0] = 3\n",
    "input[1,1] = 2\n",
    "org_input = th.clone(input)\n",
    "input.requires_grad = True\n",
    "\n",
    "goal = th.ones([batch_size, seq_len, 1],\n",
    "        dtype=th.float, device='cuda')\n",
    "\n",
    "opt = th.optim.SGD([input], lr=1e-1)\n",
    "\n",
    "wsa_setup = make_wsm_setup(seq_len=seq_len, d_output=d_output)\n",
    "wsm = WholeSequenceModel(wsms=wsa_setup)\n",
    "mask = get_seq_end_mask(input, current_step)\n",
    "loss = wsm.loss_fct(result=input, label=goal, mask=mask)\n",
    "\n",
    "assert loss == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step = 1\n",
    "mask = get_seq_end_mask(input, current_step)\n",
    "loss = wsm.loss_fct(result=input, label=goal, mask=mask)\n",
    "assert loss == 1/(batch_size*(seq_len-current_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]], device='cuda:0')\n",
      "tensor(0.5556, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "goal[0,3:] = -1\n",
    "mask = get_rew_mask(reward=goal)\n",
    "print(mask)\n",
    "loss = wsm.loss_fct(result=input, label=goal, mask=mask)\n",
    "print(loss)\n",
    "assert th.allclose(loss, th.tensor(5 / 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rew_mask(reward):\n",
    "    return (reward.squeeze()>=0)\n",
    "\n",
    "actions = th.randint(0, 10, [2,3,4])\n",
    "actions[:,:1] = 0\n",
    "rew = th.ones([2,3,1])\n",
    "rew[:,:1] = -1\n",
    "actions\n",
    "mask = get_rew_mask(rew)\n",
    "actions[mask]\n",
    "assert th.equal(actions[mask].reshape(-1), actions[:,1:].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = th.randint(0, 10, [2,3,4])\n",
    "actions[:,:1] = 0\n",
    "rew = th.ones([2,3,1])\n",
    "rew[:,:1] = -1\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_rew_mask(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[mask]\n",
    "assert th.equal(actions[mask].reshape(-1), actions[:,1:].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "\n",
    "seq_len = 6\n",
    "change_spot = 4\n",
    "ntoken = 3\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "current_step = 1\n",
    "\n",
    "input = th.zeros([batch_size, seq_len, 1],\n",
    "                dtype=th.float, device='cuda')\n",
    "input[:,change_spot:] = 1\n",
    "org_input = th.clone(input)\n",
    "input.requires_grad = True\n",
    "\n",
    "goal = th.ones([batch_size, seq_len, 1],\n",
    "        dtype=th.float, device='cuda')\n",
    "\n",
    "opt = th.optim.SGD([input], lr=1e-1)\n",
    "\n",
    "wsa_setup = make_wsm_setup(seq_len=seq_len, d_output=d_output)\n",
    "oec = OptimizeEndCritic(wsms=wsa_setup)\n",
    "loss = oec.loss_fct(result=input, label=goal, current_step=current_step)\n",
    "assert (loss - (change_spot-current_step)/(seq_len-current_step)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(change_spot-current_step)/(seq_len-current_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
