{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mActiveCritic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerModel, CriticTransformer, ModelSetup\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mth\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mActiveCritic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_mask_data, make_seq_encoding_data, make_critic_data, make_wsm_setup\n",
      "File \u001b[1;32mC:\\Users/Hendrik/PycharmProjects\\ActiveCritic\\tests\\test_utils\\utils.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mActiveCritic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     CriticTransformer, ModelSetup, TransformerModel,\n\u001b[0;32m      5\u001b[0m     generate_square_subsequent_mask)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mActiveCritic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwhole_sequence_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     WholeSequenceActor, WholeSequenceCritic, WholeSequenceModelSetup)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mActiveCritic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactive_critic_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActiveCriticPolicySetup\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_seq_encoding_data\u001b[39m(batch_size, seq_len, ntoken, d_out, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     12\u001b[0m     inpt_seq \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mones([batch_size,seq_len,ntoken], dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mC:\\Users/Hendrik/PycharmProjects\\ActiveCritic\\policy\\active_critic_policy.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mActiveCritic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwhole_sequence_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     WholeSequenceActor, WholeSequenceCritic, WholeSequenceModelSetup)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mActiveCritic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_partially_observed_seq\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_layers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseFeaturesExtractor\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mACPOptResult\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/Hendrik/PycharmProjects') # go to parent dir\n",
    "\n",
    "from ActiveCritic.model_src.whole_sequence_model import WholeSequenceActor, WholeSequenceCritic, WholeSequenceModelSetup\n",
    "from ActiveCritic.model_src.transformer import TransformerModel, CriticTransformer, ModelSetup\n",
    "import torch as th\n",
    "from ActiveCritic.tests.test_utils.utils import make_mask_data, make_seq_encoding_data, make_critic_data, make_wsm_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = WholeSequenceModelSetup()\n",
    "wsm.model_setup = ModelSetup()\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "wsm.model_setup.d_output = d_output\n",
    "wsm.model_setup.nhead = 1\n",
    "wsm.model_setup.d_hid = 10\n",
    "wsm.model_setup.d_model = 10\n",
    "wsm.model_setup.nlayers = 2\n",
    "wsm.model_setup.seq_len = seq_len\n",
    "wsm.model_setup.dropout = 0\n",
    "wsm.lr = 1e-3\n",
    "wsm.model_setup.device = 'cuda'\n",
    "wsm.optimizer_class = th.optim.Adam\n",
    "wsm.optimizer_kwargs = {}\n",
    "wsm.model_setup.model_class:TransformerModel = TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = make_wsm_setup(seq_len=seq_len, d_output=d_output, model_class=TransformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa = WholeSequenceActor(wsms=wsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = th.ones([batch_size, seq_len, d_intput], dtype=th.float, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wsa.forward(inputs=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = output.shape\n",
    "assert shape[0] == batch_size\n",
    "assert shape[1] == seq_len\n",
    "assert shape[2] == d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq, outpt_seq = make_seq_encoding_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken, d_out=d_output)\n",
    "success = th.ones_like(inpt_seq, dtype=th.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa = WholeSequenceActor(wsms=wsm)\n",
    "data = inpt_seq, outpt_seq, success\n",
    "for i in range(3000):\n",
    "    result = wsa.forward(inputs=inpt_seq)\n",
    "    resp = wsa.optimizer_step(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wsa.optimizer_step(data=data)\n",
    "assert res['Trajectory Loss '] < 1e-2\n",
    "\n",
    "res = wsa.optimizer_step(data=data, prefix='test')\n",
    "assert 'Trajectory Loss test' in res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inpt_seq, outpt_seq, success\n",
    "for i in range(1000):\n",
    "    result = wsa.forward(inputs=inpt_seq)\n",
    "    wsa.optimizer_step(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wsa.optimizer_step(data=data)\n",
    "assert res['Trajectory Loss '] < 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = WholeSequenceModelSetup()\n",
    "wsm.model_setup = ModelSetup()\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_result = 1\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "wsm.model_setup.d_output = d_output\n",
    "wsm.model_setup.nhead = 1\n",
    "wsm.model_setup.d_hid = 10\n",
    "wsm.model_setup.d_model = 10\n",
    "wsm.model_setup.nlayers = 2\n",
    "wsm.model_setup.seq_len = seq_len\n",
    "wsm.model_setup.dropout = 0\n",
    "wsm.lr = 1e-3\n",
    "wsm.model_setup.d_result = d_result\n",
    "wsm.model_setup.device = 'cuda'\n",
    "wsm.optimizer_class = th.optim.Adam\n",
    "wsm.optimizer_kwargs = {}\n",
    "wsm.model_setup.model_class:TransformerModel = CriticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc = make_wsm_setup(seq_len=seq_len, d_output=d_output, model_class=CriticTransformer, d_result=d_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq, outpt_seq = make_critic_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken)\n",
    "data = inpt_seq, None, outpt_seq\n",
    "model = WholeSequenceCritic(wsc)\n",
    "for i in range(3000):\n",
    "    res = model.optimizer_step(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'critic loss': tensor(1.4034e-12, device='cuda:0'),\n",
       " 'critic loss positive': tensor(1.4211e-12, device='cuda:0'),\n",
       " 'critic loss negative': tensor(1.3858e-12, device='cuda:0')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.optimizer_step(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'critic loss': tensor(0.3448, device='cuda:0'),\n",
       " 'critic loss positive': tensor(0.6702, device='cuda:0'),\n",
       " 'critic loss negative': tensor(0.0193, device='cuda:0')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:42: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:45: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:42: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:45: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.001\n",
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9577/2482493908.py:42: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(res['Trajectory Loss '] > 1e-1, 'Init Model did not cange the parameters.')\n",
      "/tmp/ipykernel_9577/2482493908.py:45: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(res['Trajectory Loss '] < 1e-2, 'Did not converge after reinit.')\n"
     ]
    }
   ],
   "source": [
    "wsm = WholeSequenceModelSetup()\n",
    "wsm.model_setup = ModelSetup()\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "wsm.model_setup.d_output = d_output\n",
    "wsm.model_setup.nhead = 1\n",
    "wsm.model_setup.d_hid = 10\n",
    "wsm.model_setup.d_model = 10\n",
    "wsm.model_setup.nlayers = 2\n",
    "wsm.model_setup.seq_len = seq_len\n",
    "wsm.model_setup.dropout = 0\n",
    "wsm.lr = 1e-3\n",
    "wsm.model_setup.device = 'cuda'\n",
    "wsm.optimizer_class = th.optim.AdamW\n",
    "wsm.optimizer_kwargs = {}\n",
    "wsm.model_setup.model_class:TransformerModel = TransformerModel\n",
    "wsa = WholeSequenceActor(wsms=wsm)\n",
    "input = th.ones([batch_size, seq_len, d_intput], dtype=th.float, device='cuda')\n",
    "output = wsa.forward(inputs=input)\n",
    "shape = output.shape\n",
    "\n",
    "assert(shape[0] == batch_size)\n",
    "assert(shape[1] == seq_len)\n",
    "assert(shape[2] == d_output)\n",
    "\n",
    "inpt_seq, outpt_seq = make_seq_encoding_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken, d_out=d_output)\n",
    "success = th.ones_like(inpt_seq, dtype=th.bool)\n",
    "wsa = WholeSequenceActor(wsms=wsm)\n",
    "data = inpt_seq, outpt_seq, success\n",
    "for i in range(1000):\n",
    "    res = wsa.optimizer_step(data=data)\n",
    "assert(res['Trajectory Loss '] < 1e-2)\n",
    "\n",
    "res = wsa.optimizer_step(data=data, prefix='test')\n",
    "assert('Trajectory Loss test' in res)\n",
    "\n",
    "wsa.init_model()\n",
    "res = wsa.optimizer_step(data=data)\n",
    "assert(res['Trajectory Loss '] > 1e-1, 'Init Model did not cange the parameters.')\n",
    "for i in range(1000):\n",
    "    res = wsa.optimizer_step(data=data)   \n",
    "assert(res['Trajectory Loss '] < 1e-2, 'Did not converge after reinit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trajectory Loss ': tensor(2.4412e-09, device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv_torch",
   "language": "python",
   "name": "local-venv_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bee90e249730b85f00f3915f0cf4f21bc0729131dcc7008c941068256fd0d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
