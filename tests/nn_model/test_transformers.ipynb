{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from active_critic.model_src.transformer import ModelSetup, TransformerModel, generate_square_subsequent_mask\n",
    "from active_critic.model_src.base_transformer import DebugTEL\n",
    "import torch as th\n",
    "from active_critic.utils.pytorch_utils import calcMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ModelSetup()\n",
    "seq_len = 3\n",
    "ntoken = 3\n",
    "batch_size = 2\n",
    "d_output = 4\n",
    "\n",
    "ms.d_output = d_output\n",
    "ms.nhead = 1\n",
    "ms.d_hid = 10\n",
    "ms.d_model = 10\n",
    "ms.nlayers = 1\n",
    "ms.seq_len = seq_len\n",
    "ms.dropout = 0\n",
    "ms.ntoken = 1\n",
    "ms.lr = None\n",
    "ms.device = 'cpu'\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq_encoding_data(batch_size, seq_len, ntoken, d_out, device = 'cuda'):\n",
    "    inpt_seq = th.ones([batch_size,seq_len,ntoken], dtype=th.float, device=device)\n",
    "    outpt_seq = th.ones([batch_size,seq_len,d_out], dtype=th.float, device=device)\n",
    "    outpt_seq[:,::2] = 0\n",
    "    return inpt_seq, outpt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TransformerModel(model_setup=ms)\n",
    "mask = generate_square_subsequent_mask(3)\n",
    "mask = mask.unsqueeze(0).repeat([2, 1, 1])\n",
    "mask[0] = 0\n",
    "#mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_tens_to_front(src, i):\n",
    "    if i > 0:\n",
    "        src[i, :, :-i] = src[i,:,i:]\n",
    "        src[i, :, -i:] = -1\n",
    "\n",
    "    return src\n",
    "\n",
    "def pull_tens_to_front_sparse(src, i):\n",
    "    pulled = src[i,:,i:]\n",
    "    max_pulled = pulled.max(dim=-2).values == 1\n",
    "    max_pulled = max_pulled.unsqueeze(-2).repeat([1, 1, src.shape[2] - i, 1])\n",
    "    if i > 0:\n",
    "        src[i, :, :-i] = max_pulled\n",
    "        src[i, :, -i:] = -1\n",
    "    else:\n",
    "        src[i, :] = max_pulled\n",
    "\n",
    "    return src\n",
    "def repeat_along_seq(src):\n",
    "    src = src.repeat([src.shape[1], 1, 1]).reshape([src.shape[1], src.shape[0], src.shape[1], src.shape[2]])\n",
    "    return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv = th.rand([2,3,4], requires_grad=True)\n",
    "act = th.rand([2,3,2])\n",
    "rewards = th.rand([2,3,1])\n",
    "\n",
    "def make_seq_encoding_data(actions, obsv, rewards):\n",
    "    actions = repeat_along_seq(actions)\n",
    "    obsv = repeat_along_seq(obsv)\n",
    "    rewards = repeat_along_seq(rewards)\n",
    "    for i in range(len(obsv)):\n",
    "        obsv[i] = obsv[i,:,i].unsqueeze(1)\n",
    "\n",
    "        actions = pull_tens_to_front(actions, i)\n",
    "        rewards = pull_tens_to_front_sparse(rewards, i)\n",
    "    return actions, obsv, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[0,0,0] = 1\n",
    "a, o, r = make_seq_encoding_data(actions=act, obsv=obsv, rewards=rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.7002,  0.0646],\n",
       "          [ 0.1922,  0.2072],\n",
       "          [ 0.7661,  0.7508]],\n",
       "\n",
       "         [[ 0.2337,  0.7739],\n",
       "          [ 0.4613,  0.7738],\n",
       "          [ 0.9919,  0.7391]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1922,  0.2072],\n",
       "          [ 0.7661,  0.7508],\n",
       "          [-1.0000, -1.0000]],\n",
       "\n",
       "         [[ 0.4613,  0.7738],\n",
       "          [ 0.9919,  0.7391],\n",
       "          [-1.0000, -1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7661,  0.7508],\n",
       "          [-1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000]],\n",
       "\n",
       "         [[ 0.9919,  0.7391],\n",
       "          [-1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000]]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8519, 0.9005, 0.6526, 0.2717],\n",
       "          [0.8519, 0.9005, 0.6526, 0.2717],\n",
       "          [0.8519, 0.9005, 0.6526, 0.2717]],\n",
       "\n",
       "         [[0.9758, 0.7468, 0.4285, 0.2169],\n",
       "          [0.9758, 0.7468, 0.4285, 0.2169],\n",
       "          [0.9758, 0.7468, 0.4285, 0.2169]]],\n",
       "\n",
       "\n",
       "        [[[0.5881, 0.1576, 0.6900, 0.2280],\n",
       "          [0.5881, 0.1576, 0.6900, 0.2280],\n",
       "          [0.5881, 0.1576, 0.6900, 0.2280]],\n",
       "\n",
       "         [[0.5547, 0.6262, 0.1660, 0.7259],\n",
       "          [0.5547, 0.6262, 0.1660, 0.7259],\n",
       "          [0.5547, 0.6262, 0.1660, 0.7259]]],\n",
       "\n",
       "\n",
       "        [[[0.8337, 0.4164, 0.6145, 0.5064],\n",
       "          [0.8337, 0.4164, 0.6145, 0.5064],\n",
       "          [0.8337, 0.4164, 0.6145, 0.5064]],\n",
       "\n",
       "         [[0.8653, 0.3316, 0.2090, 0.4454],\n",
       "          [0.8653, 0.3316, 0.2090, 0.4454],\n",
       "          [0.8653, 0.3316, 0.2090, 0.4454]]]], grad_fn=<AsStridedBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0807, 0.7646, 0.1549, 0.7881],\n",
       "         [0.9413, 0.9708, 0.5088, 0.2152],\n",
       "         [0.6848, 0.7937, 0.8356, 0.0640]],\n",
       "\n",
       "        [[0.5669, 0.9826, 0.7552, 0.0908],\n",
       "         [0.8813, 0.0539, 0.3907, 0.1964],\n",
       "         [0.2069, 0.7606, 0.5515, 0.8838]]], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0807, 0.7646, 0.1549, 0.7881],\n",
       "          [0.0807, 0.7646, 0.1549, 0.7881],\n",
       "          [0.0807, 0.7646, 0.1549, 0.7881]],\n",
       "\n",
       "         [[0.5669, 0.9826, 0.7552, 0.0908],\n",
       "          [0.5669, 0.9826, 0.7552, 0.0908],\n",
       "          [0.5669, 0.9826, 0.7552, 0.0908]]],\n",
       "\n",
       "\n",
       "        [[[0.9413, 0.9708, 0.5088, 0.2152],\n",
       "          [0.9413, 0.9708, 0.5088, 0.2152],\n",
       "          [0.9413, 0.9708, 0.5088, 0.2152]],\n",
       "\n",
       "         [[0.8813, 0.0539, 0.3907, 0.1964],\n",
       "          [0.8813, 0.0539, 0.3907, 0.1964],\n",
       "          [0.8813, 0.0539, 0.3907, 0.1964]]],\n",
       "\n",
       "\n",
       "        [[[0.6848, 0.7937, 0.8356, 0.0640],\n",
       "          [0.6848, 0.7937, 0.8356, 0.0640],\n",
       "          [0.6848, 0.7937, 0.8356, 0.0640]],\n",
       "\n",
       "         [[0.2069, 0.7606, 0.5515, 0.8838],\n",
       "          [0.2069, 0.7606, 0.5515, 0.8838],\n",
       "          [0.2069, 0.7606, 0.5515, 0.8838]]]], grad_fn=<AsStridedBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partial_sequence(seq):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tm.forward(src=obsv, mask=mask)\n",
    "loss = ((result[:, -2])**2).mean()\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0131,  0.0150, -0.0092,  0.0273],\n",
       "         [ 0.1039, -0.0696, -0.1979,  0.1286],\n",
       "         [ 0.0202,  0.0083,  0.0017,  0.0149]],\n",
       "\n",
       "        [[ 0.0111, -0.0017,  0.0104,  0.0088],\n",
       "         [ 0.1019, -0.0422, -0.2138,  0.1818],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obsv.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0821,  0.2359,  0.4964, -0.1755],\n",
       "         [ 0.0684,  0.1798,  0.3541, -0.0939],\n",
       "         [ 0.1277,  0.1856,  0.5528, -0.3450]],\n",
       "\n",
       "        [[-0.0402,  0.0384,  0.2835, -0.0325],\n",
       "         [ 0.0895,  0.2347,  0.6526, -0.3985],\n",
       "         [ 0.1158,  0.1166,  0.4114, -0.2630]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask_data(batch_size, seq_len, ntoken, device = 'cuda'):\n",
    "    mask = generate_square_subsequent_mask(seq_len).to(device)\n",
    "    inpt_seq = th.ones([batch_size,seq_len,ntoken], dtype=th.float, device=device)\n",
    "    inpt_seq[0,-1,0] = 0\n",
    "    outpt_seq = th.ones_like(inpt_seq)\n",
    "    outpt_seq[0] = 0\n",
    "    return inpt_seq, outpt_seq, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq, outpt_seq = make_seq_encoding_data(batch_size, seq_len, ntoken, d_out = d_output, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_no_conflict_part_data(batch_size, seq_len, obs_dim, d_out, device='cpu'):\n",
    "    inpt = th.ones([batch_size, seq_len, obs_dim])\n",
    "    inpt[0, 1:] = 2\n",
    "    actions = th.arange(seq_len)\n",
    "    actions = actions.reshape([1,-1,1]).repeat([batch_size, 1, d_out])\n",
    "    actions[0] += 1\n",
    "\n",
    "    res, actions = make_part_observed(inpt, actions)\n",
    "    return res, actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_part_observed(inpt, actions):\n",
    "    #batch, seq, dim\n",
    "    inpt = inpt.unsqueeze(1).repeat([1,inpt.shape[1], 1, 1])\n",
    "    inpt = inpt.permute([0,3,1,2])\n",
    "    res = th.triu(inpt)\n",
    "    res = res.permute([0,2,3,1]).reshape([-1,seq_len, obs_dim])\n",
    "    \n",
    "    rep_actions = actions.repeat([1,seq_len,1]).reshape([-1,seq_len, d_output])\n",
    "    return res, rep_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conflict_part_data(batch_size, seq_len, obs_dim, d_out, device='cpu'):\n",
    "    inpt = th.ones([batch_size, seq_len, obs_dim])\n",
    "    inpt[0, :1] = 2\n",
    "    actions = th.arange(seq_len)\n",
    "    actions = actions.reshape([1,-1,1]).repeat([batch_size, 1, d_out])\n",
    "    actions[0, :1] += 1\n",
    "\n",
    "    res, actions = make_part_observed(inpt, actions)\n",
    "    rew = th.ones([batch_size * seq_len, seq_len], device = device)\n",
    "    \n",
    "    return res, actions, rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conflict_part_data_neg(batch_size, seq_len, obs_dim, d_out, device='cpu'):\n",
    "    inpt = th.ones([batch_size, seq_len, obs_dim])\n",
    "    inpt[0, -1:] = 2\n",
    "    actions = th.arange(seq_len)\n",
    "    actions = actions.reshape([1,-1,1]).repeat([batch_size, 1, d_out])\n",
    "    actions[0, -1:] += 1\n",
    "\n",
    "    res, actions = make_part_observed(inpt, actions)\n",
    "    rew = th.ones([batch_size * seq_len, seq_len], device = device)\n",
    "    scale = th.arange(seq_len).reshape([1,-1]).repeat([rew.shape[0], 1])\n",
    "    rew = rew * scale / (seq_len - 1)\n",
    "    rew[:,-1:] = 0\n",
    "    \n",
    "    res_copy = res.clone()\n",
    "    res[:seq_len] = res_copy[seq_len:]\n",
    "    res[seq_len:] = res_copy[:seq_len]\n",
    "    \n",
    "    return res, actions, rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequence(obs, acts):\n",
    "    result = th.cat((obs, acts), dim=-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_nc, act_nc = make_no_conflict_part_data(batch_size, seq_len, obs_dim, d_output, device='cpu')\n",
    "obs_c, act_c, rew_c_pos = make_conflict_part_data(batch_size, seq_len, obs_dim, d_output, device='cpu')\n",
    "obs_c_n, act_c_n, rew_c_neg = make_conflict_part_data_neg(batch_size, seq_len, obs_dim, d_output, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5000, 0.0000],\n",
       "        [0.0000, 0.5000, 0.0000],\n",
       "        [0.0000, 0.5000, 0.0000],\n",
       "        [0.0000, 0.5000, 0.0000],\n",
       "        [0.0000, 0.5000, 0.0000],\n",
       "        [0.0000, 0.5000, 0.0000]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rew_c_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_n = make_sequence(obs_c_n,act_c_n)\n",
    "seq_p = make_sequence(obs_c,act_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = th.cat((seq_n, seq_p), dim=0)\n",
    "rew = th.cat((rew_c_pos, rew_c_neg), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ModelSetup()\n",
    "seq_len = 3\n",
    "ntoken = 3 + 4\n",
    "batch_size = 2\n",
    "d_output = 1\n",
    "\n",
    "ms.d_output = d_output\n",
    "ms.nhead = 1\n",
    "ms.d_hid = 10\n",
    "ms.d_model = 10\n",
    "ms.nlayers = 1\n",
    "ms.seq_len = seq_len\n",
    "ms.dropout = 0\n",
    "ms.ntoken = 1\n",
    "ms.lr = None\n",
    "ms.device = 'cpu'\n",
    "ms.optimizer_class = th.optim.AdamW\n",
    "ms.optimizer_kwargs = {}\n",
    "ms.model_class:TransformerModel = TransformerModel\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = seq\n",
    "\n",
    "model = TransformerModel(model_setup=ms).to(device)\n",
    "with th.no_grad():\n",
    "    answer = model.forward(inpt)\n",
    "optimizer = th.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "loss = 0\n",
    "for i in range(3000):\n",
    "    result = model.forward(inpt)\n",
    "    loss = calcMSE(result, rew)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0556, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = model.forward(inpt[5].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4987],\n",
       "         [1.0001],\n",
       "         [0.9994]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rew[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, attention = model.forward(res_c[0].unsqueeze(0), return_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0152, 0.9970, 1.0143, 0.9915],\n",
       "         [1.9825, 1.9594, 1.9742, 1.9546],\n",
       "         [2.9630, 2.9960, 2.9902, 3.0279],\n",
       "         [3.9759, 3.9846, 3.9950, 3.9762],\n",
       "         [4.9925, 4.9972, 4.9863, 4.9976]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.8680e-05, 4.4999e-03, 9.9548e-01, 1.9782e-08, 2.7214e-10],\n",
       "         [9.6357e-06, 3.2713e-03, 9.9672e-01, 6.2094e-09, 6.4015e-11],\n",
       "         [1.2000e-05, 3.4134e-03, 9.9657e-01, 8.6596e-09, 1.0958e-10],\n",
       "         [7.7636e-04, 2.9140e-02, 9.7008e-01, 7.3897e-06, 4.3202e-07],\n",
       "         [1.4408e-03, 3.8925e-02, 9.5961e-01, 2.1836e-05, 1.7041e-06]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, attention = model.forward(res[4].unsqueeze(0), return_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = generate_square_subsequent_mask(seq_len).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq = th.ones([2,seq_len,1], dtype=th.float, device='cuda')\n",
    "inpt_seq[0,-1,0] = 0\n",
    "\n",
    "outpt_seq = th.ones_like(inpt_seq)\n",
    "outpt_seq[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq, outpt_seq, mask = make_mask_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(model_setup=ms).to('cuda')\n",
    "with th.no_grad():\n",
    "    model.forward(inpt_seq)\n",
    "optimizer = th.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "loss = 0\n",
    "for i in range(1000):\n",
    "    result = model.forward(inpt_seq, mask=None)\n",
    "    loss = calcMSE(result, outpt_seq)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ActiveCritic.model_src.transformer import CriticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ModelSetup()\n",
    "seq_len = 6\n",
    "d_output = 1\n",
    "ms.d_output = d_output\n",
    "ms.nhead = 1\n",
    "ms.d_hid = 10\n",
    "ms.d_model = 10\n",
    "ms.nlayers = 2\n",
    "ms.seq_len = seq_len\n",
    "ms.dropout = 0\n",
    "ms.ntoken = 1\n",
    "ms.lr = None\n",
    "ms.device = 'cuda'\n",
    "ms.optimizer_class = th.optim.AdamW\n",
    "ms.optimizer_kwargs = {}\n",
    "ms.d_result = 1\n",
    "ms.model_class:TransformerModel = CriticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq = th.ones([2,seq_len,4], dtype=th.float, device='cuda')\n",
    "inpt_seq[0,-1,0] = 0\n",
    "\n",
    "outpt_seq = th.ones([2,1], dtype=th.float, device='cuda')\n",
    "outpt_seq[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CriticTransformer(model_setup=ms).to('cuda')\n",
    "with th.no_grad():\n",
    "    a = model.forward(inpt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CriticTransformer(model_setup=ms).to('cuda')\n",
    "with th.no_grad():\n",
    "    model.forward(inpt_seq)\n",
    "optimizer = th.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "loss = 0\n",
    "for i in range(2000):\n",
    "    result = model.forward(inpt_seq)\n",
    "    loss = calcMSE(result, outpt_seq)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
