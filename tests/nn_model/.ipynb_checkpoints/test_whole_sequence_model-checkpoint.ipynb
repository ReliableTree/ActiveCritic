{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ActiveCritic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ca14a138e108>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mActiveCritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhole_sequence_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWholeSequenceActor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWholeSequenceCritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWholeSequenceModelSetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mActiveCritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTransformerModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCriticTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelSetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mActiveCritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_mask_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_seq_encoding_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_critic_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_wsm_setup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ActiveCritic'"
     ]
    }
   ],
   "source": [
    "from ActiveCritic.model_src.whole_sequence_model import WholeSequenceActor, WholeSequenceCritic, WholeSequenceModelSetup\n",
    "from ActiveCritic.model_src.transformer import TransformerModel, CriticTransformer, ModelSetup\n",
    "import torch as th\n",
    "from ActiveCritic.tests.test_utils.utils import make_mask_data, make_seq_encoding_data, make_critic_data, make_wsm_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = WholeSequenceModelSetup()\n",
    "wsm.model_setup = ModelSetup()\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "wsm.model_setup.d_output = d_output\n",
    "wsm.model_setup.nhead = 1\n",
    "wsm.model_setup.d_hid = 10\n",
    "wsm.model_setup.d_model = 10\n",
    "wsm.model_setup.nlayers = 2\n",
    "wsm.model_setup.seq_len = seq_len\n",
    "wsm.model_setup.dropout = 0\n",
    "wsm.lr = 1e-3\n",
    "wsm.model_setup.device = 'cuda'\n",
    "wsm.optimizer_class = th.optim.Adam\n",
    "wsm.optimizer_kwargs = {}\n",
    "wsm.model_setup.model_class:TransformerModel = TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = make_wsm_setup(seq_len=seq_len, d_output=d_output, model_class=TransformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa = WholeSequenceActor(wsms=wsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = th.ones([batch_size, seq_len, d_intput], dtype=th.float, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wsa.forward(inputs=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = output.shape\n",
    "assert shape[0] == batch_size\n",
    "assert shape[1] == seq_len\n",
    "assert shape[2] == d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq, outpt_seq = make_seq_encoding_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken, d_out=d_output)\n",
    "success = th.ones_like(inpt_seq, dtype=th.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa = WholeSequenceActor(wsms=wsm)\n",
    "data = inpt_seq, outpt_seq, success\n",
    "for i in range(3000):\n",
    "    result = wsa.forward(inputs=inpt_seq)\n",
    "    resp = wsa.optimizer_step(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wsa.optimizer_step(data=data)\n",
    "assert res['Trajectory Loss '] < 1e-2\n",
    "\n",
    "res = wsa.optimizer_step(data=data, prefix='test')\n",
    "assert 'Trajectory Loss test' in res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inpt_seq, outpt_seq, success\n",
    "for i in range(1000):\n",
    "    result = wsa.forward(inputs=inpt_seq)\n",
    "    wsa.optimizer_step(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wsa.optimizer_step(data=data)\n",
    "assert res['Trajectory Loss '] < 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = WholeSequenceModelSetup()\n",
    "wsm.model_setup = ModelSetup()\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_result = 1\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "wsm.model_setup.d_output = d_output\n",
    "wsm.model_setup.nhead = 1\n",
    "wsm.model_setup.d_hid = 10\n",
    "wsm.model_setup.d_model = 10\n",
    "wsm.model_setup.nlayers = 2\n",
    "wsm.model_setup.seq_len = seq_len\n",
    "wsm.model_setup.dropout = 0\n",
    "wsm.lr = 1e-3\n",
    "wsm.model_setup.d_result = d_result\n",
    "wsm.model_setup.device = 'cuda'\n",
    "wsm.optimizer_class = th.optim.Adam\n",
    "wsm.optimizer_kwargs = {}\n",
    "wsm.model_setup.model_class:TransformerModel = CriticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc = make_wsm_setup(seq_len=seq_len, d_output=d_output, model_class=CriticTransformer, d_result=d_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq, outpt_seq = make_critic_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken)\n",
    "data = inpt_seq, None, outpt_seq\n",
    "model = WholeSequenceCritic(wsc)\n",
    "for i in range(3000):\n",
    "    res = model.optimizer_step(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'critic loss': tensor(1.4034e-12, device='cuda:0'),\n",
       " 'critic loss positive': tensor(1.4211e-12, device='cuda:0'),\n",
       " 'critic loss negative': tensor(1.3858e-12, device='cuda:0')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.optimizer_step(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'critic loss': tensor(0.3448, device='cuda:0'),\n",
       " 'critic loss positive': tensor(0.6702, device='cuda:0'),\n",
       " 'critic loss negative': tensor(0.0193, device='cuda:0')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:42: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:45: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:42: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:45: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.001\n",
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9577/2482493908.py:42: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(res['Trajectory Loss '] > 1e-1, 'Init Model did not cange the parameters.')\n",
      "/tmp/ipykernel_9577/2482493908.py:45: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(res['Trajectory Loss '] < 1e-2, 'Did not converge after reinit.')\n"
     ]
    }
   ],
   "source": [
    "wsm = WholeSequenceModelSetup()\n",
    "wsm.model_setup = ModelSetup()\n",
    "seq_len = 6\n",
    "ntoken = 3\n",
    "d_output = 2\n",
    "batch_size = 2\n",
    "d_intput = 3\n",
    "wsm.model_setup.d_output = d_output\n",
    "wsm.model_setup.nhead = 1\n",
    "wsm.model_setup.d_hid = 10\n",
    "wsm.model_setup.d_model = 10\n",
    "wsm.model_setup.nlayers = 2\n",
    "wsm.model_setup.seq_len = seq_len\n",
    "wsm.model_setup.dropout = 0\n",
    "wsm.lr = 1e-3\n",
    "wsm.model_setup.device = 'cuda'\n",
    "wsm.optimizer_class = th.optim.AdamW\n",
    "wsm.optimizer_kwargs = {}\n",
    "wsm.model_setup.model_class:TransformerModel = TransformerModel\n",
    "wsa = WholeSequenceActor(wsms=wsm)\n",
    "input = th.ones([batch_size, seq_len, d_intput], dtype=th.float, device='cuda')\n",
    "output = wsa.forward(inputs=input)\n",
    "shape = output.shape\n",
    "\n",
    "assert(shape[0] == batch_size)\n",
    "assert(shape[1] == seq_len)\n",
    "assert(shape[2] == d_output)\n",
    "\n",
    "inpt_seq, outpt_seq = make_seq_encoding_data(batch_size=batch_size, seq_len=seq_len, ntoken=ntoken, d_out=d_output)\n",
    "success = th.ones_like(inpt_seq, dtype=th.bool)\n",
    "wsa = WholeSequenceActor(wsms=wsm)\n",
    "data = inpt_seq, outpt_seq, success\n",
    "for i in range(1000):\n",
    "    res = wsa.optimizer_step(data=data)\n",
    "assert(res['Trajectory Loss '] < 1e-2)\n",
    "\n",
    "res = wsa.optimizer_step(data=data, prefix='test')\n",
    "assert('Trajectory Loss test' in res)\n",
    "\n",
    "wsa.init_model()\n",
    "res = wsa.optimizer_step(data=data)\n",
    "assert(res['Trajectory Loss '] > 1e-1, 'Init Model did not cange the parameters.')\n",
    "for i in range(1000):\n",
    "    res = wsa.optimizer_step(data=data)   \n",
    "assert(res['Trajectory Loss '] < 1e-2, 'Did not converge after reinit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trajectory Loss ': tensor(2.4412e-09, device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "bee90e249730b85f00f3915f0cf4f21bc0729131dcc7008c941068256fd0d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
