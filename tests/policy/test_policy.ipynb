{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/miniconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "from active_critic.model_src.whole_sequence_model import WholeSequenceModel\n",
    "from active_critic.policy.active_critic_policy import *\n",
    "from active_critic.utils.test_utils import (make_obs_act_space,\n",
    "                                            make_wsm_setup)\n",
    "from active_critic.utils.gym_utils import (DummyExtractor, make_dummy_vec_env,\n",
    "                                           new_epoch_pap,\n",
    "                                           new_epoch_reach)\n",
    "\n",
    "from active_critic.utils.gym_utils import make_policy_dict, new_epoch_reach, make_dummy_vec_env, sample_expert_transitions, parse_sampled_transitions\n",
    "from active_critic.model_src.state_model import StateModel, StateModelArgs\n",
    "from active_critic.utils.pytorch_utils import build_tf_horizon_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.model_src.whole_sequence_model import *\n",
    "\n",
    "def make_acps(seq_len, extractor, new_epoch, batch_size = 32, device='cpu', horizon = 0):\n",
    "    acps = ActiveCriticPolicySetup()\n",
    "    acps.device=device\n",
    "    acps.epoch_len=seq_len\n",
    "    acps.extractor=extractor\n",
    "    acps.new_epoch=new_epoch\n",
    "    acps.opt_steps=100\n",
    "    acps.inference_opt_lr = 1e-1\n",
    "    acps.optimizer_class = th.optim.Adam\n",
    "    acps.optimize = True\n",
    "    acps.batch_size = batch_size\n",
    "    acps.pred_mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "    acps.opt_mask = th.zeros([batch_size, seq_len, 1], device=device, dtype=bool)\n",
    "    acps.opt_mask[:,-1] = 1\n",
    "    return acps\n",
    "\n",
    "def setup_opt_state(device='cuda'):\n",
    "    seq_len = 6\n",
    "    action_dim = 2\n",
    "    obs_dim = 3\n",
    "    batch_size = 2\n",
    "    embed_dim = 4\n",
    "    lr = 1e-3\n",
    "\n",
    "    actor_args = StateModelArgs()\n",
    "    actor_args.arch = [20, action_dim]\n",
    "    actor_args.device = device\n",
    "    actor_args.lr = lr\n",
    "    actor = StateModel(args=actor_args)\n",
    "\n",
    "    critic_args = StateModelArgs()\n",
    "    critic_args.arch = [10, 1]\n",
    "    critic_args.device = device\n",
    "    critic_args.lr = lr\n",
    "    critic = StateModel(args=critic_args)\n",
    "\n",
    "    emitter_args = StateModelArgs()\n",
    "    emitter_args.arch = [20, embed_dim]\n",
    "    emitter_args.device = device\n",
    "    emitter_args.lr = lr\n",
    "    emitter = StateModel(args=emitter_args)\n",
    "\n",
    "    predictor_args = make_wsm_setup(\n",
    "        seq_len=seq_len, d_output=embed_dim, device=device)\n",
    "    predictor_args.model_setup.d_hid = 200\n",
    "    predictor_args.model_setup.d_model = 200\n",
    "    predictor_args.model_setup.nlayers = 1\n",
    "    predictor = WholeSequenceModel(args=predictor_args)\n",
    "\n",
    "\n",
    "    acps = make_acps(\n",
    "        seq_len=seq_len, extractor=DummyExtractor(), new_epoch=new_epoch_pap, device=device, batch_size=batch_size)\n",
    "    acps.opt_steps = 2\n",
    "    obs_space, acts_space = make_obs_act_space(\n",
    "        obs_dim=obs_dim, action_dim=action_dim)\n",
    "    ac = ActiveCriticPolicy(observation_space=obs_space, \n",
    "                            action_space=acts_space,\n",
    "                            actor=actor,\n",
    "                            critic=critic,\n",
    "                            predictor=predictor,\n",
    "                            emitter=emitter,\n",
    "                            acps=acps)\n",
    "    return ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/miniconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "th.manual_seed(0)\n",
    "device = 'cpu'\n",
    "ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len = setup_opt_state(device=device)\n",
    "acps.opt_steps = 100\n",
    "acps.inference_opt_lr = 1\n",
    "observation = th.ones([batch_size, 1, obs_dim])\n",
    "action = ac.predict(observation=observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert th.all((ac.history.scores[0][:,1:,-1] - ac.history.scores[0][:,:-1,-1]) >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 6, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.history.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = action.clone().unsqueeze(1)\n",
    "all_embeddings = ac.current_embeddings.clone()\n",
    "expected_action_shape = [batch_size, 1, action_dim]\n",
    "expected_embedding_shape = [batch_size, 1, embed_dim]\n",
    "assert list(all_actions.shape) == expected_action_shape\n",
    "assert list(all_embeddings.shape) == expected_embedding_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(seq_len-1):\n",
    "    observation = th.ones([batch_size, 1, obs_dim])\n",
    "    action = ac.predict(observation=observation)\n",
    "    assert th.equal(ac.current_actions[:,:-1], all_actions)\n",
    "    assert th.equal(ac.current_embeddings[:,:-1], all_embeddings)\n",
    "    all_actions = th.cat((all_actions, action.clone().unsqueeze(1)), dim=1)\n",
    "    all_embeddings = ac.current_embeddings.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = 2*th.ones([batch_size, 1, obs_dim])\n",
    "action = ac.predict(observation=observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = action.clone().unsqueeze(1)\n",
    "all_embeddings = ac.current_embeddings.clone()\n",
    "expected_action_shape = [batch_size, 1, action_dim]\n",
    "expected_embedding_shape = [batch_size, 1, embed_dim]\n",
    "assert list(all_actions.shape) == expected_action_shape\n",
    "assert list(all_embeddings.shape) == expected_embedding_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "device = 'cpu'\n",
    "ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len = setup_opt_state(device=device)\n",
    "horizon = 0\n",
    "embeddings = th.ones([batch_size, 1, embed_dim], device=device, requires_grad=True)\n",
    "actions = th.ones([batch_size, seq_len, action_dim], device=device, requires_grad=True)\n",
    "\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "device = 'cpu'\n",
    "ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len = setup_opt_state(device=device)\n",
    "horizon = 0\n",
    "embeddings = th.ones([batch_size, 1, embed_dim], device=device, requires_grad=True)\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "seq_embeddings, actions = ac.build_sequence(embeddings=embeddings, actions=None, seq_len=seq_len, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "device = 'cpu'\n",
    "ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len = setup_opt_state(device=device)\n",
    "horizon = 0\n",
    "embeddings = th.ones([batch_size, 1, embed_dim], device=device, requires_grad=True)\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "steps = 100\n",
    "current_step = 2\n",
    "\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "goal_label = th.ones([batch_size, seq_len], device=device, dtype=th.double)\n",
    "opt_mask = th.zeros_like(goal_label, dtype=th.bool)\n",
    "opt_mask[:,-current_step:] = 1\n",
    "loss_reward, actions, seq_embeddings, scores = ac.optimize_sequence(actions=None, seq_embeddings=embeddings, pred_mask=mask, opt_mask=opt_mask, steps=steps, current_step=current_step, goal_label=goal_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "device = 'cpu'\n",
    "ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len = setup_opt_state(device=device)\n",
    "horizon = 0\n",
    "embeddings = th.ones([batch_size, 1, embed_dim], device=device, requires_grad=True)\n",
    "actions = th.ones([batch_size, seq_len, action_dim], device=device, requires_grad=True)\n",
    "\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "seq_embeddings, actions = ac.build_sequence(embeddings=embeddings, actions=actions, seq_len=seq_len, mask=mask)\n",
    "loss = ((seq_embeddings[:,-1] - th.ones_like(seq_embeddings[:,-1] ))**2).mean()\n",
    "loss.backward()\n",
    "assert (actions.grad!=0).sum() == actions.numel() - action_dim*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 0\n",
    "embeddings = th.ones([batch_size, 1, embed_dim], device=device, requires_grad=True)\n",
    "actions = th.ones([batch_size, seq_len, action_dim], device=device, requires_grad=True)\n",
    "action_optim = th.optim.Adam([actions], lr=1e-1)\n",
    "opt_paras = action_optim.state_dict()\n",
    "\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "seq_embeddings, na = ac.build_sequence(embeddings=embeddings, actions=actions, seq_len=seq_len, mask=mask)\n",
    "opt_actions = actions.detach()\n",
    "opt_actions.requires_grad = True\n",
    "next_embedding = ac.predict_step(embeddings=seq_embeddings.detach(), actions=opt_actions, mask=mask)\n",
    "seq_embedding = th.cat((embeddings[:,:1], next_embedding[:,:-1]), dim=1)\n",
    "loss = ((seq_embedding[:,-1] - th.ones_like(seq_embedding[:,-1] ))**2).mean()\n",
    "loss.backward()\n",
    "assert (opt_actions.grad != 0).sum() == batch_size*action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "device = 'cpu'\n",
    "ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len = setup_opt_state(device=device)\n",
    "horizon = 0\n",
    "embeddings = th.ones([batch_size, 1, embed_dim], device=device, requires_grad=True)\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "seq_embeddings, na = ac.build_sequence(embeddings=embeddings, actions=actions, seq_len=seq_len, mask=mask)\n",
    "actions = th.ones([batch_size, seq_len, action_dim], device=device, requires_grad=True)\n",
    "\n",
    "steps = 1\n",
    "current_step = 4\n",
    "\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "goal_label = th.ones([batch_size, seq_len], device=device, dtype=th.double)\n",
    "opt_mask = th.zeros_like(goal_label, dtype=th.bool)\n",
    "opt_mask[:,-current_step:] = 1\n",
    "loss_reward, actions, seq_embeddings, scores = ac.optimize_sequence(actions, seq_embeddings, pred_mask=mask, opt_mask=opt_mask, steps=steps, current_step=current_step, goal_label=goal_label)\n",
    "\n",
    "assert loss_reward > 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "device = 'cpu'\n",
    "ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len = setup_opt_state(device=device)\n",
    "horizon = 0\n",
    "embeddings = th.ones([batch_size, 1, embed_dim], device=device, requires_grad=True)\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "seq_embeddings, na = ac.build_sequence(embeddings=embeddings, actions=actions, seq_len=seq_len, mask=mask)\n",
    "actions = th.ones([batch_size, seq_len, action_dim], device=device, requires_grad=True)\n",
    "org_actions = actions.detach().clone()\n",
    "steps = 100\n",
    "current_step = 2\n",
    "\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "goal_label = th.ones([batch_size, seq_len], device=device, dtype=th.double)\n",
    "opt_mask = th.zeros_like(goal_label, dtype=th.bool)\n",
    "opt_mask[:,-current_step:] = 1\n",
    "loss_reward, actions, seq_embeddings, scores = ac.optimize_sequence(actions, seq_embeddings, pred_mask=mask, opt_mask=opt_mask, steps=steps, current_step=current_step, goal_label=goal_label)\n",
    "\n",
    "assert loss_reward < 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert th.equal(actions[:, :current_step], org_actions[:,:current_step])\n",
    "assert not th.equal(actions[:,current_step:], org_actions[:,current_step:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "device = 'cpu'\n",
    "ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len = setup_opt_state(device=device)\n",
    "seq_len = 100\n",
    "horizon = 0\n",
    "steps = 100\n",
    "current_step = 0\n",
    "embeddings = th.ones([batch_size, 1, embed_dim], device=device, requires_grad=True)\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "seq_embeddings, na = ac.build_sequence(embeddings=embeddings, actions=actions, seq_len=seq_len, mask=mask)\n",
    "actions = th.ones([batch_size, seq_len, action_dim], device=device, requires_grad=True)\n",
    "org_actions = actions.detach().clone()\n",
    "\n",
    "goal_label = th.ones([batch_size, seq_len], device=device, dtype=th.double)\n",
    "opt_mask = th.zeros_like(goal_label, dtype=th.bool)\n",
    "opt_mask[:,-1] = 1\n",
    "acps.optimizer_class = th.optim.Adam\n",
    "acps.inference_opt_lr = 1e-1\n",
    "loss_reward_full, actions, seq_embeddings, scores = ac.optimize_sequence(actions, seq_embeddings, pred_mask=mask, opt_mask=opt_mask, steps=steps, current_step=current_step, goal_label=goal_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reward_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(0)\n",
    "device = 'cpu'\n",
    "ac, acps, action_dim, obs_dim, batch_size, embed_dim, seq_len = setup_opt_state(device=device)\n",
    "seq_len = 100\n",
    "horizon = 0\n",
    "steps = 100\n",
    "current_step = 98\n",
    "embeddings = th.ones([batch_size, 1, embed_dim], device=device, requires_grad=True)\n",
    "mask = build_tf_horizon_mask(seq_len=seq_len, horizon=horizon, device=device)\n",
    "seq_embeddings, na = ac.build_sequence(embeddings=embeddings, actions=actions, seq_len=seq_len, mask=mask)\n",
    "actions = th.ones([batch_size, seq_len, action_dim], device=device, requires_grad=True)\n",
    "org_actions = actions.detach().clone()\n",
    "\n",
    "goal_label = th.ones([batch_size, seq_len], device=device, dtype=th.double)\n",
    "opt_mask = th.zeros_like(goal_label, dtype=th.bool)\n",
    "opt_mask[:,-1] = 1\n",
    "acps.optimizer_class = th.optim.Adam\n",
    "acps.inference_opt_lr = 1e-1\n",
    "loss_reward_part, actions, seq_embeddings, scores = ac.optimize_sequence(actions, seq_embeddings, pred_mask=mask, opt_mask=opt_mask, steps=steps, current_step=current_step, goal_label=goal_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reward_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert loss_reward_part > loss_reward_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55d3bc2b8c4272bbea65d85e1c1c189fa11db70743df9511061edc4d7dc4f3cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
