{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env, \n",
    "    make_dummy_vec_env, \n",
    "    sample_expert_transitions_rollouts, \n",
    "    make_pomdp_rollouts, \n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew_det\n",
    ")\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "from active_critic.model_src.transformer import PositionalEncoding\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "import copy\n",
    "\n",
    "from active_critic.TQC.tqc import TQC\n",
    "from active_critic.TQC.tqc_policy import TQCPolicyEval\n",
    "\n",
    "def run_experiment(device):\n",
    "    pass\n",
    "device='cuda'\n",
    "lookup_freq = 1\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, env, 10)\n",
    "env.envs[0].reset_count = 0\n",
    "pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200, lookup_freq=lookup_freq)\n",
    "policy_kwargs = {'net_arch' : [32, 32, 32]}\n",
    "tqc_learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device, policy_kwargs=policy_kwargs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqc_learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device, policy_kwargs=policy_kwargs)\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=pomdp_transitions,\n",
    "    device=device,\n",
    "    policy=tqc_learner.policy)\n",
    "th.save(bc_trainer.policy.state_dict(), 'test')\n",
    "policy = copy.deepcopy(bc_trainer.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_learner(env_tag, logname, save_path, seq_len, n_demonstrations, bc_epochs, n_samples, device, learner:TQC=None):\n",
    "    lookup_freq = 1000\n",
    "    env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "    val_env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "    transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, val_env, n_demonstrations)\n",
    "\n",
    "    pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "    pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "    if learner is None:\n",
    "        bc_learner = bc.BC(\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            demonstrations=pomdp_transitions,\n",
    "            device=device)\n",
    "    else:\n",
    "        bc_learner = bc.BC(\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            demonstrations=pomdp_transitions,\n",
    "            device=device,\n",
    "            policy=learner.policy)\n",
    "\n",
    "    pomdp_env_val, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "\n",
    "    tboard = TBoardGraphs(logname=logname + ' BC' , data_path='/data/bing/hendrik/gboard/')\n",
    "    best_succes_rate = -1\n",
    "    best_model = None\n",
    "    runs_per_epoch = 20\n",
    "    for i in range(bc_epochs):\n",
    "        bc_learner.train(n_epochs=runs_per_epoch)\n",
    "        success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=bc_learner.policy, epsiodes=200)\n",
    "        success_rate = success.mean()\n",
    "        tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "        tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=i)\n",
    "        if success_rate > best_succes_rate:\n",
    "            best_succes_rate = success_rate\n",
    "            th.save(bc_learner.policy.state_dict(), save_path + logname + ' BC best')\n",
    "            print(save_path + logname + ' BC best')\n",
    "    \n",
    "    if learner is not None:\n",
    "\n",
    "        tboard = TBoardGraphs(logname=logname + str(' Reinforcement') , data_path='/data/bing/hendrik/gboard/')\n",
    "        learner.policy.load_state_dict(th.load(save_path + logname + ' BC best'))\n",
    "        success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=learner.policy, epsiodes=200)\n",
    "        tboard.addValidationScalar('Reloaded Success Rate', value=th.tensor(success.mean()), stepid=0)\n",
    "        tboard.addValidationScalar('Reloaded Reward', value=th.tensor(rews.mean()), stepid=0)\n",
    "\n",
    "        tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=pomdp_env.envs[0].reset_count)\n",
    "        tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=pomdp_env.envs[0].reset_count)\n",
    "\n",
    "        while learner.env.envs[0].reset_count <= n_samples:\n",
    "            print('before learn')\n",
    "            learner.learn(2000)\n",
    "            print('after learn')\n",
    "            print(learner.env.envs[0].reset_count)\n",
    "            success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=learner.policy, epsiodes=200)\n",
    "            success_rate = success.mean()\n",
    "            tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=pomdp_env.envs[0].reset_count)\n",
    "            tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=pomdp_env.envs[0].reset_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "env_tag = 'pickplace'\n",
    "seq_len = 200\n",
    "pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "tqc_learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device)\n",
    "evaluate_learner(env_tag, 'TQC 10', save_path='/data/bing/hendrik/Evaluate Baseline/', seq_len=seq_len, n_demonstrations=10, bc_epochs=400, n_samples=400, device='cuda', learner=tqc_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "venv = make_vec_env(\"seals/CartPole-v0\", n_envs=8, rng=rng)\n",
    "learner = PPO(\n",
    "    env=venv,\n",
    "    policy=MlpPolicy,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    ")\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "gail_trainer = GAIL(\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=1024,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAIL(env_tag, logname, seq_len, n_demonstrations, n_samples, learner, pomdp_env, save_path, bc_epochs, bc_logname):\n",
    "    lookup_freq = 1000\n",
    "    env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "    val_env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "    transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, val_env, n_demonstrations)\n",
    "\n",
    "    pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "    pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "    pomdp_env_val, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "    if (not os.path.isfile(save_path + bc_logname + ' BC best')):\n",
    "        print('BC')\n",
    "        bc_learner = bc.BC(\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            demonstrations=pomdp_transitions,\n",
    "            device=device,\n",
    "            policy=learner.policy)\n",
    "        \n",
    "        tboard = TBoardGraphs(logname=logname + ' BC' , data_path='/data/bing/hendrik/gboard/')\n",
    "        best_succes_rate = -1\n",
    "        best_model = None\n",
    "        runs_per_epoch = 20\n",
    "        for i in range(bc_epochs):\n",
    "            bc_learner.train(n_epochs=runs_per_epoch)\n",
    "            success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=bc_learner.policy, epsiodes=200)\n",
    "            success_rate = success.mean()\n",
    "            tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "            tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=i)\n",
    "            if success_rate > best_succes_rate:\n",
    "                best_succes_rate = success_rate\n",
    "                th.save(bc_learner.policy.state_dict(), save_path + bc_logname + ' BC best')\n",
    "                print(save_path + logname + ' BC best')\n",
    "    else:\n",
    "        print('skipping BC')\n",
    "\n",
    "    reward_net = BasicRewardNet(\n",
    "        pomdp_env.observation_space, pomdp_env.action_space, normalize_input_layer=RunningNorm\n",
    "    )\n",
    "\n",
    "    learner.policy.load_state_dict(th.load(save_path + bc_logname + ' BC best'))\n",
    "\n",
    "    gail_trainer = GAIL(\n",
    "        demonstrations=pomdp_transitions,\n",
    "        demo_batch_size=min(1024, len(transitions)),\n",
    "        gen_replay_buffer_capacity=2048,\n",
    "        n_disc_updates_per_round=4,\n",
    "        venv=pomdp_env,\n",
    "        gen_algo=learner,\n",
    "        reward_net=reward_net,\n",
    "    ) \n",
    "\n",
    "    tboard = TBoardGraphs(logname=logname , data_path=save_path)\n",
    "    success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=learner.policy, epsiodes=200)\n",
    "    success_rate = success.mean()\n",
    "    tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=min(learner.env.envs[0].reset_count, n_samples))\n",
    "    tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=min(learner.env.envs[0].reset_count, n_samples))\n",
    "\n",
    "    while learner.env.envs[0].reset_count <= n_samples:\n",
    "        print(f'nsamples: {n_samples}')\n",
    "        print(f'learner.env.envs[0].reset_count')\n",
    "        print('before learn')\n",
    "        gail_trainer.train(5000)\n",
    "        print('after learn')\n",
    "        print(learner.env.envs[0].reset_count)\n",
    "        success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=learner.policy, epsiodes=200)\n",
    "        success_rate = success.mean()\n",
    "        tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=min(learner.env.envs[0].reset_count, n_samples))\n",
    "        tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=min(learner.env.envs[0].reset_count, n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAIL_PPO(device):\n",
    "    env_tag = 'pickplace'\n",
    "    lookup_freq = 1000\n",
    "    lr = 1e-3\n",
    "    seq_lens = [50, 100, 200]\n",
    "    for i in range(5):\n",
    "        for seq_len in seq_lens:\n",
    "            demonstrations = 6\n",
    "            for j in range(4):\n",
    "                demonstrations += 2\n",
    "                pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "                learner = PPO(\n",
    "                        env=pomdp_env,\n",
    "                        policy=MlpPolicy,\n",
    "                        batch_size=64,\n",
    "                        ent_coef=0.0,\n",
    "                        learning_rate=lr,\n",
    "                        n_epochs=10,\n",
    "                        device=device\n",
    "                    )\n",
    "                logname = f'GAIL + PPO lr: {lr}, Demonstrations: {demonstrations}, seq_len: {seq_len}'\n",
    "                bc_logname = f'GAIL + PPO Demonstrations: {demonstrations}, seq_len: {seq_len}'\n",
    "                evaluate_GAIL(\n",
    "                    env_tag, \n",
    "                    logname, \n",
    "                    seq_len, \n",
    "                    demonstrations, \n",
    "                    400, \n",
    "                    learner, \n",
    "                    pomdp_env, \n",
    "                    save_path='/data/bing/hendrik/Evaluate Baseline/',\n",
    "                    bc_epochs = 400,\n",
    "                    bc_logname = bc_logname)\n",
    "        lr = lr * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAIL_TQC(device):\n",
    "    env_tag = 'pickplace'\n",
    "    lookup_freq = 1000\n",
    "    lr = 1e-3\n",
    "    seq_lens = [50, 100, 200]\n",
    "    for i in range(5):\n",
    "        for seq_len in seq_lens:\n",
    "            demonstrations = 6\n",
    "            for j in range(4):\n",
    "                demonstrations += 2\n",
    "                pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "                learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device, learning_rate=lr)\n",
    "\n",
    "                logname = f'GAIL + TQC lr: {lr}, Demonstrations: {demonstrations}, seq_len: {seq_len}'\n",
    "                evaluate_GAIL(env_tag, logname, seq_len, demonstrations, 400, learner, pomdp_env, save_path='/data/bing/hendrik/Evaluate Baseline/')\n",
    "        lr = lr * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/metaworld/policies/policy.py:41: UserWarning: Constant(s) may be too high. Environments clip response to [-1, 1]\n",
      "  warnings.warn('Constant(s) may be too high. Environments clip response to [-1, 1]')\n",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00568 |\n",
      "|    entropy        | 5.68     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 207      |\n",
      "|    loss           | 5.07     |\n",
      "|    neglogp        | 5.08     |\n",
      "|    prob_true_act  | 0.0115   |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240batch [00:00, 248.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/bing/hendrik/Evaluate Baseline/GAIL + PPO lr: 0.001, Demonstrations: 8, seq_len: 50 BC best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.0047  |\n",
      "|    entropy        | 4.7      |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 218      |\n",
      "|    loss           | 3.15     |\n",
      "|    neglogp        | 3.16     |\n",
      "|    prob_true_act  | 0.0476   |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240batch [00:00, 247.44batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate_GAIL_PPO(device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [33], line 23\u001b[0m, in \u001b[0;36mevaluate_GAIL_PPO\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m     21\u001b[0m         logname \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGAIL + PPO lr: \u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m, Demonstrations: \u001b[39m\u001b[39m{\u001b[39;00mdemonstrations\u001b[39m}\u001b[39;00m\u001b[39m, seq_len: \u001b[39m\u001b[39m{\u001b[39;00mseq_len\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     22\u001b[0m         bc_logname \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGAIL + PPO Demonstrations: \u001b[39m\u001b[39m{\u001b[39;00mdemonstrations\u001b[39m}\u001b[39;00m\u001b[39m, seq_len: \u001b[39m\u001b[39m{\u001b[39;00mseq_len\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 23\u001b[0m         evaluate_GAIL(\n\u001b[1;32m     24\u001b[0m             env_tag, \n\u001b[1;32m     25\u001b[0m             logname, \n\u001b[1;32m     26\u001b[0m             seq_len, \n\u001b[1;32m     27\u001b[0m             demonstrations, \n\u001b[1;32m     28\u001b[0m             \u001b[39m400\u001b[39;49m, \n\u001b[1;32m     29\u001b[0m             learner, \n\u001b[1;32m     30\u001b[0m             pomdp_env, \n\u001b[1;32m     31\u001b[0m             save_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/data/bing/hendrik/Evaluate Baseline/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m             bc_epochs \u001b[39m=\u001b[39;49m \u001b[39m400\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m             bc_logname \u001b[39m=\u001b[39;49m bc_logname)\n\u001b[1;32m     34\u001b[0m lr \u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m \u001b[39m0.6\u001b[39m\n",
      "Cell \u001b[0;32mIn [32], line 25\u001b[0m, in \u001b[0;36mevaluate_GAIL\u001b[0;34m(env_tag, logname, seq_len, n_demonstrations, n_samples, learner, pomdp_env, save_path, bc_epochs, bc_logname)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(bc_epochs):\n\u001b[1;32m     24\u001b[0m     bc_learner\u001b[39m.\u001b[39mtrain(n_epochs\u001b[39m=\u001b[39mruns_per_epoch)\n\u001b[0;32m---> 25\u001b[0m     success, rews \u001b[39m=\u001b[39m get_avr_succ_rew_det(env\u001b[39m=\u001b[39;49mpomdp_env_val, learner\u001b[39m=\u001b[39;49mbc_learner\u001b[39m.\u001b[39;49mpolicy, epsiodes\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m)\n\u001b[1;32m     26\u001b[0m     success_rate \u001b[39m=\u001b[39m success\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     27\u001b[0m     tboard\u001b[39m.\u001b[39maddValidationScalar(\u001b[39m'\u001b[39m\u001b[39mReward\u001b[39m\u001b[39m'\u001b[39m, value\u001b[39m=\u001b[39mth\u001b[39m.\u001b[39mtensor(rews\u001b[39m.\u001b[39mmean()), stepid\u001b[39m=\u001b[39mi)\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/utils/gym_utils.py:426\u001b[0m, in \u001b[0;36mget_avr_succ_rew_det\u001b[0;34m(env, learner, epsiodes)\u001b[0m\n\u001b[1;32m    424\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m--> 426\u001b[0m     action, _ \u001b[39m=\u001b[39m learner\u001b[39m.\u001b[39;49mpredict(obs, deterministic\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    427\u001b[0m     obs, rew, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    428\u001b[0m     rews\u001b[39m.\u001b[39mappend(rew)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/stable_baselines3/common/policies.py:363\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    360\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    362\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 363\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(observation, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[1;32m    364\u001b[0m \u001b[39m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[1;32m    365\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/stable_baselines3/common/policies.py:656\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, observation: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    649\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[39m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[39m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 656\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_distribution(observation)\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/stable_baselines3/common/policies.py:685\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    683\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_features(obs)\n\u001b[1;32m    684\u001b[0m latent_pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_extractor\u001b[39m.\u001b[39mforward_actor(features)\n\u001b[0;32m--> 685\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/stable_baselines3/common/policies.py:630\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_action_dist_from_latent\u001b[39m(\u001b[39mself\u001b[39m, latent_pi: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Distribution:\n\u001b[1;32m    624\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[39m    Retrieve action distribution given the latent codes.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[39m    :param latent_pi: Latent code for the actor\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m    :return: Action distribution\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     mean_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_net(latent_pi)\n\u001b[1;32m    632\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[1;32m    633\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist\u001b[39m.\u001b[39mproba_distribution(mean_actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_std)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_GAIL_PPO(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_GAIL_TQC(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
