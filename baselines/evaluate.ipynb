{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env, \n",
    "    make_dummy_vec_env, \n",
    "    sample_expert_transitions_rollouts, \n",
    "    make_pomdp_rollouts, \n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew_det\n",
    ")\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "from active_critic.model_src.transformer import PositionalEncoding\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "import copy\n",
    "\n",
    "from active_critic.TQC.tqc import TQC\n",
    "from active_critic.TQC.tqc_policy import TQCPolicyEval\n",
    "\n",
    "def run_experiment(device):\n",
    "    pass\n",
    "device='cuda'\n",
    "lookup_freq = 1\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, env, 10)\n",
    "env.envs[0].reset_count = 0\n",
    "pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200, lookup_freq=lookup_freq)\n",
    "policy_kwargs = {'net_arch' : [32, 32, 32]}\n",
    "tqc_learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device, policy_kwargs=policy_kwargs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling transitions. 1\n"
     ]
    }
   ],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env\n",
    "import numpy as np\n",
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env, \n",
    "    make_dummy_vec_env, \n",
    "    sample_expert_transitions_rollouts, \n",
    "    sample_expert_transitions,\n",
    "    make_pomdp_rollouts, \n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew_det\n",
    ")\n",
    "tag = 'pickplace'\n",
    "num_cpu = 1\n",
    "seq_len = 5\n",
    "env, expert = make_vec_env(tag, num_cpu, seq_len=seq_len)\n",
    "transitions = sample_expert_transitions(expert.predict, env=env, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "i = 0\n",
    "done = False\n",
    "while not done:\n",
    "    oobsv, rew, done, info = env.step(np.array([[0,0,0,0]]))\n",
    "    done = done[0]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqc_learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device, policy_kwargs=policy_kwargs)\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=pomdp_transitions,\n",
    "    device=device,\n",
    "    policy=tqc_learner.policy)\n",
    "th.save(bc_trainer.policy.state_dict(), 'test')\n",
    "policy = copy.deepcopy(bc_trainer.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_learner(env_tag, logname, save_path, seq_len, n_demonstrations, bc_epochs, n_samples, device, learner:TQC=None):\n",
    "    lookup_freq = 1000\n",
    "    env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "    val_env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "    transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, val_env, n_demonstrations)\n",
    "\n",
    "    pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "    pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "    if learner is None:\n",
    "        bc_learner = bc.BC(\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            demonstrations=pomdp_transitions,\n",
    "            device=device)\n",
    "    else:\n",
    "        bc_learner = bc.BC(\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            demonstrations=pomdp_transitions,\n",
    "            device=device,\n",
    "            policy=learner.policy)\n",
    "\n",
    "    pomdp_env_val, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "\n",
    "    tboard = TBoardGraphs(logname=logname + ' BC' , data_path='/data/bing/hendrik/gboard/')\n",
    "    best_succes_rate = -1\n",
    "    best_model = None\n",
    "    runs_per_epoch = 20\n",
    "    for i in range(bc_epochs):\n",
    "        bc_learner.train(n_epochs=runs_per_epoch)\n",
    "        success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=bc_learner.policy, epsiodes=200)\n",
    "        success_rate = success.mean()\n",
    "        tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "        tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=i)\n",
    "        if success_rate > best_succes_rate:\n",
    "            best_succes_rate = success_rate\n",
    "            th.save(bc_learner.policy.state_dict(), save_path + logname + ' BC best')\n",
    "            print(save_path + logname + ' BC best')\n",
    "    \n",
    "    if learner is not None:\n",
    "\n",
    "        tboard = TBoardGraphs(logname=logname + str(' Reinforcement') , data_path='/data/bing/hendrik/gboard/')\n",
    "        learner.policy.load_state_dict(th.load(save_path + logname + ' BC best'))\n",
    "        success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=learner.policy, epsiodes=200)\n",
    "        tboard.addValidationScalar('Reloaded Success Rate', value=th.tensor(success.mean()), stepid=0)\n",
    "        tboard.addValidationScalar('Reloaded Reward', value=th.tensor(rews.mean()), stepid=0)\n",
    "\n",
    "        tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=pomdp_env.envs[0].reset_count)\n",
    "        tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=pomdp_env.envs[0].reset_count)\n",
    "\n",
    "        while learner.env.envs[0].reset_count <= n_samples:\n",
    "            print('before learn')\n",
    "            learner.learn(2000)\n",
    "            print('after learn')\n",
    "            print(learner.env.envs[0].reset_count)\n",
    "            success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=learner.policy, epsiodes=200)\n",
    "            success_rate = success.mean()\n",
    "            tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=pomdp_env.envs[0].reset_count)\n",
    "            tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=pomdp_env.envs[0].reset_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "env_tag = 'pickplace'\n",
    "seq_len = 200\n",
    "pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "tqc_learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device)\n",
    "evaluate_learner(env_tag, 'TQC 10', save_path='/data/bing/hendrik/Evaluate Baseline/', seq_len=seq_len, n_demonstrations=10, bc_epochs=400, n_samples=400, device='cuda', learner=tqc_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "venv = make_vec_env(\"seals/CartPole-v0\", n_envs=8, rng=rng)\n",
    "learner = PPO(\n",
    "    env=venv,\n",
    "    policy=MlpPolicy,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    ")\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "gail_trainer = GAIL(\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=1024,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAIL(env_tag, logname, seq_len, n_demonstrations, n_samples, learner, pomdp_env, save_path, bc_epochs, bc_logname):\n",
    "    lookup_freq = 1000\n",
    "    env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "    val_env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "    transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, val_env, n_demonstrations)\n",
    "\n",
    "    pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "    pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "    pomdp_env_val, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "    if (not os.path.isfile(save_path + bc_logname + ' BC best')):\n",
    "        print('BC')\n",
    "        bc_learner = bc.BC(\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            demonstrations=pomdp_transitions,\n",
    "            device=device,\n",
    "            policy=learner.policy)\n",
    "        \n",
    "        tboard = TBoardGraphs(logname=logname + ' BC' , data_path='/data/bing/hendrik/gboard/')\n",
    "        best_succes_rate = -1\n",
    "        best_model = None\n",
    "        runs_per_epoch = 20\n",
    "        for i in range(bc_epochs):\n",
    "            bc_learner.train(n_epochs=runs_per_epoch)\n",
    "            success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=bc_learner.policy, epsiodes=200)\n",
    "            success_rate = success.mean()\n",
    "            tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "            tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=i)\n",
    "            if success_rate > best_succes_rate:\n",
    "                best_succes_rate = success_rate\n",
    "                th.save(bc_learner.policy.state_dict(), save_path + bc_logname + ' BC best')\n",
    "                print(save_path + logname + ' BC best')\n",
    "    else:\n",
    "        print('skipping BC')\n",
    "\n",
    "    reward_net = BasicRewardNet(\n",
    "        pomdp_env.observation_space, pomdp_env.action_space, normalize_input_layer=RunningNorm\n",
    "    )\n",
    "\n",
    "    learner.policy.load_state_dict(th.load(save_path + bc_logname + ' BC best'))\n",
    "\n",
    "    gail_trainer = GAIL(\n",
    "        demonstrations=pomdp_transitions,\n",
    "        demo_batch_size=min(1024, len(transitions)),\n",
    "        gen_replay_buffer_capacity=2048,\n",
    "        n_disc_updates_per_round=4,\n",
    "        venv=pomdp_env,\n",
    "        gen_algo=learner,\n",
    "        reward_net=reward_net,\n",
    "    ) \n",
    "\n",
    "    tboard = TBoardGraphs(logname=logname , data_path=save_path)\n",
    "    success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=learner.policy, epsiodes=200)\n",
    "    success_rate = success.mean()\n",
    "    tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=min(learner.env.envs[0].reset_count, n_samples))\n",
    "    tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=min(learner.env.envs[0].reset_count, n_samples))\n",
    "\n",
    "    while learner.env.envs[0].reset_count <= n_samples:\n",
    "        print(f'nsamples: {n_samples}')\n",
    "        print(f'learner.env.envs[0].reset_count')\n",
    "        print('before learn')\n",
    "        gail_trainer.train(5000)\n",
    "        print('after learn')\n",
    "        print(learner.env.envs[0].reset_count)\n",
    "        success, rews = get_avr_succ_rew_det(env=pomdp_env_val, learner=learner.policy, epsiodes=200)\n",
    "        success_rate = success.mean()\n",
    "        tboard.addValidationScalar('Reward', value=th.tensor(rews.mean()), stepid=min(learner.env.envs[0].reset_count, n_samples))\n",
    "        tboard.addValidationScalar('Success Rate', value=th.tensor(success_rate), stepid=min(learner.env.envs[0].reset_count, n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAIL_PPO(device):\n",
    "    env_tag = 'pickplace'\n",
    "    lookup_freq = 1000\n",
    "    lr = 1e-3\n",
    "    seq_lens = [50, 100, 200]\n",
    "    for i in range(5):\n",
    "        for seq_len in seq_lens:\n",
    "            demonstrations = 6\n",
    "            for j in range(4):\n",
    "                demonstrations += 2\n",
    "                pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "                learner = PPO(\n",
    "                        env=pomdp_env,\n",
    "                        policy=MlpPolicy,\n",
    "                        batch_size=64,\n",
    "                        ent_coef=0.0,\n",
    "                        learning_rate=lr,\n",
    "                        n_epochs=10,\n",
    "                        device=device\n",
    "                    )\n",
    "                logname = f'GAIL + PPO lr: {lr}, Demonstrations: {demonstrations}, seq_len: {seq_len}'\n",
    "                bc_logname = f'GAIL + PPO Demonstrations: {demonstrations}, seq_len: {seq_len}'\n",
    "                evaluate_GAIL(\n",
    "                    env_tag=env_tag, \n",
    "                    logname=logname, \n",
    "                    seq_len=seq_len, \n",
    "                    demonstrations=demonstrations, \n",
    "                    n_samples = 400, \n",
    "                    learner = learner, \n",
    "                    pomdp_env = pomdp_env, \n",
    "                    save_path='/data/bing/hendrik/Evaluate Baseline/',\n",
    "                    bc_epochs = 400,\n",
    "                    bc_logname = bc_logname)\n",
    "        lr = lr * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAIL_TQC(device):\n",
    "    env_tag = 'pickplace'\n",
    "    lookup_freq = 1000\n",
    "    lr = 1e-3\n",
    "    seq_lens = [50, 100, 200]\n",
    "    for i in range(5):\n",
    "        for seq_len in seq_lens:\n",
    "            demonstrations = 6\n",
    "            for j in range(4):\n",
    "                demonstrations += 2\n",
    "                pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "                learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device, learning_rate=lr)\n",
    "                logname = f'GAIL + TQC lr: {lr}, Demonstrations: {demonstrations}, seq_len: {seq_len}'\n",
    "                bc_logname = f'GAIL + TQC Demonstrations: {demonstrations}, seq_len: {seq_len}'\n",
    "                evaluate_GAIL(\n",
    "                    env_tag=env_tag, \n",
    "                    logname=logname, \n",
    "                    seq_len=seq_len, \n",
    "                    demonstrations=demonstrations, \n",
    "                    n_samples = 400, \n",
    "                    learner = learner, \n",
    "                    pomdp_env = pomdp_env, \n",
    "                    save_path='/data/bing/hendrik/Evaluate Baseline/',\n",
    "                    bc_epochs = 400,\n",
    "                    bc_logname = bc_logname)\n",
    "        lr = lr * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAIL_TQC(device):\n",
    "    env_tag = 'pickplace'\n",
    "    lookup_freq = 1000\n",
    "    lr = 1e-3\n",
    "    seq_lens = [50, 100, 200]\n",
    "    for i in range(5):\n",
    "        for seq_len in seq_lens:\n",
    "            demonstrations = 6\n",
    "            for j in range(4):\n",
    "                demonstrations += 2\n",
    "                pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name=env_tag, seq_len=seq_len, lookup_freq=lookup_freq)\n",
    "                learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device, learning_rate=lr)\n",
    "\n",
    "                logname = f'GAIL + TQC lr: {lr}, Demonstrations: {demonstrations}, seq_len: {seq_len}'\n",
    "                bc_logname = f'GAIL + TQC Demonstrations: {demonstrations}, seq_len: {seq_len}'\n",
    "                evaluate_GAIL(\n",
    "                    env_tag=env_tag, \n",
    "                    logname=logname, \n",
    "                    seq_len=seq_len, \n",
    "                    demonstrations=demonstrations, \n",
    "                    n_samples = 400, \n",
    "                    learner = learner, \n",
    "                    pomdp_env = pomdp_env, \n",
    "                    save_path='/data/bing/hendrik/Evaluate Baseline/',\n",
    "                    bc_epochs = 400,\n",
    "                    bc_logname = bc_logname)\n",
    "        lr = lr * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_GAIL_PPO(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_GAIL_TQC(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
