{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env, \n",
    "    make_dummy_vec_env, \n",
    "    sample_expert_transitions_rollouts, \n",
    "    make_pomdp_rollouts, \n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew\n",
    ")\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "from active_critic.model_src.transformer import PositionalEncoding\n",
    "\n",
    "import copy\n",
    "\n",
    "from active_critic.utils.gym_utils import make_policy_dict, ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE, ResetCounterWrapper, TimeLimit, StrictSeqLenWrapper, ImitationLearningWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "def run_experiment(device):\n",
    "    lookup_freq = 10\n",
    "    env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "    val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "    transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, env, 10)\n",
    "    env.envs[0].reset_count = 0\n",
    "    pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "    pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "    pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200, lookup_freq=lookup_freq)\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", pomdp_env, verbose=1)\n",
    "\n",
    "    bc_trainer = bc.BC(\n",
    "        observation_space=env.observation_space,\n",
    "        action_space=env.action_space,\n",
    "        policy=model.policy,\n",
    "        demonstrations=pomdp_transitions,\n",
    "        device=device\n",
    "    )\n",
    "    bc_trainer = bc.BC(\n",
    "        observation_space=env.observation_space,\n",
    "        action_space=env.action_space,\n",
    "        demonstrations=pomdp_transitions,\n",
    "        policy=model.policy,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    tboard = TBoardGraphs(logname='BC pickplace lookup '+str(lookup_freq) , data_path='/data/bing/hendrik/gboard/')\n",
    "    for i in range(10):\n",
    "        bc_trainer.train(n_epochs=20)\n",
    "        success, rews = get_avr_succ_rew(env=pomdp_env, learner=bc_trainer.policy, epsiodes=200)\n",
    "        tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "        tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/data/bing/hendrik/Baselines_Stats/TQC_GAIL_pickplace_lr_0.001_demonstrations_10_id_0/learner_stats_gail', 'rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success_rate': array([0., 0.]), 'step': array([  0, 100])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "lookup_freq = 10\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, env, 10)\n",
    "env.envs[0].reset_count = 0\n",
    "pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200, lookup_freq=lookup_freq)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", pomdp_env, verbose=1)\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    policy=model.policy,\n",
    "    demonstrations=pomdp_transitions,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "tboard = TBoardGraphs(logname='BC pickplace lookup test '+str(lookup_freq) , data_path='/data/bing/hendrik/gboard/')\n",
    "tboard_ppo = TBoardGraphs(logname='BC + PPO '+str(lookup_freq) , data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10):\n",
    "    bc_trainer.train(n_epochs=20)\n",
    "    success, rews = get_avr_succ_rew(env=pomdp_env, learner=bc_trainer.policy, epsiodes=200)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=i)\n",
    "\n",
    "for i in range(1000):\n",
    "    model.learn(2000)\n",
    "    success, rews = get_avr_succ_rew(env=pomdp_env, learner=bc_trainer.policy, epsiodes=200)\n",
    "    tboard_ppo.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=i*10)\n",
    "    tboard_ppo.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=i*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env, \n",
    "    make_dummy_vec_env, \n",
    "    sample_expert_transitions_rollouts, \n",
    "    make_pomdp_rollouts, \n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew\n",
    ")\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "from active_critic.model_src.transformer import PositionalEncoding\n",
    "\n",
    "import copy\n",
    "\n",
    "from active_critic.utils.gym_utils import make_policy_dict, ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE, ResetCounterWrapper, TimeLimit, StrictSeqLenWrapper, ImitationLearningWrapper\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "from sb3_contrib import TQC\n",
    "\n",
    "lookup_freq = 10\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, env, 10)\n",
    "env.envs[0].reset_count = 0\n",
    "pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200, lookup_freq=lookup_freq)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", pomdp_env, verbose=1)\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    policy=model.policy,\n",
    "    demonstrations=pomdp_transitions,\n",
    "    device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=pomdp_transitions,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward32Policy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential(\n",
       "      (0): Linear(in_features=39, out_features=32, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=32, out_features=4, bias=True)\n",
       "  (value_net): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_trainer.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer.train(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
