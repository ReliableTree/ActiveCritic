{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env, \n",
    "    make_dummy_vec_env, \n",
    "    sample_expert_transitions_rollouts, \n",
    "    make_pomdp_rollouts, \n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew\n",
    ")\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "from active_critic.model_src.transformer import PositionalEncoding\n",
    "\n",
    "import copy\n",
    "\n",
    "from active_critic.utils.gym_utils import make_policy_dict, ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE, ResetCounterWrapper, TimeLimit, StrictSeqLenWrapper, ImitationLearningWrapper\n",
    "from active_critic.TQC.tqc import TQC\n",
    "from active_critic.TQC.tqc_policy import TQCPolicyEval\n",
    "\n",
    "def run_experiment(device):\n",
    "    pass\n",
    "device='cuda'\n",
    "lookup_freq = 1\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, env, 10)\n",
    "env.envs[0].reset_count = 0\n",
    "pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200, lookup_freq=lookup_freq)\n",
    "\n",
    "tqc_learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device)\n",
    "\n",
    "#opt_kwargs = {'lr':1e-5}\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=pomdp_transitions,\n",
    "    device=device,\n",
    "    policy=tqc_learner.policy)\n",
    "\n",
    "tboard = TBoardGraphs(logname='BC TQC pickplace lookup det'+str(lookup_freq) , data_path='/data/bing/hendrik/gboard/')\n",
    "def get_avr_succ_rew_det(env, learner, epsiodes):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(epsiodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs, deterministic=True)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "for i in range(10000):\n",
    "    bc_trainer.train(n_epochs=20)\n",
    "    success, rews = get_avr_succ_rew(env=pomdp_env, learner=bc_trainer.policy, epsiodes=200)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqc_learner.save('tqc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqc_learner.load('tqc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, rews = get_avr_succ_rew_det(env=pomdp_env, learner=tqc_learner.policy, epsiodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    }
   ],
   "source": [
    "print(success.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    }
   ],
   "source": [
    "print(success.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.2852957e-02],\n",
       "       [2.3167025e-02],\n",
       "       [2.3714440e-02],\n",
       "       ...,\n",
       "       [1.3438389e-05],\n",
       "       [3.5262790e-06],\n",
       "       [6.4357708e-05]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = TBoardGraphs(logname='BC TQC pickplace continue 17.5 lookup '+str(lookup_freq) , data_path='/data/bing/hendrik/gboard/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp_env_eval, _ = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200, lookup_freq=lookup_freq)\n",
    "pomdp_env.envs[0].reset_count = 0\n",
    "for i in range(10000):\n",
    "    tqc_learner.learn(total_timesteps=200, log_interval=100000)    \n",
    "    success, rews = get_avr_succ_rew_det(env=pomdp_env_eval, learner=tqc_learner.policy, epsiodes=200)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=pomdp_env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=.reset_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    bc_trainer.train(n_epochs=20)\n",
    "    success, rews = get_avr_succ_rew(env=pomdp_env, learner=bc_trainer.policy, epsiodes=200)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
