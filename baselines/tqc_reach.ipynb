{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from active_critic.utils.gym_utils import make_policy_dict\n",
    "from stable_baselines3.common.torch_layers import CombinedExtractor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from active_critic.policy.active_critic_policy import ActiveCriticPolicy\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from metaworld.envs import \\\n",
    "    ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from metaworld.policies import *\n",
    "from gym.wrappers import TimeLimit\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from active_critic.utils.rollout import rollout, make_sample_until, flatten_trajectories\n",
    "from stable_baselines3.common.type_aliases import GymEnv\n",
    "from gym import Env\n",
    "import gym\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from active_critic.utils.gym_utils import sample_expert_transitions, parse_sampled_transitions, new_epoch_reach, DummyExtractor\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "\n",
    "\n",
    "def evaluate_policy(policy, env, episodes, device, path, logname, step):\n",
    "    transitions = sample_expert_transitions(\n",
    "        policy, env, episodes)\n",
    "    actions, observations, rewards = parse_sampled_transitions(\n",
    "        transitions=transitions, new_epoch=new_epoch_reach, extractor=DummyExtractor(), device=device)\n",
    "    rewards = rewards.unsqueeze(-1)\n",
    "    last_reward, _ = rewards.max(dim=1)\n",
    "    success = (last_reward == 1)\n",
    "    success = success.type(th.float)\n",
    "    tboard = TBoardGraphs(logname, data_path=path)\n",
    "\n",
    "    tboard.plotDMPTrajectory(actions[0], actions[0], th.zeros_like(actions[0]),\n",
    "                                      None, None, None, stepid=step, save=False, name_plot='Trajectory', path=None,\n",
    "                                      tol_neg=None, tol_pos=None, inpt=None, name='Trajectory', opt_gen_trj=None, window=None)\n",
    "    tboard.plotDMPTrajectory(rewards[0], rewards[0], th.zeros_like(rewards[0]),\n",
    "                                      None, None, None, stepid=step, save=False, name_plot='Rewards', path=None,\n",
    "                                      tol_neg=None, tol_pos=None, inpt=None, name='Rewards', opt_gen_trj=None, window=None)\n",
    "    tboard.addTrainScalar('rewards', value=last_reward.mean().to('cpu'), stepid=step)      \n",
    "    tboard.addTrainScalar('success', value=success.mean().to('cpu'), stepid=step)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env, \n",
    "    make_dummy_vec_env, \n",
    "    sample_expert_transitions_rollouts, \n",
    "    make_pomdp_rollouts, \n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew\n",
    ")\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "from active_critic.model_src.transformer import PositionalEncoding\n",
    "\n",
    "import copy\n",
    "\n",
    "from active_critic.utils.gym_utils import make_policy_dict, ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE, ResetCounterWrapper, TimeLimit, StrictSeqLenWrapper, ImitationLearningWrapper\n",
    "from active_critic.TQC.tqc import TQC\n",
    "from active_critic.TQC.tqc_policy import TQCPolicyEval\n",
    "\n",
    "def run_experiment(device):\n",
    "    pass\n",
    "device='cuda'\n",
    "lookup_freq = 1\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(vec_expert.predict, env, 10)\n",
    "env.envs[0].reset_count = 0\n",
    "pomdp_rollouts = make_pomdp_rollouts(rollouts, lookup_frq=lookup_freq, count_dim=10)\n",
    "pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)\n",
    "\n",
    "pomdp_env, pomdp_vec_expert = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200, lookup_freq=lookup_freq)\n",
    "policy_kwargs = {'net_arch' : [32, 32, 32]}\n",
    "tqc_learner = TQC(policy='MlpPolicy', env=pomdp_env, device=device, policy_kwargs=policy_kwargs)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avr_succ_rew_det(env, learner, epsiodes):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(epsiodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs, deterministic=True)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=pomdp_transitions,\n",
    "    device=device,\n",
    "    policy=tqc_learner.policy)\n",
    "\n",
    "tboard = TBoardGraphs(logname='BC TQC pickplace lookup netarch[32 32 32] '+str(lookup_freq) , data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    bc_trainer.train(n_epochs=20)\n",
    "    success, rews = get_avr_succ_rew_det(env=pomdp_env, learner=bc_trainer.policy, epsiodes=200)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, rews = get_avr_succ_rew_det(env=pomdp_env, learner=tqc_learner.policy, epsiodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = TBoardGraphs(logname='BC TQC pickplace continue 17.5 lookup '+str(lookup_freq) , data_path='/data/bing/hendrik/gboard/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp_env_eval, _ = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200, lookup_freq=lookup_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    tqc_learner.learn(total_timesteps=200, log_interval=100000)    \n",
    "    success, rews = get_avr_succ_rew(env=pomdp_env_eval, learner=tqc_learner.policy, epsiodes=200)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=i)\n",
    "    print(f'succes: {success.mean()}')\n",
    "    print(f'rews: {rews.mean()}')\n",
    "    print(f'resets: {pomdp_env.envs[0].reset_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pomdp_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp_env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    action = np.array([[0,0,0,0]])\n",
    "    obs, rew, done, info = pomdp_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_env, expert = make_dummy_vec_env(name='reach', seq_len=100)\n",
    "seq_env_eval, expert = make_dummy_vec_env(name='reach', seq_len=100)\n",
    "\n",
    "policy_kwargs = dict(net_arch=[64, 64], n_critics=1)\n",
    "tqc_learner = TQC(policy='MlpPolicy',learning_rate=0.001, env=seq_env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n",
    "path = '/home/hendrik/Documents/master_project/LokalData/baselines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = seq_env.reset()\n",
    "obs = th.tensor(obs).unsqueeze(0)\n",
    "action = th.tensor(np.array([0,0,0,0]), dtype=th.float).unsqueeze(0)\n",
    "tqc_learner.policy.evaluate_actions(obs=obs, actions=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    tqc_learner.learn(2000, log_interval=float('inf'))\n",
    "    evaluate_policy(policy=tqc_learner.policy, env=seq_env_eval, episodes=100, device='cuda', path=path, logname='tqc_reach_success_stop', step=seq_env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv = seq_env.reset()\n",
    "rews = []\n",
    "success = []\n",
    "actions = []\n",
    "for i in range(10000):\n",
    "    action = tqc_learner.policy.predict(obsv[0])\n",
    "    actions.append(action[0])\n",
    "    obsv, rew, done, info = seq_env.step(action)\n",
    "    rews.append(rew)\n",
    "    if done[0]:\n",
    "        success.append(info[0]['success'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(actions)\n",
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(np.arange(len(rews)), actions[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(np.arange(len(rews)), np.array(rews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
