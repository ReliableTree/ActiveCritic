{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib.common.recurrent.type_aliases import RNNStates\n",
    "import gym\n",
    "import torch as th\n",
    "from active_critic.utils.gym_utils import make_dummy_vec_env, sample_expert_transitions_rollouts, parse_sampled_transitions, DummyExtractor\n",
    "from active_critic.utils.pytorch_utils import calcMSE\n",
    "from active_critic.utils.dataset import DatasetAC\n",
    "from torch.utils.data.dataloader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_policy_dict\n",
    "def make_ppo_rec_data_loader(env, vec_expert, n_demonstrations, device):\n",
    "    transitions, rollouts = sample_expert_transitions_rollouts(\n",
    "        vec_expert.predict, env, n_demonstrations)\n",
    "    actions, observations, rewards = parse_sampled_transitions(transitions=transitions, extractor=DummyExtractor(), seq_len=seq_len, device='cuda')\n",
    "    inpt_obsv = observations[:,:1].repeat([1, observations.shape[1], 1])\n",
    "    train_data = DatasetAC(device=device)\n",
    "    train_data.add_data(obsv=inpt_obsv, actions=actions, reward=rewards)\n",
    "    train_data.onyl_positiv = False\n",
    "\n",
    "    dataloader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "class Rec_PPO_BC:\n",
    "    def __init__(self, model:RecurrentPPO, dataloader:DatasetAC, device) -> None:\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, n_epochs, verbose = False):\n",
    "        self.model.policy.train()\n",
    "        for epoch in range(n_epochs):\n",
    "            for obs, act, rew in self.dataloader:\n",
    "                batch_size = obs.shape[0]\n",
    "                lstm = self.model.policy.lstm_actor\n",
    "                self.model.policy.train()\n",
    "\n",
    "                single_hidden_state_shape = (lstm.num_layers, batch_size, lstm.hidden_size)\n",
    "                # hidden and cell states for actor and critic\n",
    "                lstm_states = RNNStates(\n",
    "                    (\n",
    "                        th.zeros(single_hidden_state_shape).to(self.model.device),\n",
    "                        th.zeros(single_hidden_state_shape).to(self.model.device),\n",
    "                    ),\n",
    "                    (\n",
    "                        th.zeros(single_hidden_state_shape).to(self.model.device),\n",
    "                        th.zeros(single_hidden_state_shape).to(self.model.device),\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                actions_res, values, log_prob, rnn_states = self.model.policy.forward(obs = obs, lstm_states=lstm_states, episode_starts = th.zeros([obs.shape[0] * obs.shape[1]], device=self.device), deterministic=False)\n",
    "                self.model.policy.optimizer.zero_grad()\n",
    "                loss = calcMSE(actions_res.reshape(-1), act.reshape(-1))\n",
    "                loss.backward()\n",
    "                self.model.policy.optimizer.step()\n",
    "                if verbose and epoch % 2000 == 0:\n",
    "                    print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "env_tag = 'pickplace'\n",
    "device = 'cuda'\n",
    "seq_len = 100\n",
    "n_demonstrations = 10\n",
    "env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", env=env, verbose=1)\n",
    "dataloader = make_ppo_rec_data_loader(env=env, vec_expert=vec_expert, n_demonstrations=n_demonstrations, device=device)\n",
    "\n",
    "bc_trainer = Rec_PPO_BC(model=model, dataloader=dataloader, device=device)\n",
    "bc_trainer.train(n_epochs=20000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ppo_rec_data_loader(env, vec_expert, n_demonstrations, device):\n",
    "    transitions, rollouts = sample_expert_transitions_rollouts(\n",
    "        vec_expert.predict, env, n_demonstrations)\n",
    "    actions, observations, rewards = parse_sampled_transitions(transitions=transitions, extractor=DummyExtractor(), seq_len=seq_len, device='cuda')\n",
    "    inpt_obsv = observations[:,:1].repeat([1, observations.shape[1], 1])\n",
    "    train_data = DatasetAC(device=device)\n",
    "    train_data.add_data(obsv=inpt_obsv, actions=actions, reward=rewards)\n",
    "    train_data.onyl_positiv = False\n",
    "\n",
    "    dataloader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/metaworld/policies/policy.py:41: UserWarning: Constant(s) may be too high. Environments clip response to [-1, 1]\n",
      "  warnings.warn('Constant(s) may be too high. Environments clip response to [-1, 1]')\n",
      "/home/hendrik/Documents/master_project/Code/active_critic/src/active_critic/utils/gym_utils.py:268: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  actions = th.tensor(fill_arrays(actions, seq_len = seq_len), dtype=th.float, device=device)\n"
     ]
    }
   ],
   "source": [
    "dataloader = make_ppo_rec_data_loader(env=env, vec_expert=vec_expert, n_demonstrations=n_demonstrations, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_obsv = observations[:,:1].repeat([1, observations.shape[1], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpt_obsv = inpt_obsv.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_obsv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = actions.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inpt_obsv.shape[0]\n",
    "lstm = model.policy.lstm_actor\n",
    "a = model.policy.train()\n",
    "\n",
    "single_hidden_state_shape = (lstm.num_layers, batch_size, lstm.hidden_size)\n",
    "# hidden and cell states for actor and critic\n",
    "lstm_states = RNNStates(\n",
    "    (\n",
    "        th.zeros(single_hidden_state_shape).to(model.device),\n",
    "        th.zeros(single_hidden_state_shape).to(model.device),\n",
    "    ),\n",
    "    (\n",
    "        th.zeros(single_hidden_state_shape).to(model.device),\n",
    "        th.zeros(single_hidden_state_shape).to(model.device),\n",
    "    ),\n",
    ")\n",
    "\n",
    "for i in range(20000):\n",
    "    rnn_states_list = [lstm_states]\n",
    "    actions_res, values, log_prob, rnn_states = model.policy.forward(obs = inpt_obsv, lstm_states=lstm_states, episode_starts = th.zeros([inpt_obsv.shape[0] * inpt_obsv.shape[1]], device='cuda'), deterministic=False)\n",
    "    model.policy.optimizer.zero_grad()\n",
    "    loss = calcMSE(actions_res.reshape(-1), actions.reshape(-1))\n",
    "    loss.backward()\n",
    "    model.policy.optimizer.step()\n",
    "    if i % 2000 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = []\n",
    "lstm_states = None\n",
    "for i in range(observations.shape[1]):\n",
    "    if i == 0:\n",
    "        episode_start_c = 1\n",
    "    else:\n",
    "        episode_start_c = 0\n",
    "    pred_actions, lstm_states = model.predict(observation=inpt_obsv[0,:1].cpu().numpy(), deterministic=True, episode_start=np.array([episode_start_c]), state=lstm_states)\n",
    "    all_actions.append(pred_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = np.array(all_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_actions = actions[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = ((all_actions.reshape([-1]) - wanted_actions.reshape([-1]))**2).mean()\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actions, rnn_states = model.predict(observation=inpt_obsv[0,:1].cpu().numpy(), deterministic=True, episode_start=np.array([0]), state=rnn_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[0,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_res.reshape([2, 20,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(100000):\n",
    "    rnn_states_list = [RNNStates(pi=lstm_states[0], vf=lstm_states[1])]\n",
    "\n",
    "    for i in range(actions.shape[1]):\n",
    "        _, values, log_prob, rnn_states = model.policy.forward(obs = obs, lstm_states=rnn_states_list[-1], episode_starts = th.zeros([1], device='cuda'), deterministic=False)\n",
    "        rnn_states_list.append(rnn_states)\n",
    "\n",
    "    model.policy.optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for i in range(len(rnn_states_list) - 1):\n",
    "        if i == 0:\n",
    "            episode_starts = th.ones([1], device='cuda')\n",
    "        else:\n",
    "            episode_starts = th.zeros([1], device='cuda')\n",
    "\n",
    "        wanted_action = actions[:,i]\n",
    "\n",
    "        value, log_prob, entropy = model.policy.evaluate_actions(obs, wanted_action, rnn_states_list[i], episode_starts = episode_starts)\n",
    "        prob_true_act = th.exp(log_prob).mean()\n",
    "        log_prob = log_prob.mean()\n",
    "        entropy = entropy.mean()\n",
    "\n",
    "\n",
    "        ent_loss = 0 * entropy\n",
    "        neglogp = -log_prob\n",
    "        loss = neglogp + ent_loss\n",
    "        total_loss = total_loss + loss\n",
    "    total_loss = total_loss.mean()\n",
    "    if j%100 == 0:\n",
    "        print(f'total_loss {total_loss}')\n",
    "    total_loss.backward()\n",
    "    model.policy.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_actions = []\n",
    "rnn_states_list = [RNNStates(pi=lstm_states[0], vf=lstm_states[1])]\n",
    "for i in range(actions.shape[1]):\n",
    "    rnn_action, values, log_prob, rnn_states = model.policy.forward(obs = obs, lstm_states=rnn_states_list[-1], episode_starts = th.zeros([1], device='cuda'), deterministic=True)\n",
    "    rnn_actions.append(rnn_action)    \n",
    "    rnn_states_list.append(rnn_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for i, rnn_action in enumerate(rnn_actions):\n",
    "    loss += calcMSE(rnn_action, actions[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, states = model.policy.predict(obs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, states = model.policy.predict(obs.cpu().numpy(), state=states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, states = model.policy.predict(obs.cpu().numpy(), state=states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_states_list[-1][0][0].mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.lstm_actor.all_weights[0][0]._grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_weight = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval actions:         \n",
    "obs: th.Tensor,\n",
    "actions: th.Tensor,\n",
    "lstm_states: RNNStates,\n",
    "episode_starts: th.Tensor,\n",
    "\n",
    "-> \n",
    "values, log_prob, distribution.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, log_prob, entropy = model.policy.evaluate_actions(obs, actions, rnn_states, episode_starts = th.ones([1], device='cuda'))\n",
    "#print(f'max log: {log_prob.max()}')\n",
    "\n",
    "prob_true_act = th.exp(log_prob).mean()\n",
    "log_prob = log_prob.mean()\n",
    "entropy = entropy.mean()\n",
    "\n",
    "\n",
    "ent_loss = ent_weight * entropy\n",
    "neglogp = -log_prob\n",
    "loss = neglogp + ent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.lstm_actor.forward(obs = env.reset(), lstm_states=lstm_states, epsidode_starts = th.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.bc import BC\n",
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env,\n",
    "    make_dummy_vec_env,\n",
    "    sample_expert_transitions_rollouts,\n",
    "    make_pomdp_rollouts,\n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew_det\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tag = 'pickaplce'\n",
    "seq_len = 100\n",
    "n_demonstrations\n",
    "\n",
    "env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "val_env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(\n",
    "    vec_expert.predict, val_env, n_demonstrations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
