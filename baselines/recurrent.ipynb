{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib.common.recurrent.type_aliases import RNNStates\n",
    "import gym\n",
    "import torch as th\n",
    "from active_critic.utils.gym_utils import make_dummy_vec_env, sample_expert_transitions_rollouts, parse_sampled_transitions, DummyExtractor\n",
    "from active_critic.utils.pytorch_utils import calcMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_policy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/metaworld/policies/policy.py:41: UserWarning: Constant(s) may be too high. Environments clip response to [-1, 1]\n",
      "  warnings.warn('Constant(s) may be too high. Environments clip response to [-1, 1]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/Documents/master_project/Code/active_critic/src/active_critic/utils/gym_utils.py:268: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  actions = th.tensor(fill_arrays(actions, seq_len = seq_len), dtype=th.float, device=device)\n"
     ]
    }
   ],
   "source": [
    "env_tag = 'pickplace'\n",
    "seq_len = 100\n",
    "n_demonstrations = 10\n",
    "env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(\n",
    "    vec_expert.predict, env, n_demonstrations)\n",
    "actions, observations, rewards = parse_sampled_transitions(transitions=transitions, extractor=DummyExtractor(), seq_len=seq_len, device='cuda')\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", env=env, verbose=1)\n",
    "lstm_states = model._last_lstm_states\n",
    "a = model.policy.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 39])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_obsv = observations[:,:1].repeat([1, observations.shape[1], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpt_obsv = inpt_obsv.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 39])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt_obsv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = actions.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = inpt_obsv.shape[0]\n",
    "lstm = model.policy.lstm_actor\n",
    "a = model.policy.train()\n",
    "\n",
    "single_hidden_state_shape = (lstm.num_layers, batch_size, lstm.hidden_size)\n",
    "# hidden and cell states for actor and critic\n",
    "lstm_states = RNNStates(\n",
    "    (\n",
    "        th.zeros(single_hidden_state_shape).to(model.device),\n",
    "        th.zeros(single_hidden_state_shape).to(model.device),\n",
    "    ),\n",
    "    (\n",
    "        th.zeros(single_hidden_state_shape).to(model.device),\n",
    "        th.zeros(single_hidden_state_shape).to(model.device),\n",
    "    ),\n",
    ")\n",
    "\n",
    "for i in range(20000):\n",
    "    rnn_states_list = [lstm_states]\n",
    "    actions_res, values, log_prob, rnn_states = model.policy.forward(obs = inpt_obsv, lstm_states=lstm_states, episode_starts = th.zeros([inpt_obsv.shape[0] * inpt_obsv.shape[1]], device='cuda'), deterministic=False)\n",
    "    model.policy.optimizer.zero_grad()\n",
    "    loss = calcMSE(actions_res.reshape(-1), actions.reshape(-1))\n",
    "    loss.backward()\n",
    "    model.policy.optimizer.step()\n",
    "    if i % 2000 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = []\n",
    "lstm_states = None\n",
    "for i in range(observations.shape[1]):\n",
    "    if i == 0:\n",
    "        episode_start_c = 1\n",
    "    else:\n",
    "        episode_start_c = 0\n",
    "    pred_actions, lstm_states = model.predict(observation=inpt_obsv[0,:1].cpu().numpy(), deterministic=True, episode_start=np.array([episode_start_c]), state=lstm_states)\n",
    "    all_actions.append(pred_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = np.array(all_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_actions = actions[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00072560186"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = ((all_actions.reshape([-1]) - wanted_actions.reshape([-1]))**2).mean()\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actions, rnn_states = model.predict(observation=inpt_obsv[0,:1].cpu().numpy(), deterministic=True, episode_start=np.array([0]), state=rnn_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84174454,  0.05192827, -0.75935364, -0.03078466]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7563,  0.0587, -0.7430,  0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 20, 4]' is invalid for input of size 4000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m actions_res\u001b[39m.\u001b[39;49mreshape([\u001b[39m2\u001b[39;49m, \u001b[39m20\u001b[39;49m,\u001b[39m4\u001b[39;49m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 20, 4]' is invalid for input of size 4000"
     ]
    }
   ],
   "source": [
    "actions_res.reshape([2, 20,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(100000):\n",
    "    rnn_states_list = [RNNStates(pi=lstm_states[0], vf=lstm_states[1])]\n",
    "\n",
    "    for i in range(actions.shape[1]):\n",
    "        _, values, log_prob, rnn_states = model.policy.forward(obs = obs, lstm_states=rnn_states_list[-1], episode_starts = th.zeros([1], device='cuda'), deterministic=False)\n",
    "        rnn_states_list.append(rnn_states)\n",
    "\n",
    "    model.policy.optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for i in range(len(rnn_states_list) - 1):\n",
    "        if i == 0:\n",
    "            episode_starts = th.ones([1], device='cuda')\n",
    "        else:\n",
    "            episode_starts = th.zeros([1], device='cuda')\n",
    "\n",
    "        wanted_action = actions[:,i]\n",
    "\n",
    "        value, log_prob, entropy = model.policy.evaluate_actions(obs, wanted_action, rnn_states_list[i], episode_starts = episode_starts)\n",
    "        prob_true_act = th.exp(log_prob).mean()\n",
    "        log_prob = log_prob.mean()\n",
    "        entropy = entropy.mean()\n",
    "\n",
    "\n",
    "        ent_loss = 0 * entropy\n",
    "        neglogp = -log_prob\n",
    "        loss = neglogp + ent_loss\n",
    "        total_loss = total_loss + loss\n",
    "    total_loss = total_loss.mean()\n",
    "    if j%100 == 0:\n",
    "        print(f'total_loss {total_loss}')\n",
    "    total_loss.backward()\n",
    "    model.policy.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_actions = []\n",
    "rnn_states_list = [RNNStates(pi=lstm_states[0], vf=lstm_states[1])]\n",
    "for i in range(actions.shape[1]):\n",
    "    rnn_action, values, log_prob, rnn_states = model.policy.forward(obs = obs, lstm_states=rnn_states_list[-1], episode_starts = th.zeros([1], device='cuda'), deterministic=True)\n",
    "    rnn_actions.append(rnn_action)    \n",
    "    rnn_states_list.append(rnn_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for i, rnn_action in enumerate(rnn_actions):\n",
    "    loss += calcMSE(rnn_action, actions[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, states = model.policy.predict(obs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, states = model.policy.predict(obs.cpu().numpy(), state=states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, states = model.policy.predict(obs.cpu().numpy(), state=states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_states_list[-1][0][0].mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.lstm_actor.all_weights[0][0]._grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_weight = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval actions:         \n",
    "obs: th.Tensor,\n",
    "actions: th.Tensor,\n",
    "lstm_states: RNNStates,\n",
    "episode_starts: th.Tensor,\n",
    "\n",
    "-> \n",
    "values, log_prob, distribution.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, log_prob, entropy = model.policy.evaluate_actions(obs, actions, rnn_states, episode_starts = th.ones([1], device='cuda'))\n",
    "#print(f'max log: {log_prob.max()}')\n",
    "\n",
    "prob_true_act = th.exp(log_prob).mean()\n",
    "log_prob = log_prob.mean()\n",
    "entropy = entropy.mean()\n",
    "\n",
    "\n",
    "ent_loss = ent_weight * entropy\n",
    "neglogp = -log_prob\n",
    "loss = neglogp + ent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.lstm_actor.forward(obs = env.reset(), lstm_states=lstm_states, epsidode_starts = th.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.bc import BC\n",
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env,\n",
    "    make_dummy_vec_env,\n",
    "    sample_expert_transitions_rollouts,\n",
    "    make_pomdp_rollouts,\n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew_det\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tag = 'pickaplce'\n",
    "seq_len = 100\n",
    "n_demonstrations\n",
    "\n",
    "env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "val_env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(\n",
    "    vec_expert.predict, val_env, n_demonstrations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
