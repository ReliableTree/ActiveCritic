{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib.common.recurrent.type_aliases import RNNStates\n",
    "import gym\n",
    "import torch as th\n",
    "from active_critic.utils.gym_utils import make_dummy_vec_env, sample_expert_transitions_rollouts, parse_sampled_transitions, DummyExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_policy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tag = 'pickplace'\n",
    "seq_len = 20\n",
    "n_demonstrations = 1\n",
    "env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(\n",
    "    vec_expert.predict, env, n_demonstrations)\n",
    "actions, observations, rewards = parse_sampled_transitions(transitions=transitions, extractor=DummyExtractor(), seq_len=seq_len, device='cuda')\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", env=env, verbose=1)\n",
    "lstm_states = model._last_lstm_states\n",
    "a = model.policy.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs =  th.tensor(env.reset(),device='cuda').reshape([1,-1])\n",
    "obs = observations[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(100000):\n",
    "    rnn_states_list = [RNNStates(pi=lstm_states[0], vf=lstm_states[1])]\n",
    "\n",
    "    for i in range(actions.shape[1]):\n",
    "        _, values, log_prob, rnn_states = model.policy.forward(obs = obs, lstm_states=rnn_states_list[-1], episode_starts = th.zeros([1], device='cuda'), deterministic=False)\n",
    "        rnn_states_list.append(rnn_states)\n",
    "\n",
    "    model.policy.optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for i in range(len(rnn_states_list) - 1):\n",
    "        if i == 0:\n",
    "            episode_starts = th.ones([1], device='cuda')\n",
    "        else:\n",
    "            episode_starts = th.zeros([1], device='cuda')\n",
    "\n",
    "        wanted_action = actions[:,i]\n",
    "\n",
    "        value, log_prob, entropy = model.policy.evaluate_actions(obs, wanted_action, rnn_states_list[i], episode_starts = episode_starts)\n",
    "        prob_true_act = th.exp(log_prob).mean()\n",
    "        log_prob = log_prob.mean()\n",
    "        entropy = entropy.mean()\n",
    "\n",
    "\n",
    "        ent_loss = 0 * entropy\n",
    "        neglogp = -log_prob\n",
    "        loss = neglogp + ent_loss\n",
    "        total_loss = total_loss + loss\n",
    "    total_loss = total_loss.mean()\n",
    "    if j%100 == 0:\n",
    "        print(f'total_loss {total_loss}')\n",
    "    total_loss.backward()\n",
    "    model.policy.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_actions = []\n",
    "rnn_states_list = [RNNStates(pi=lstm_states[0], vf=lstm_states[1])]\n",
    "for i in range(actions.shape[1]):\n",
    "    rnn_action, values, log_prob, rnn_states = model.policy.forward(obs = obs, lstm_states=rnn_states_list[-1], episode_starts = th.zeros([1], device='cuda'), deterministic=True)\n",
    "    rnn_actions.append(rnn_action)    \n",
    "    rnn_states_list.append(rnn_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.pytorch_utils import calcMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for i, rnn_action in enumerate(rnn_actions):\n",
    "    loss += calcMSE(rnn_action, actions[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6342e-05, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, states = model.policy.predict(obs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, states = model.policy.predict(obs.cpu().numpy(), state=states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, states = model.policy.predict(obs.cpu().numpy(), state=states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_states_list[-1][0][0].mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.lstm_actor.all_weights[0][0]._grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_weight = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval actions:         \n",
    "obs: th.Tensor,\n",
    "actions: th.Tensor,\n",
    "lstm_states: RNNStates,\n",
    "episode_starts: th.Tensor,\n",
    "\n",
    "-> \n",
    "values, log_prob, distribution.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, log_prob, entropy = model.policy.evaluate_actions(obs, actions, rnn_states, episode_starts = th.ones([1], device='cuda'))\n",
    "#print(f'max log: {log_prob.max()}')\n",
    "\n",
    "prob_true_act = th.exp(log_prob).mean()\n",
    "log_prob = log_prob.mean()\n",
    "entropy = entropy.mean()\n",
    "\n",
    "\n",
    "ent_loss = ent_weight * entropy\n",
    "neglogp = -log_prob\n",
    "loss = neglogp + ent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.lstm_actor.forward(obs = env.reset(), lstm_states=lstm_states, epsidode_starts = th.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.bc import BC\n",
    "from active_critic.utils.gym_utils import (\n",
    "    make_vec_env,\n",
    "    make_dummy_vec_env,\n",
    "    sample_expert_transitions_rollouts,\n",
    "    make_pomdp_rollouts,\n",
    "    make_dummy_vec_env_pomdp,\n",
    "    get_avr_succ_rew_det\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tag = 'pickaplce'\n",
    "seq_len = 100\n",
    "n_demonstrations\n",
    "\n",
    "env, vec_expert = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "val_env, _ = make_dummy_vec_env(name=env_tag, seq_len=seq_len)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions_rollouts(\n",
    "    vec_expert.predict, val_env, n_demonstrations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
